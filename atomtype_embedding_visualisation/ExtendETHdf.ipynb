{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "152cd555",
   "metadata": {},
   "source": [
    "# Extension of ETH dataset dataframe to include:\n",
    "- mapping atoms of mol-file to SMILES is correct\n",
    "- SELFIES\n",
    "- SELFIES mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03c25d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ifender/miniconda3/envs/fairseq_git2/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Skipped loading some Tensorflow models, missing a dependency. No module named 'tensorflow'\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'torch_geometric'\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. cannot import name 'DMPNN' from 'deepchem.models.torch_models' (/home/ifender/miniconda3/envs/fairseq_git2/lib/python3.10/site-packages/deepchem/models/torch_models/__init__.py)\n",
      "Skipped loading modules with pytorch-lightning dependency, missing a dependency. No module named 'pytorch_lightning'\n",
      "Skipped loading some Jax models, missing a dependency. No module named 'jax'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tokenisation import tokenize_dataset, get_tokenizer\n",
    "from constants import (\n",
    "    TOKENIZER_PATH\n",
    ")\n",
    "from typing import List\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58091644",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_SMILES(SMILES_tok):\n",
    "    \"\"\"Cleaning of SMILES tokens input from hydrogens and digits\n",
    "\n",
    "    Args:\n",
    "        SMILES_tok (_list_): List of SMILES_tokens for a given SMILES\n",
    "\n",
    "    Returns:\n",
    "        _list,list_: Processed SMILES_token list and list of positions in input tokens list that were kept \n",
    "        (needed to distinguish which embeddings are relevant)\n",
    "    \"\"\"\n",
    "    SMILES_tok_prep = list()\n",
    "    struc_toks = r\"()=:~1234567890#/\\\\\"\n",
    "    posToKeep = list()\n",
    "    pos = 0\n",
    "    for i in range(len(SMILES_tok)):\n",
    "        # If token is bracketed, keep as is, even if contains H or other atoms or numbers\n",
    "        if SMILES_tok[i].startswith(\"[\") and SMILES_tok[i].endswith(\"]\"):\n",
    "            SMILES_tok_prep.append(SMILES_tok[i])\n",
    "            posToKeep.append(pos)\n",
    "        # when it's an H in the SMILES, ignore, cannot deal\n",
    "        elif SMILES_tok[i] != \"H\" and SMILES_tok[i] != \"h\" and not SMILES_tok[i].isdigit() and not SMILES_tok[i].isspace():\n",
    "            if not any(elem in struc_toks for elem in SMILES_tok[i]):\n",
    "                if SMILES_tok[i] != \"-\":\n",
    "                    SMILES_tok_prep.append(SMILES_tok[i])\n",
    "                    # keep pos where you keep SMILES token\n",
    "                    posToKeep.append(pos)\n",
    "        pos += 1\n",
    "    assert(len(posToKeep) == (len(SMILES_tok_prep))\n",
    "           ), f\"Length of positions-to-keep-array ({len(posToKeep)}) and length of SMILES_tok_prep ({len(SMILES_tok_prep)}) are not the same\"\n",
    "    print(\"SMILES_tok: \", SMILES_tok)\n",
    "    print(\"posToKeep: \", posToKeep)\n",
    "    print(\"SMILES_tok_prep: \", SMILES_tok_prep)\n",
    "\n",
    "    return SMILES_tok_prep, posToKeep\n",
    "\n",
    "def get_tokenized_SMILES(task_SMILES: List[str]):\n",
    "    \"\"\"Tokenize SMILES string\n",
    "\n",
    "    Args:\n",
    "        input_list of strings (str): List of SMILES input string\n",
    "\n",
    "    Returns:\n",
    "        dict: dictionary that links canonize SMILES string\n",
    "    \"\"\"\n",
    "    tokenizer = get_tokenizer(TOKENIZER_PATH)\n",
    "    print(f\"tokenizer {tokenizer}\")\n",
    "    smi_toks = tokenize_dataset(tokenizer, task_SMILES, False)\n",
    "    smi_toks = [smi_tok.split() for smi_tok in smi_toks]\n",
    "    print(f\"SMILES tokens: {smi_toks[0]}\")\n",
    "    smiles_dict = dict(zip(task_SMILES,smi_toks))\n",
    "    return smiles_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "733707b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  atom_idx element  mulliken     resp1     resp2      dual  \\\n",
      "0           0         0       C -0.290919 -0.287521 -0.287521 -0.001670   \n",
      "1           1         1       N  0.419352  0.458319  0.458319  0.195391   \n",
      "2           2         2       O -0.397038 -0.535125 -0.535125 -0.351755   \n",
      "3           3         0       C -0.387290 -0.449151 -0.449151  0.157883   \n",
      "4           4         1       C  0.022985  0.052905  0.052905  0.220494   \n",
      "5           5         2       C -0.261951 -0.473223 -0.473223 -0.052606   \n",
      "6           6         3       C  0.001897  0.634123  0.631025 -0.002175   \n",
      "7           7         4       N -0.591325 -1.196668 -1.196668 -0.486405   \n",
      "8           8         0       C -0.387167 -0.462836 -0.462836  0.164734   \n",
      "9           9         1       C  0.022232  0.069244  0.069244  0.222098   \n",
      "\n",
      "   cnf_idx  mbis_dipole_strength     DASH_IDX            comp_key      SMILES  \\\n",
      "0        0              0.042085   QMUGS500_1   conf_00QMUGS500_1  C#[N+][O-]   \n",
      "1        0              0.200745   QMUGS500_1   conf_00QMUGS500_1  C#[N+][O-]   \n",
      "2        0              0.048333   QMUGS500_1   conf_00QMUGS500_1  C#[N+][O-]   \n",
      "3        0              0.050744  QMUGS500_17  conf_00QMUGS500_17     C=C=CCN   \n",
      "4        0              0.014284  QMUGS500_17  conf_00QMUGS500_17     C=C=CCN   \n",
      "5        0              0.081285  QMUGS500_17  conf_00QMUGS500_17     C=C=CCN   \n",
      "6        0              0.142321  QMUGS500_17  conf_00QMUGS500_17     C=C=CCN   \n",
      "7        0              0.149160  QMUGS500_17  conf_00QMUGS500_17     C=C=CCN   \n",
      "8        1              0.050662  QMUGS500_17  conf_01QMUGS500_17     C=C=CCN   \n",
      "9        1              0.014335  QMUGS500_17  conf_01QMUGS500_17     C=C=CCN   \n",
      "\n",
      "       CHEMBL_ID  \n",
      "0   CHEMBL185198  \n",
      "1   CHEMBL185198  \n",
      "2   CHEMBL185198  \n",
      "3  CHEMBL3951138  \n",
      "4  CHEMBL3951138  \n",
      "5  CHEMBL3951138  \n",
      "6  CHEMBL3951138  \n",
      "7  CHEMBL3951138  \n",
      "8  CHEMBL3951138  \n",
      "9  CHEMBL3951138  \n"
     ]
    }
   ],
   "source": [
    "# \n",
    "csv_data = \"/data/jgut/SMILES_or_SELFIES/dash_dataset.csv\"\n",
    "df = pd.read_csv(csv_data)\n",
    "#print(df.head())\n",
    "# show first 10 lines\n",
    "print(df.head(10))\n",
    "df = df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de9fcaee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15dc7310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer PreTrainedTokenizerFast(name_or_path='/data/jgut/SMILES_or_SELFIES/tokenizer/smiles_atom_isomers', vocab_size=432, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True)})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 130.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMILES tokens: ['C', '#', '[N+]', '[O-]']\n",
      "['C#[N+][O-]' 'C=C=CCN']\n",
      "SMILES_tok:  ['C', '#', '[N+]', '[O-]']\n",
      "posToKeep:  [0, 2, 3]\n",
      "SMILES_tok_prep:  ['C', '[N+]', '[O-]']\n",
      "SMILES_tok:  ['C', '#', '[N+]', '[O-]']\n",
      "posToKeep:  [0, 2, 3]\n",
      "SMILES_tok_prep:  ['C', '[N+]', '[O-]']\n",
      "SMILES_tok:  ['C', '#', '[N+]', '[O-]']\n",
      "posToKeep:  [0, 2, 3]\n",
      "SMILES_tok_prep:  ['C', '[N+]', '[O-]']\n",
      "SMILES_tok:  ['C', '=', 'C', '=', 'C', 'C', 'N']\n",
      "posToKeep:  [0, 2, 4, 5, 6]\n",
      "SMILES_tok_prep:  ['C', 'C', 'C', 'C', 'N']\n",
      "SMILES_tok:  ['C', '=', 'C', '=', 'C', 'C', 'N']\n",
      "posToKeep:  [0, 2, 4, 5, 6]\n",
      "SMILES_tok_prep:  ['C', 'C', 'C', 'C', 'N']\n",
      "SMILES_tok:  ['C', '=', 'C', '=', 'C', 'C', 'N']\n",
      "posToKeep:  [0, 2, 4, 5, 6]\n",
      "SMILES_tok_prep:  ['C', 'C', 'C', 'C', 'N']\n",
      "SMILES_tok:  ['C', '=', 'C', '=', 'C', 'C', 'N']\n",
      "posToKeep:  [0, 2, 4, 5, 6]\n",
      "SMILES_tok_prep:  ['C', 'C', 'C', 'C', 'N']\n",
      "SMILES_tok:  ['C', '=', 'C', '=', 'C', 'C', 'N']\n",
      "posToKeep:  [0, 2, 4, 5, 6]\n",
      "SMILES_tok_prep:  ['C', 'C', 'C', 'C', 'N']\n",
      "SMILES_tok:  ['C', '=', 'C', '=', 'C', 'C', 'N']\n",
      "posToKeep:  [0, 2, 4, 5, 6]\n",
      "SMILES_tok_prep:  ['C', 'C', 'C', 'C', 'N']\n",
      "SMILES_tok:  ['C', '=', 'C', '=', 'C', 'C', 'N']\n",
      "posToKeep:  [0, 2, 4, 5, 6]\n",
      "SMILES_tok_prep:  ['C', 'C', 'C', 'C', 'N']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# add a column of the SMILES as tokenized SMILES to df\n",
    "smiles_dict = get_tokenized_SMILES(list(df[\"SMILES\"].unique().tolist()))\n",
    "df[\"tokenized_SMILES\"] = df[\"SMILES\"].map(smiles_dict)\n",
    "print(df['SMILES'].unique())\n",
    "\n",
    "# clean the tokenized SMILES,get a list of cleaned SMILES\n",
    "df['cleaned_tokenized_SMILES'] = df['tokenized_SMILES'].apply(lambda x: clean_SMILES(x)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07b1aad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  atom_idx element  mulliken     resp1     resp2      dual  \\\n",
      "0           0         0       C -0.290919 -0.287521 -0.287521 -0.001670   \n",
      "1           1         1       N  0.419352  0.458319  0.458319  0.195391   \n",
      "2           2         2       O -0.397038 -0.535125 -0.535125 -0.351755   \n",
      "3           3         0       C -0.387290 -0.449151 -0.449151  0.157883   \n",
      "4           4         1       C  0.022985  0.052905  0.052905  0.220494   \n",
      "5           5         2       C -0.261951 -0.473223 -0.473223 -0.052606   \n",
      "6           6         3       C  0.001897  0.634123  0.631025 -0.002175   \n",
      "7           7         4       N -0.591325 -1.196668 -1.196668 -0.486405   \n",
      "8           8         0       C -0.387167 -0.462836 -0.462836  0.164734   \n",
      "9           9         1       C  0.022232  0.069244  0.069244  0.222098   \n",
      "\n",
      "   cnf_idx  mbis_dipole_strength     DASH_IDX            comp_key      SMILES  \\\n",
      "0        0              0.042085   QMUGS500_1   conf_00QMUGS500_1  C#[N+][O-]   \n",
      "1        0              0.200745   QMUGS500_1   conf_00QMUGS500_1  C#[N+][O-]   \n",
      "2        0              0.048333   QMUGS500_1   conf_00QMUGS500_1  C#[N+][O-]   \n",
      "3        0              0.050744  QMUGS500_17  conf_00QMUGS500_17     C=C=CCN   \n",
      "4        0              0.014284  QMUGS500_17  conf_00QMUGS500_17     C=C=CCN   \n",
      "5        0              0.081285  QMUGS500_17  conf_00QMUGS500_17     C=C=CCN   \n",
      "6        0              0.142321  QMUGS500_17  conf_00QMUGS500_17     C=C=CCN   \n",
      "7        0              0.149160  QMUGS500_17  conf_00QMUGS500_17     C=C=CCN   \n",
      "8        1              0.050662  QMUGS500_17  conf_01QMUGS500_17     C=C=CCN   \n",
      "9        1              0.014335  QMUGS500_17  conf_01QMUGS500_17     C=C=CCN   \n",
      "\n",
      "       CHEMBL_ID       tokenized_SMILES cleaned_tokenized_SMILES  \n",
      "0   CHEMBL185198     [C, #, [N+], [O-]]          [C, [N+], [O-]]  \n",
      "1   CHEMBL185198     [C, #, [N+], [O-]]          [C, [N+], [O-]]  \n",
      "2   CHEMBL185198     [C, #, [N+], [O-]]          [C, [N+], [O-]]  \n",
      "3  CHEMBL3951138  [C, =, C, =, C, C, N]          [C, C, C, C, N]  \n",
      "4  CHEMBL3951138  [C, =, C, =, C, C, N]          [C, C, C, C, N]  \n",
      "5  CHEMBL3951138  [C, =, C, =, C, C, N]          [C, C, C, C, N]  \n",
      "6  CHEMBL3951138  [C, =, C, =, C, C, N]          [C, C, C, C, N]  \n",
      "7  CHEMBL3951138  [C, =, C, =, C, C, N]          [C, C, C, C, N]  \n",
      "8  CHEMBL3951138  [C, =, C, =, C, C, N]          [C, C, C, C, N]  \n",
      "9  CHEMBL3951138  [C, =, C, =, C, C, N]          [C, C, C, C, N]  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53965da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  atom_idx element  mulliken     resp1     resp2      dual  \\\n",
      "0           0         0       C -0.290919 -0.287521 -0.287521 -0.001670   \n",
      "1           1         1       N  0.419352  0.458319  0.458319  0.195391   \n",
      "2           2         2       O -0.397038 -0.535125 -0.535125 -0.351755   \n",
      "\n",
      "   cnf_idx  mbis_dipole_strength    DASH_IDX           comp_key      SMILES  \\\n",
      "0        0              0.042085  QMUGS500_1  conf_00QMUGS500_1  C#[N+][O-]   \n",
      "1        0              0.200745  QMUGS500_1  conf_00QMUGS500_1  C#[N+][O-]   \n",
      "2        0              0.048333  QMUGS500_1  conf_00QMUGS500_1  C#[N+][O-]   \n",
      "\n",
      "      CHEMBL_ID    tokenized_SMILES cleaned_tokenized_SMILES  \n",
      "0  CHEMBL185198  [C, #, [N+], [O-]]          [C, [N+], [O-]]  \n",
      "1  CHEMBL185198  [C, #, [N+], [O-]]          [C, [N+], [O-]]  \n",
      "2  CHEMBL185198  [C, #, [N+], [O-]]          [C, [N+], [O-]]  \n",
      "SMILES: ['C', '[N+]', '[O-]']\n",
      "Elements: ['C', 'N', 'O']\n",
      "SMILES letters: ['C', '[N+]', '[O-]']\n",
      "Element: C, SMILES token: C\n",
      "Element: N, SMILES token: [N+]\n",
      "Element: O, SMILES token: [O-]\n"
     ]
    }
   ],
   "source": [
    "# Select rows up to the first repeated 0 in 'atom_idx' and the next set and the next set\n",
    "first_smiles = df.iloc[0]['SMILES']\n",
    "rows = []\n",
    "seen_zero = False\n",
    "for idx, row in df.iterrows():\n",
    "    if seen_zero and row['atom_idx'] == 0:\n",
    "        break\n",
    "    rows.append(row)\n",
    "    if row['atom_idx'] == 0:\n",
    "        seen_zero = True\n",
    "\n",
    "selected_df = pd.DataFrame(rows)\n",
    "print(selected_df)\n",
    "\n",
    "# Check if all selected rows have the same SMILES\n",
    "if selected_df['SMILES'].nunique() == 1:\n",
    "    smiles_toks = selected_df.iloc[0]['cleaned_tokenized_SMILES']\n",
    "    elements = selected_df['element'].tolist()\n",
    "    # Compare order of elements to SMILES string\n",
    "    #print(\"SMILES:\", smiles_toks)\n",
    "    #print(\"Elements:\", elements)\n",
    "    assert len(elements) == len(smiles_toks), f\"Length mismatch: {len(elements)} != {len(smiles_toks)}\"\n",
    "    # check through elements and SMILES letters\n",
    "    for elem, tok in zip(elements, smiles_toks):\n",
    "        #print(f\"Element: {elem}, SMILES token: {tok}\")\n",
    "        assert(tok[1] if tok.startswith(\"[\") else tok[0].lower() == elem[0].lower()), f\"Atom assignment failed: {tok} != {elem}\"\n",
    "    # You may want to implement a more sophisticated mapping here\n",
    "else:\n",
    "    print(\"Selected rows do not have the same SMILES. Something went wrong, these will not further be considered.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e17fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_groupedbySMILES = df.groupby(\"SMILES\")\n",
    "print(len(df_groupedbySMILES))\n",
    "valid_groups = []\n",
    "\n",
    "for smiles, group in df_groupedbySMILES:\n",
    "    try:\n",
    "        elements = group['element'].tolist()\n",
    "        smiles_toks = group.iloc[0]['cleaned_tokenized_SMILES']\n",
    "        # Compare order of elements to SMILES string\n",
    "        print(\"SMILES:\", smiles_toks)\n",
    "        print(\"Elements:\", elements)\n",
    "        assert len(elements) == len(smiles_toks), f\"Length mismatch: {len(elements)} != {len(smiles_toks)}\"\n",
    "        # check through elements and SMILES letters\n",
    "        for elem, tok in zip(elements, smiles_toks):\n",
    "            #print(f\"Element: {elem}, SMILES token: {tok}\")\n",
    "            assert(tok[1] if tok.startswith(\"[\") else tok[0].lower() == elem[0].lower()), f\"Atom assignment failed: {tok} != {elem}\"\n",
    "        #add to valid groups\n",
    "        \n",
    "    # You may want to implement a more sophisticated mapping here\n",
    "    except AssertionError as e:\n",
    "        print(f\"AssertionError: {e}. Skipping SMILES: {smiles}\")\n",
    "        continue\n",
    "else:\n",
    "    print(\"Selected rows do not have the same SMILES. Something went wrong, these will not further be considered.\")\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fairseq_git2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
