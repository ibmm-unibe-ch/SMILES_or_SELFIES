{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Saving embeddings of atom types for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import json\n",
    "import logging\n",
    "from typing import List\n",
    "from tokenisation import tokenize_dataset, get_tokenizer\n",
    "from pathlib import Path\n",
    "from fairseq_utils2 import compute_model_output, compute_model_output_RoBERTa, compute_random_model_output, load_dataset, load_model\n",
    "from fairseq.data import Dictionary\n",
    "from SMILES_to_SELFIES_mapping import canonize_smiles, generate_mapping, generate_mappings_for_task_SMILES_to_SELFIES\n",
    "from itertools import chain\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from constants import SEED\n",
    "import numpy as np\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "from rdkit import Chem\n",
    "\n",
    "from constants import (\n",
    "    TASK_PATH,\n",
    "    MOLNET_DIRECTORY,\n",
    "    TOKENIZER_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_atomtype_embedding_perelem_dict(filtered_dict, colordict, entry_name_atomtype_to_embedding):\n",
    "    print(\"in atomtype embedding per element rearrangement\")\n",
    "    atomtype_to_embedding_lists = [value[entry_name_atomtype_to_embedding] for value in filtered_dict.values() if entry_name_atomtype_to_embedding in value and value[entry_name_atomtype_to_embedding] is not None]\n",
    "    print(\"len atomtype to embedding list smiles: \",len(atomtype_to_embedding_lists))\n",
    "    print(\"atomtype to embedding list:\", atomtype_to_embedding_lists)\n",
    "    \n",
    "    # sort embeddings according to atomtype, I checked it visually and the mapping works\n",
    "    embeddings_by_atomtype = {}  # Dictionary to hold lists of embeddings for each atom type\n",
    "    #listembeddings = list()\n",
    "    for atom_type_list in atomtype_to_embedding_lists:\n",
    "        # go through single dictionary\n",
    "        for tuple in atom_type_list:\n",
    "           # print(f\"atomtype {atom_type} embeddings {embeddings[1]}\")\n",
    "            if tuple[0] not in embeddings_by_atomtype:\n",
    "                embeddings_by_atomtype[tuple[0]] = []\n",
    "            # extend the list of embeddings for this atom type(, but ONLY by the embedding not the attached token)\n",
    "            embeddings_by_atomtype[tuple[0]].append(tuple[1][0])\n",
    "            #print(\"\\ntuple01\",len(tuple[1][0]),tuple[1][0])\n",
    "            #print(len(embeddings[0]))\n",
    "    print(\"embeddings c\",len(embeddings_by_atomtype['c']))\n",
    "    \n",
    "    # sort dictionary that is mapping embeddings to atomtypes to elements so that e.g. all carbon atom types can be accessed at once in one list\n",
    "    #atom_types_repeated = []\n",
    "    #embeddings_list = []\n",
    "    atomtype_embedding_perelem_dict = dict()\n",
    "    ctr = 0\n",
    "    for key in colordict.keys():\n",
    "        print(f\"key {key}\")\n",
    "        for atype in colordict[key]:\n",
    "            print(atype) \n",
    "            if atype in embeddings_by_atomtype.keys():\n",
    "                embsofatype = embeddings_by_atomtype[atype]\n",
    "                atypes = [atype] * len(embeddings_by_atomtype[atype])\n",
    "                assert len(embsofatype) == len(atypes), \"Length of embeddings and atom types do not match.\"\n",
    "                if key not in atomtype_embedding_perelem_dict:\n",
    "                    atomtype_embedding_perelem_dict[key] = ([],[])\n",
    "                if key in atomtype_embedding_perelem_dict:\n",
    "                    atomtype_embedding_perelem_dict[key][0].extend(atypes)\n",
    "                    atomtype_embedding_perelem_dict[key][1].extend(embsofatype)\n",
    "    \n",
    "    print(atomtype_embedding_perelem_dict.keys())\n",
    "    return atomtype_embedding_perelem_dict\n",
    "\n",
    "def colorstoatomtypesbyelement(atomtoelems_dict):\n",
    "    \"\"\"Generating a dictionary of colors given a dictionary that maps atomtypes to elements\n",
    "\n",
    "    Args:\n",
    "        atomtoelems_dict (_dict_): Dictionary that maps atom types to elements\n",
    "\n",
    "    Returns:\n",
    "        _dict,: Dictionary that maps atom types to colors\n",
    "    \"\"\"\n",
    "    # https://sashamaps.net/docs/resources/20-colors/ #95% accessible only, subject to change, no white\n",
    "    sash = ['#e6194B', '#3cb44b', '#ffe119', '#4363d8', '#f58231', '#911eb4', '#42d4f4', '#f032e6', '#bfef45', '#fabed4',\n",
    "                   '#469990', '#dcbeff', '#9A6324', '#fffac8', '#800000', '#aaffc3', '#808000', '#ffd8b1', '#000075', '#a9a9a9', '#000000']\n",
    "    #viridis\n",
    "    viridis = plt.cm.get_cmap('viridis', 8)  # Get the Viridis colormap\n",
    "    viridis_colors = [mcolors.to_hex(viridis(i)) for i in range(8)]  # Convert to hex \n",
    "    # tol colorscale https://personal.sron.nl/~pault/ \n",
    "\n",
    "    #        purple, dark blue, light blue, green,    light green, yellow,     orange,      red \n",
    "    #tol = ['#882e72', '#1965B0', '#7bafde', '#4eb265', '#cae0ab', '#f7f056', '#f1932d', '#dc050c'] \n",
    "    # 9 10 14 15 17 18 26, but switched pos of bright blue with orange\n",
    "    #        purple, dark blue,   orange,    green, light green,     yellow,    red,     light blue --> in this order light blue should never be used\n",
    "    #tol = ['#882e72', '#1965B0', '#f1932d', '#4eb265', '#cae0ab', '#f7f056',   '#dc050c', '#7bafde'] \n",
    "    #        purple, dark blue,   orange,    green, light green,     dark blue again,    red,     light blue --> in this order light blue should never be used, dark blue used twice, but thats okay, never occurs twice together\n",
    "    # dark blue twice appearing is okay, because it never appears twice in a plot together\n",
    "    tol = ['#882e72', '#1965B0', '#f1932d', '#4eb265', '#cae0ab', '#1965B0',   '#dc050c', '#7bafde'] \n",
    "    # change to 9 10 skip 14 15 17 18 26\n",
    "    #tol = ['#882e72','#1965B0',  '#4eb265', '#cae0ab', '#f7f056', '#f1932d', '#dc050c']\n",
    "    \n",
    "    markers = [\"o\", \"s\", \"X\", \"D\", \"P\", \"^\", \"X\", \"v\",  \"P\", \"*\"]\n",
    "    \n",
    "    # create dict for colors\n",
    "    colornames=['sash', 'tol', 'viridis']\n",
    "    colors = [sash, tol, viridis_colors]\n",
    "    colordict = dict()\n",
    "\n",
    "    for colourname, colour in zip(colornames,colors):\n",
    "        colordict[colourname]={}\n",
    "        for key in atomtoelems_dict.keys():\n",
    "            atypes = atomtoelems_dict[key]\n",
    "            keycoldict=dict()\n",
    "            for at, col in zip(atypes, colour[0:len(atypes)]):\n",
    "                keycoldict[at]=col    \n",
    "            colordict[colourname][key]=keycoldict \n",
    "    print(colordict.items())\n",
    "    print(colordict['sash'])\n",
    "    \n",
    "    # create dict for markers\n",
    "    markerdict = dict()\n",
    "    for key in atomtoelems_dict.keys():\n",
    "            atypes = atomtoelems_dict[key]\n",
    "            markerdictdict=dict()\n",
    "            i=0\n",
    "            for at in atypes:\n",
    "                markerdictdict[at]=markers[i]\n",
    "                i+=1  \n",
    "            markerdict[key]=markerdictdict\n",
    "    print(markerdict.items())\n",
    "    \n",
    "    \n",
    "    \n",
    "    # now instead for each element, get colors for a combination of atomtypes\n",
    "    # p f cl o s\n",
    "    #key='p f cl o s'\n",
    "    #pfclos_types = atomtoelems_dict['p']+atomtoelems_dict['f']+atomtoelems_dict['cl']+atomtoelems_dict['o']+atomtoelems_dict['s']\n",
    "    #keycoldicti=dict()\n",
    "    #for at, col in zip(pfclos_types, colors_sash[0:len(pfclos_types)]):\n",
    "    #    keycoldicti[at]=col\n",
    "    #colordict[key]=keycoldicti \n",
    "    # c o\n",
    "    #key='c o'\n",
    "    #pfclos_types = atomtoelems_dict['c']+atomtoelems_dict['o']\n",
    "    #keycoldicti=dict()\n",
    "    #for at, col in zip(pfclos_types, colors_sash[0:len(pfclos_types)]):\n",
    "    #    keycoldicti[at]=col\n",
    "    #colordict[key]=keycoldicti \n",
    "    #print(colordict.keys())\n",
    "    #print(colordict.items())\n",
    "    return colordict, markerdict\n",
    "    \n",
    "\n",
    "def create_elementsubsets(atomtype_set):\n",
    "    \"\"\"Creation of element subsets according to alphabet\n",
    "\n",
    "    Args:\n",
    "        big_set (_set_): Set of atom types\n",
    "    Returns:\n",
    "        _list,dict[string][list[float],list[string]]_: List of keys (elements), dictionary that contains atomtypes sorted by element\n",
    "    \"\"\"\n",
    "    atomtype_set=sorted(atomtype_set)\n",
    "    element_dict = dict()\n",
    "    elements = list()\n",
    "    ctr=0\n",
    "    last_firstval = ''\n",
    "    for atype in atomtype_set:\n",
    "        if ctr==0:\n",
    "            last_firstval = atype[0]\n",
    "        if not atype.startswith('cl') and atype not in element_dict.items() and atype[0]==last_firstval:\n",
    "            #print(elements)\n",
    "            elements.append(atype)\n",
    "            element_dict[last_firstval] = elements\n",
    "        elif last_firstval != atype[0] and atype != 'cl' and atype != 'br':\n",
    "            element_dict[last_firstval] = elements\n",
    "            elements = list()\n",
    "            elements.append(atype)\n",
    "            last_firstval = atype[0]\n",
    "        ctr+=1\n",
    "    element_dict['cl']=['cl']\n",
    "    element_dict['br']=['br']\n",
    "    return element_dict\n",
    "\n",
    "def map_selfies_embeddings_to_smiles(embeds_selfies, smiles_to_selfies_mapping, dikt):\n",
    "    \"\"\"Map  clean SELFIES embeddings to their corresponding SMILES and atomtypes\n",
    "    Args:\n",
    "        embeds_selfies (_list_): List of lists of SELFIES embeddings\n",
    "        smiles_to_selfies_mapping (_dict_): Dictionary that maps SMILES to SELFIES and SELFIES tokens to SMILES tokens (mappings[smiles]['selfiesstr_tok_map'] = (selfies_str,tokenised_selfies,mapping))\n",
    "        dikt (_dict_): Dictionary of atom mappings etc\n",
    "    Returns:\n",
    "        adds SELFIES embeddings to atomtype mappings to dikt\n",
    "    \"\"\"\n",
    "    # get embeddings for SELFIES that have a mapping to SMILES and map to SMILES in smiles_to_selfies_mapping\n",
    "    for emb, smiles in zip(embeds_selfies[0], smiles_to_selfies_mapping.keys()):\n",
    "        # Check if the mapping for the current smiles has a non-None value at index 2 for mapping of SELFIES to SMILES\n",
    "        if smiles_to_selfies_mapping[smiles]['selfiesstr_tok_map'][2] is not None:\n",
    "            # If so, set 'selfies_emb' to emb, otherwise set it to None\n",
    "            smiles_to_selfies_mapping[smiles].setdefault(\"raw_selfies_emb\", emb)\n",
    "        else:\n",
    "            smiles_to_selfies_mapping[smiles].setdefault(\"raw_selfies_emb\", None)\n",
    "\n",
    "    print(\"within\", len(dikt.keys()))\n",
    "    for key,val in dikt.items():\n",
    "        #print(\"smiles:\",key, val['atomtype_to_embedding'][0])\n",
    "        if key in smiles_to_selfies_mapping.keys():\n",
    "            # get list with positions to keep from dikt\n",
    "            #if assignment failed posToKeep will be empty, then there is no need to map anything\n",
    "            posToKeep = dikt[key][\"posToKeep\"]\n",
    "            if posToKeep is not None:\n",
    "               # print(\"selfies:\", smiles_to_selfies_mapping[key]['selfiesstr_tok_map'][0], smiles_to_selfies_mapping[key]['selfiesstr_tok_map'][1])\n",
    "                # if mapping exists\n",
    "                if smiles_to_selfies_mapping[key]['selfiesstr_tok_map'][2] is not None and smiles_to_selfies_mapping[key]['raw_selfies_emb'] is not None:\n",
    "                   # print(\"key:\",key)\n",
    "                   # print(\"1111111111: \",smiles_to_selfies_mapping[key]['selfiesstr_tok_map'][1])\n",
    "                   # print(\"1111111111: \",smiles_to_selfies_mapping[key]['selfiesstr_tok_map'][2].keys())\n",
    "                    # 1 atom mappings and number of embeddings do not have to be even because branch, ring and overloaded tokens cannot be mapped to tokens in canonized SMILES\n",
    "                    # 1 keep only the mebeddings that have a mapping\n",
    "                    embs_with_mapping = []\n",
    "                    for x, val in smiles_to_selfies_mapping[key]['selfiesstr_tok_map'][2].items():\n",
    "                        token_id = x[0]\n",
    "                        #print(\"\\ttoken:\",x[1])\n",
    "                        #print(\"\\ttoken id:\",x[0]) \n",
    "                        #print(\"\\t in embedding: \",smiles_to_selfies_mapping[key]['raw_selfies_emb'][token_id][1])\n",
    "                        #print()\n",
    "                        #print(\"maps to smiles id: \",val)\n",
    "                        #print()\n",
    "                        assert x[1]==smiles_to_selfies_mapping[key]['raw_selfies_emb'][token_id][1], f\"Token {x[1]} does not match token {smiles_to_selfies_mapping[key]['raw_selfies_emb'][token_id][1]}\"\n",
    "                        embs_with_mapping.append((val, smiles_to_selfies_mapping[key]['raw_selfies_emb'][token_id]))\n",
    "                        #embs_with_mapping.append(smiles_to_selfies_mapping[key]['raw_selfies_emb'][token_id])\n",
    "                    # 2 resort embeddings according to their position in the SMILES string\n",
    "                    embs_with_mapping = sorted(embs_with_mapping, key=lambda item: item[0])\n",
    "                    #print(\"sorted \", [key for key, _ in embs_with_mapping])\n",
    "                   ## for _, value in embs_with_mapping:\n",
    "                     #   print(value[1])\n",
    "                    # 3 only keep embeddings with smiles id that belong to id that is in posToKeepList\n",
    "                    filtered_embs = [(key, value) for key, value in embs_with_mapping if key in posToKeep]\n",
    "                    # 4 assert that the length of the filtered embeddings is the same as the length of the posToKeep list\n",
    "                    assert len(filtered_embs) == len(posToKeep), f\"Length of filtered embeddings {len(filtered_embs)} and posToKeep list {len(posToKeep)} do not agree.\"\n",
    "                    # 5 map the filtered embeddings to the atom types\n",
    "                    atomtypes= dikt[key]['atom_types']\n",
    "                    assert len(atomtypes) == len(filtered_embs), f\"Length of atom types {len(atomtypes)} and filtered embeddings {len(filtered_embs)} do not agree.\"\n",
    "                    atomtypes_to_selfies_embs = []\n",
    "                    for atomtype, emb in zip(atomtypes, filtered_embs):\n",
    "                        atomtypes_to_selfies_embs.append((atomtype, emb[1]))  \n",
    "                        # assert letters of atomtype and token of embedding match\n",
    "                        # checked visually, looks good\n",
    "                        #print(\"----------------------------------------------------\")\n",
    "                        #print(f\"atomtype {atomtype} emb {emb[1][1]}\")\n",
    "                        \n",
    "                    # 6 attach this dictionary with name 'atomtype_to_clean_selfies_embedding' to the dikt\n",
    "                    dikt[key].setdefault(\"atomtype_to_clean_selfies_embedding\", atomtypes_to_selfies_embs) \n",
    "                else:\n",
    "                    dikt[key].setdefault(\"atomtype_to_clean_selfies_embedding\", None)\n",
    "            else:\n",
    "                dikt[key].setdefault(\"atomtype_to_clean_selfies_embedding\", None)\n",
    "        else:\n",
    "            dikt[key].setdefault(\"atomtype_to_clean_selfies_embedding\", None)    \n",
    "\n",
    "def map_embeddings_to_atomtypes(dikt,task_SMILES):\n",
    "    for SMILES in task_SMILES:\n",
    "        if dikt[SMILES][\"posToKeep\"] is not None:\n",
    "            atomtype_to_embedding = {}\n",
    "            atom_types = dikt[SMILES]['atom_types']\n",
    "            embeddings = dikt[SMILES]['clean_embedding']\n",
    "            type_to_emb_tuples_list = list()\n",
    "            for atom_type, embedding in zip(atom_types, embeddings):\n",
    "                type_to_emb_tuples_list.append((atom_type, embedding))\n",
    "                #atomtype_to_embedding.setdefault(atom_type, []).append(embedding)\n",
    "                #type_to_emb_dict[atom_type] = embedding\n",
    "                # assert to check whether atom type is the same as the first letter of the embedding\n",
    "                assert(atom_type.lower() if atom_type.lower() =='cl' or atom_type.lower() =='br' else atom_type[0].lower()==(embedding[1][1].lower() if embedding[1].startswith(\"[\") else embedding[1]).lower()), f\"Atom assignment failed: {atom_type} != {embedding[1]}\"\n",
    "            dikt[SMILES][\"atomtype_to_embedding\"] = type_to_emb_tuples_list\n",
    "        else:\n",
    "            dikt[SMILES][\"atomtype_to_embedding\"]= (None,None)\n",
    "    logging.info(\"Embeddings mapped to atom types, all checks passed\")\n",
    "\n",
    "def get_clean_embeds(embeds, dikt, creation_assignment_fails, task_SMILES):\n",
    "    \"\"\"Clean embeddings of embeddings that encode for digits, hydrogens, or structural tokens\n",
    "\n",
    "    Args:\n",
    "        embeds (_List[List[float]_): Embeddings of a SMILES\n",
    "        failedSmiPos (_list_): Positions of SMILES in list where no file and/or assignment could be generated\n",
    "        posToKeep_list (_list_): List of positions in a SMILES according to tokens that need to be kept (not digits, hydrogens, or structural tokens)\n",
    "\n",
    "    Returns:\n",
    "        _list[float]_: Embeddings that do not encode hydrogens, digits, or structural tokens, but only atoms\n",
    "    \"\"\"\n",
    "    # some sanity checks on embeddings per SMILES\n",
    "    assert (len(dikt.keys())) == (len(\n",
    "        embeds[0])), f\"Number of SMILES and embeddings do not agree. Number of SMILES: {len(dikt.keys())} of which {creation_assignment_fails} failures and Number of embeddings: {len(embeds[0])}\"\n",
    "    print(f\"Number of SMILES: {len(dikt.keys())} with {creation_assignment_fails} failures and Number of embeddings: {len(embeds[0])}\")\n",
    "    \n",
    "    none_embeddings = sum([1 for emb in embeds[0] if emb is None])\n",
    "    print(\"Sum of NONE embeddings:\",none_embeddings)\n",
    "    \n",
    "    none_sth =0\n",
    "    #only keep embeddings for SMILES where atoms could be assigned to types\n",
    "    embeds_clean = list()\n",
    "    for smi, emb in zip(task_SMILES, embeds[0]):\n",
    "        print(\"checking pos to keep\")\n",
    "        if smi not in dikt:\n",
    "            print(f\"SMILES {smi} not in dictionary\")\n",
    "            continue\n",
    "        posToKeep = dikt[smi][\"posToKeep\"]\n",
    "        # new: embeddings can be none too\n",
    "        if posToKeep is not None and emb is not None:\n",
    "            embeds_clean.append(emb)\n",
    "            dikt[smi][\"orig_embedding\"]=emb\n",
    "        else:\n",
    "            dikt[smi][\"orig_embedding\"]=None\n",
    "            none_sth+=1\n",
    "    \n",
    "    logging.info(\n",
    "        f\"Length embeddings before removal: {len(embeds[0])}, after removal where atom assignment failed or embedding is None: {len(embeds_clean)}\")\n",
    "    creation_assignment_fails_AND_none_embeddings =creation_assignment_fails+none_embeddings\n",
    "    numberdel_embs = (len(embeds[0])-len(embeds_clean))\n",
    "    assert none_sth == (len(\n",
    "        embeds[0])-len(embeds_clean)), f\"Assignment fails ({creation_assignment_fails}) plus none embeddings {none_embeddings} (leads to: {none_sth}) and number of deleted embeddings do not agree ({numberdel_embs}).\"\n",
    "\n",
    "    embeds_cleaner = []\n",
    "    #assert len(embeds_clean) == (len([item for item in posToKeep_list if item is not None])\n",
    "     #                            ), f\"Not the same amount of embeddings as assigned SMILES. {len(embeds_clean)} embeddings vs. {len([item for item in posToKeep_list if item is not None])} SMILES with positions\"\n",
    "    # only keep embeddings that belong to atoms\n",
    "    for SMILES in task_SMILES:\n",
    "        poslist = dikt[SMILES][\"posToKeep\"]\n",
    "        emb_clean = dikt[SMILES][\"orig_embedding\"]\n",
    "\n",
    "        if poslist is not None and emb_clean is not None:\n",
    "            newembsforsmi = []\n",
    "            newembsforsmi = [emb_clean[pos] for pos in poslist]\n",
    "            embeds_cleaner.append(newembsforsmi)\n",
    "            dikt[SMILES][\"clean_embedding\"]=newembsforsmi  \n",
    "        else:\n",
    "            # if original embeddings is None, make clean embedding None too\n",
    "            dikt[SMILES][\"clean_embedding\"]=None   \n",
    "            # also set posToKeep to None\n",
    "            dikt[SMILES][\"posToKeep\"]=None\n",
    "\n",
    "    # sanity check that length of embeddings to keep is the same as length of embeddings to keep\n",
    "    posToKeep_list = [value[\"posToKeep\"] for value in dikt.values() if value[\"posToKeep\"] is not None]\n",
    "    # sanity check that the lengths agree\n",
    "    for smiemb, pos_list in zip(embeds_cleaner, posToKeep_list):\n",
    "        assert len(smiemb) == len(\n",
    "            pos_list), \"Final selected embeddings for assigned atoms do not have same length as list of assigned atoms.\"\n",
    "        #print(len(smiemb), pos_list)\n",
    "        \n",
    "    # sanity check that length of assigned atoms map to length of clean embeddings\n",
    "    for SMILES in task_SMILES:\n",
    "        smi_clean=dikt[SMILES][\"smi_clean\"]\n",
    "        emb_clean = dikt[SMILES][\"clean_embedding\"]\n",
    "        if dikt[SMILES][\"posToKeep\"] is not None and emb_clean is not None:\n",
    "            assert len(smi_clean) == len(\n",
    "                emb_clean), \"SMILES and embeddings do not have same length.\"\n",
    "            for sm, em in zip(smi_clean,emb_clean):\n",
    "                if sm.upper() == \"C\":\n",
    "                    assert(sm.upper()==em[1]), f\"Atom assignment failed: {sm} != {em[1]}\"\n",
    "                else:\n",
    "                #print(f\"sm {sm} em {em[1]}\")\n",
    "                    assert(sm==em[1]), f\"Atom assignment failed: {sm} != {em[1]}\"\n",
    "    logging.info(\"Cleaning embeddings finished, all checks passed\")\n",
    "    return embeds_cleaner\n",
    "\n",
    "def check_lengths(smi_toks, embeds):\n",
    "    \"\"\"Check that number of tokens corresponds to number of embeddings per SMILES, otherwise sth went wrong\n",
    "     new: if sth went wrong turn that embedding to None and return the embeddings\n",
    "\n",
    "    Args:\n",
    "        smi_toks (_list[string]_): SMILES tokens for a SMILES\n",
    "        embeds (_list[float]_): Embeddings\n",
    "    \"\"\"\n",
    "    samenums = 0\n",
    "    diffnums = 0\n",
    "    smismaller = 0\n",
    "    new_embs = list()\n",
    "    for smi, embs in zip(smi_toks, embeds[0]):\n",
    "        # only compare when both are not None)\n",
    "        if embs is not None and smi is not None:\n",
    "            if len(smi) == len(embs):\n",
    "                samenums += 1\n",
    "                new_embs.append(embs)\n",
    "            else:\n",
    "                print(f\"smilen: {len(smi)} emblen: {len(embs)}\")\n",
    "                embs_signs = [emb1 for (emb0,emb1) in embs]\n",
    "                print(f\"smi: {smi} \\nemb: {embs_signs} \\nwith len diff {len(smi)-len(embs)}\")\n",
    "                diffnums += 1\n",
    "                new_embs.append(None)\n",
    "                if len(smi) < len(embs):\n",
    "                    smismaller += 1\n",
    "    embeds[0]=new_embs\n",
    "    if diffnums == 0:\n",
    "        return embeds\n",
    "    else:\n",
    "        print(\n",
    "            f\"same numbers between tokens and embeddings: {samenums} and different number betqween tokens and embeddings: {diffnums} of which smiles tokens have smaller length: {smismaller}\")\n",
    "        perc = (diffnums/(diffnums+samenums))*100\n",
    "        print(\n",
    "            \"percentage of embeddings not correct compared to smiles: {:.2f}%\".format(perc))\n",
    "        return embeds\n",
    "\n",
    "def get_embeddings(task: str, specific_model_path: str, data_path: str, cuda: int, task_reps: List[str]):\n",
    "    \"\"\"Generate the embeddings dict of a task\n",
    "    Args:\n",
    "        task (str): Task to find attention of\n",
    "        cuda (int): CUDA device to use\n",
    "    Returns:\n",
    "        Tuple[List[List[float]], np.ndarray]: attention, labels\n",
    "    \"\"\"\n",
    "    #task_SMILES, task_labels = load_molnet_test_set(task)\n",
    "    print(\"in get embeddings\")\n",
    "    # Ensure specific_model_path and data_path are not None\n",
    "    if specific_model_path is None:\n",
    "        raise ValueError(\"specific_model_path cannot be None\")\n",
    "    if data_path is None:\n",
    "        raise ValueError(\"data_path cannot be None\")\n",
    "    \n",
    "    #data_path = \"/data/jgut/SMILES_or_SELFIES/task/delaney/smiles_atom_isomers\"\n",
    "    if \"random\" not in str(specific_model_path):\n",
    "            model = load_model(specific_model_path, data_path, cuda)\n",
    "            print(\"model loaded\")\n",
    "            model.zero_grad()\n",
    "    data_path = data_path / \"input0\" / \"test\"\n",
    "    # True for classification, false for regression\n",
    "    dataset = load_dataset(data_path, True)\n",
    "    source_dictionary = Dictionary.load(str(data_path.parent / \"dict.txt\"))\n",
    "\n",
    "    assert len(task_reps) == len(\n",
    "        dataset\n",
    "    ), f\"Real and filtered dataset {task} do not have same length: len(task_reps): {len(task_reps)} vs. len(dataset):{len(dataset)} .\"\n",
    "    \n",
    "\n",
    "    #text = [canonize_smile(smile) for smile in task_SMILES]\n",
    "    text = [rep for rep in task_reps]\n",
    "    embeds= []\n",
    "    tokenizer = None\n",
    "    if \"bart\" in str(specific_model_path):\n",
    "        if \"random\" in str(specific_model_path):\n",
    "            #mol_dataset_path=\"/data/jgut/SMILES_or_SELFIES/embedding_mapping_random/train\"\n",
    "            fairseq_dict_path = \"/data/jgut/SMILES_or_SELFIES/fairseq_preprocess/smiles_atom_isomers/dict.txt\"\n",
    "            ##dataset = load_dataset(data_path, True)\n",
    "            source_dictionary = Dictionary.load(fairseq_dict_path)\n",
    "            pretrained_TASK_MODEL_PATH = Path(\"/data/jgut/SMILES_or_SELFIES/prediction_models\")\n",
    "            specific_model_path = pretrained_TASK_MODEL_PATH / \"random_bart\" / \"checkpoint_last.pt\"\n",
    "            data_path = TASK_PATH / \"bbbp\" / \"smiles_atom_isomers\"\n",
    "            model = load_model(specific_model_path, data_path, \"3\")\n",
    "            model.zero_grad()\n",
    "            print(\"got the model\")\n",
    "            embeds.append(\n",
    "                compute_model_output(\n",
    "                    dataset,\n",
    "                    model,\n",
    "                    text, #this is very important to be in same order as task_SMILES which it is\n",
    "                    source_dictionary,\n",
    "                    False,\n",
    "                    True,  # true for embeddings\n",
    "                    True,  # true for eos_embeddings\n",
    "                    tokenizer,\n",
    "                )[2]\n",
    "            )\n",
    "        else:\n",
    "            embeds.append(\n",
    "                compute_model_output(\n",
    "                    dataset,\n",
    "                    model,\n",
    "                    text, #this is very important to be in same order as task_SMILES which it is\n",
    "                    source_dictionary,\n",
    "                    False,\n",
    "                    False,\n",
    "                    True,  # true for embeddings\n",
    "                    True,  # true for eos_embeddings\n",
    "                    tokenizer,\n",
    "                )[2]\n",
    "            )\n",
    "    if \"roberta\" in str(specific_model_path):\n",
    "        embeds.append(\n",
    "            compute_model_output_RoBERTa(\n",
    "                dataset,\n",
    "                model,\n",
    "                text,\n",
    "                source_dictionary,\n",
    "                False,\n",
    "                False,\n",
    "                True,  # true for embeddings\n",
    "                True,  # true for eos_embeddings\n",
    "                tokenizer,\n",
    "            )[2]\n",
    "        )\n",
    "   # print(\"attention encodings\",len(attention_encodings[0]))\n",
    "   # print(len(attention_encodings))\n",
    "    #output = list(zip(*embeds))\n",
    "    #labels = np.array(task_labels).transpose()[0]\n",
    "    # print(\"labels\",labels)\n",
    "    # print(len(labels))\n",
    "    return embeds\n",
    "\n",
    "def get_embeddings_from_model(task, traintype, model, rep, reps, listoftokenisedreps):\n",
    "    # ----------------------specific model paths for Delaney for BART and RoBERTa-------------------------\n",
    "    finetuned_TASK_MODEL_PATH = Path(\"/data2/jgut/SoS_models\")\n",
    "    pretrained_TASK_MODEL_PATH = Path(\"/data/jgut/SMILES_or_SELFIES/prediction_models\")\n",
    "    \n",
    "    if model==\"random\":\n",
    "        specific_model_path = pretrained_TASK_MODEL_PATH / \"random_bart\" / \"checkpoint_last.pt\"\n",
    "        mol_dataset_path = Path(\"/data/jgut/SMILES_or_SELFIES/embedding_mapping_random/train\")\n",
    "        rep=\"smiles\"\n",
    "        subfolder = \"smiles_atom_isomers\"\n",
    "    # path to finetuned models\n",
    "    subfolder=\"\"\n",
    "    if rep==\"smiles\":\n",
    "        #subfolder = \"smiles_atom_isomers\"\n",
    "        subfolder = \"smiles_atom_standard\"\n",
    "    elif rep==\"selfies\":\n",
    "        #subfolder=\"selfies_atom_isomers\"\n",
    "        subfolder=\"selfies_atom_standard\"\n",
    "        \n",
    "    if model!=\"random\":\n",
    "        if traintype==\"finetuned\":\n",
    "            if model==\"BART\":\n",
    "                # path for BART  \n",
    "                specific_model_path = (\n",
    "                finetuned_TASK_MODEL_PATH\n",
    "                / task\n",
    "                / f\"{subfolder}_bart\"\n",
    "                / \"1e-05_0.2_seed_0\" \n",
    "                / \"checkpoint_best.pt\"\n",
    "                )\n",
    "            else:\n",
    "                if rep=='selfies':\n",
    "                    #path for RoBERTa\n",
    "                    specific_model_path = (\n",
    "                        finetuned_TASK_MODEL_PATH\n",
    "                        / task\n",
    "                        / f\"{subfolder}_roberta\"\n",
    "                        / \"5e-06_0.2_seed_0\" \n",
    "                        / \"checkpoint_best.pt\"\n",
    "                    )\n",
    "                else:\n",
    "                    #path for RoBERTa\n",
    "                    specific_model_path = (\n",
    "                        finetuned_TASK_MODEL_PATH\n",
    "                        / task\n",
    "                        / f\"{subfolder}_roberta\"\n",
    "                        / \"1e-05_0.2_seed_0\" \n",
    "                        / \"checkpoint_best.pt\"\n",
    "                    )\n",
    "        # ----------------------specific model paths for pretrained models of BART and RoBERTa-------------------------\n",
    "        elif traintype==\"pretrained\":\n",
    "            if model==\"BART\":\n",
    "                # path for BART   \n",
    "                specific_model_path = (\n",
    "                    pretrained_TASK_MODEL_PATH\n",
    "                    / f\"{subfolder}_bart\"\n",
    "                    / \"checkpoint_last.pt\"\n",
    "                ) \n",
    "            else:\n",
    "                #path for RoBERTa\n",
    "                specific_model_path = (\n",
    "                pretrained_TASK_MODEL_PATH\n",
    "                / f\"{subfolder}_roberta\"\n",
    "                / \"checkpoint_last.pt\"\n",
    "                )\n",
    "    print(\"specific model path: \",specific_model_path)\n",
    "    \n",
    "    if specific_model_path is None:\n",
    "        raise ValueError(\"specific_model_path cannot be None\")\n",
    "    data_path = TASK_PATH / task / f\"{subfolder}_big_c\"\n",
    "    \n",
    "    #data_path = TASK_PATH/\"bbbp\"/f\"{subfolder}\"\n",
    "    #fairseq_dict = Dictionary.load(str(fairseq_dict_path))\n",
    "    #fairseq_dict_path = FAIRSEQ_PREPROCESS_PATH/ \"smiles_atom_isomers\"/\"dict.txt\"\n",
    "    \n",
    "    embeds = []\n",
    "    if model==\"random\":\n",
    "        #data_path = TASK_PATH / \"bbbp\" / \"smiles_atom_isomers\"\n",
    "        #mol_dataset_path=\"/data/jgut/SMILES_or_SELFIES/embedding_mapping_random/train\"\n",
    "        #embeds = get_embeddings(task, specific_model_path, mol_dataset_path, False, reps)\n",
    "        #fairseq_dict_path = \"/data/jgut/SMILES_or_SELFIES/fairseq_preprocess/smiles_atom_isomers/dict.txt\"\n",
    "        #dataset = load_dataset(data_path, True)\n",
    "        #fairseq_dict = Dictionary.load(fairseq_dict_path)\n",
    "        #model = load_model(specific_model_path, data_path, \"3\")\n",
    "        #embeds=compute_random_model_output(model, mol_dataset_path, fairseq_dict, \"3\")\n",
    "        embeds = get_embeddings(task, specific_model_path, data_path, False, reps) \n",
    "        #embeds = compute_random_model_output(model, data_path, fairseq_dict)\n",
    "    else:\n",
    "        embeds = get_embeddings(task, specific_model_path, data_path, False, reps) #works for BART model with newest version of fairseq on github, see fairseq_git.yaml file\n",
    "    checked_embeds = check_lengths(listoftokenisedreps, embeds) #, \"Length of SMILES_tokens and embeddings do not agree.\"\n",
    "    print(\"got the embeddings\")\n",
    "    return checked_embeds\n",
    "\n",
    "\n",
    "def get_tokenized_SMILES(task_SMILES: List[str]):\n",
    "    \"\"\"Tokenize SMILES string\n",
    "\n",
    "    Args:\n",
    "        input_list of strings (str): List of SMILES input string\n",
    "\n",
    "    Returns:\n",
    "        dict: dictionary that links canonize SMILES string\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer = get_tokenizer(TOKENIZER_PATH)\n",
    "    print(f\"tokenizer {tokenizer}\")\n",
    "    smi_toks = tokenize_dataset(tokenizer, task_SMILES, False)\n",
    "    smi_toks = [smi_tok.split() for smi_tok in smi_toks]\n",
    "    print(f\"SMILES tokens: {smi_toks[0]}\")\n",
    "    smiles_dict = dict(zip(task_SMILES,smi_toks))\n",
    "    return smiles_dict\n",
    "\n",
    "def load_dictsandinfo_from_jsonfolder(input_folder):\n",
    "    \"\"\"\n",
    "    Load atom assignments and info on failed assignments from folder that contains dictionaries on antechamber atom assignments and info files on failed assignments\n",
    "    :param input_folder: folder that contains atom assignments and info on failed assignments\n",
    "    :return: dict of tasks with dictionary with atom assignments, total number of failed assignments, list of failed SMILES and positions that failed, list of positions that should be kept\n",
    "    \"\"\"\n",
    "    task_dikt = {}\n",
    "    task_totalfails = {}\n",
    "    for file in os.listdir(input_folder):\n",
    "        if file.endswith(\".json\"):\n",
    "            if file.startswith(\"dikt\"):\n",
    "                task = file.split(\"_\")[1].split(\".\")[0]\n",
    "                if task==\"bace\":\n",
    "                    task = \"bace classification\"\n",
    "                with open(os.path.join(input_folder, file), 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                    task_dikt[task] = data\n",
    "                    #totalfails += data['totalfails']\n",
    "                    #failedSmiPos.extend(data['failedSmiPos'])\n",
    "                    #posToKeep_list.extend(data['posToKeep'])\n",
    "            elif file.startswith(\"assignment_info\"):\n",
    "                task=file.split(\".\")[0].split(\"_\")[2]\n",
    "                if task==\"bace\":\n",
    "                    task = \"bace classification\"\n",
    "                with open(os.path.join(input_folder, file), 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                    task_totalfails[task] = data\n",
    "                    #failedSmiPos.extend(data['failedSmiPos'])\n",
    "                    #posToKeep_list.extend(data['posToKeep'])\n",
    "                \n",
    "    return task_dikt, task_totalfails    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_merged_dikt(model, traintype):\n",
    "        # get atom assignments from folder that contains antechamber atom assignments and info files on failed assignments\n",
    "    input_folder = \"./assignment_dicts/kekulized/\"\n",
    "    task_dikt, task_totalfails = load_dictsandinfo_from_jsonfolder(input_folder)\n",
    "    #currently available processed tasks \n",
    "    #task = \"delaney\" --> regression\n",
    "    #task = \"bace_classification\" --> only fails\n",
    "    #task=\"bbbp\" --> classification\n",
    "    #task=\"clearance\" --> regression\n",
    "    onlyworkingtasks=[\"delaney\", \"clearance\", \"bbbp\",\"lipo\"]\n",
    "    test_tasks=[\"delaney\"]\n",
    "    \n",
    "\n",
    "    merged_dikt = {}\n",
    "    #print(f\"totalfails: {totalfails} and total assigned molecules: {len(dikt.keys())}\")\n",
    "    for key, val in task_dikt.items():\n",
    "        print(\"================================TASK: \",key)\n",
    "        task=key\n",
    "        if task not in onlyworkingtasks:\n",
    "            continue\n",
    "        #print(f\"SMILES task: {key} \\nwith dict_keys {val.keys()}\")\n",
    "        #print(task_dikt[key].keys())\n",
    "        dikt=task_dikt[key]\n",
    "        totalfails = task_totalfails[key]['totalfails']\n",
    "        task_SMILES = dikt.keys()\n",
    "        task_SMILES = [Chem.MolToSmiles(Chem.MolFromSmiles(smi), kekuleSmiles=True) for smi in task_SMILES]\n",
    "        \n",
    "        # have to tokenize SMILES just like previously in e.g. 2_AssignEmbedsPlot.py\n",
    "        smiles_dict = get_tokenized_SMILES(task_SMILES)\n",
    "        \n",
    "        #for key2, val2 in val.items():\n",
    "        #    print(f\"{key2}: {val2}\")\n",
    "        percentagefailures = (totalfails/len(dikt.keys()))*100\n",
    "        print(f\"total fails for task {task}: {totalfails} out of {len(dikt.keys())} SMILES ({percentagefailures:.2f}%) \")\n",
    "        \n",
    "        #get embeddings from model\n",
    "        #model = \"ROBERTA\"\n",
    "        #model=\"BART\"\n",
    "        #traintype = \"pretrained\"\n",
    "        #rep = \"smiles\"\n",
    "        # task needs specifiyng for loading of finetuned model\n",
    "        ########################## Get embeddings from model for SMILES ############################################\n",
    "        try:\n",
    "            rep=\"smiles\"\n",
    "            print(f\"get embeddings for {model} and rep {rep}\")\n",
    "            embeds = get_embeddings_from_model(task, traintype, model, rep, smiles_dict.keys(), smiles_dict.values())\n",
    "            #get rid of embeddings that encode for digits or hydrogens\n",
    "            embeds_clean = get_clean_embeds(embeds, dikt, totalfails, task_SMILES)\n",
    "            print(\"-----------------------dikt with embeds: \",dikt.keys())\n",
    "            # within the dikt, map embeddings to atom types\n",
    "            map_embeddings_to_atomtypes(dikt,task_SMILES)\n",
    "            print()\n",
    "            \n",
    "            ## SELFIES------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "            # get SELFIES equivalent of SMILES and mapping between them\n",
    "            #smiles_to_selfies_mapping = generate_mappings_for_task_SMILES_to_SELFIES(task_SMILES)\n",
    "           # \n",
    "            #selfies_tokenised = []\n",
    "            #selfies = []\n",
    "            #maps_num = 0\n",
    "            #for key in smiles_to_selfies_mapping.keys():\n",
    "            #    print(f\"SMILES: {key} SELFIES: {smiles_to_selfies_mapping[key]}\")\n",
    "            #    selfies_tokenised.append(smiles_to_selfies_mapping[key]['selfiesstr_tok_map'][1])\n",
    "            #    selfies.append(smiles_to_selfies_mapping[key]['selfiesstr_tok_map'][0])\n",
    "            #    if smiles_to_selfies_mapping[key]['selfiesstr_tok_map'][2] is not None:\n",
    "            #        maps_num +=1\n",
    "            #        for key2,val in smiles_to_selfies_mapping[key]['selfiesstr_tok_map'][2].items():\n",
    "            ##            print(f\"SELFIES index:{key2[0]} with token:{key2[1]}\\tmaps to SMILES token at pos: {val}\")\n",
    "            #    print()\n",
    "                \n",
    "            #print(f\"list of tokenised selfies: {selfies_tokenised}\")\n",
    "            #print(f\"selfies {selfies} \\nwith len() {len(selfies)}\")\n",
    "            #print(f\"mappings {maps_num}\")\n",
    "            \n",
    "            #rep=\"selfies\"\n",
    "            #print(f\"get embeddings for {model} and rep {rep}\")\n",
    "            # traintype and model speicfied above\n",
    "            #embeds_selfies = get_embeddings_from_model(task, traintype, model, rep, selfies, selfies_tokenised)\n",
    "            \n",
    "            # map selfies embeddings to smiles in smiles_dict\n",
    "            #map_selfies_embeddings_to_smiles(embeds_selfies, smiles_to_selfies_mapping, dikt)\n",
    "            #for key, val in dikt.items():\n",
    "            #    dikt[key]['task']=task\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            continue\n",
    "        merged_dikt.update(dikt)\n",
    "        \n",
    "    print(len(merged_dikt.keys()))\n",
    "    print(merged_dikt.keys())\n",
    "    valid_keys_count = len([key for key in merged_dikt.keys() if merged_dikt[key]['posToKeep'] is not None and merged_dikt[key]['atomtype_to_embedding'] is not None])\n",
    "    print(\"==============================================================================================================================================\")\n",
    "    print(f\"Number of valid keys in final merged_dikt: {valid_keys_count}\")\n",
    "    return merged_dikt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limit_embeddings_SMILESonly(atomtype_dict_smiles, element, atom_type, limit):\n",
    "    # Extract the atom type names and embeddings for the specific element\n",
    "    smiles_atomtypes = atomtype_dict_smiles[element][0]\n",
    "    smiles_embeddings = atomtype_dict_smiles[element][1]\n",
    "    \n",
    "    # Ensure both lists have the same length\n",
    "    assert len(smiles_atomtypes) == len(smiles_embeddings), \"Mismatch in the number of atom types and embeddings in SMILES\"\n",
    "    \n",
    "    # Find indices of the specified atom type\n",
    "    indices = [i for i, atype in enumerate(smiles_atomtypes) if atype == atom_type]\n",
    "    \n",
    "    # Limit the number of indices to the specified limit\n",
    "    limited_indices = indices[:limit]\n",
    "    \n",
    "    # Create new lists to store the limited atom types and embeddings\n",
    "    new_smiles_atomtypes = []\n",
    "    new_smiles_embeddings = []\n",
    "    \n",
    "    # Add all atom types and embeddings, limiting the specified atom type\n",
    "    count = 0\n",
    "    for i in range(len(smiles_atomtypes)):\n",
    "        if smiles_atomtypes[i] == atom_type:\n",
    "            if count < limit:\n",
    "                new_smiles_atomtypes.append(smiles_atomtypes[i])\n",
    "                new_smiles_embeddings.append(smiles_embeddings[i])\n",
    "                count += 1\n",
    "        else:\n",
    "            new_smiles_atomtypes.append(smiles_atomtypes[i])\n",
    "            new_smiles_embeddings.append(smiles_embeddings[i])\n",
    "    \n",
    "    # Update the dictionaries with the new lists by creating new tuples, tuples are immutable, so need to create anew\n",
    "    atomtype_dict_smiles[element] = (new_smiles_atomtypes, new_smiles_embeddings)\n",
    "    assert len(atomtype_dict_smiles[element][0]) == len(atomtype_dict_smiles[element][1]), \"Mismatch in the number of atom types and embeddings in SMILES\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limit_all_atomtypes_to_threshold_SMILESonly(limit, atomtype_embedding_perelem_dict_smiles_bart, atomtype_embedding_perelem_dict_smiles_roberta, atomtype_embedding_perelem_dict_smiles_random):\n",
    "    # limit embeddings for BART\n",
    "    print(\"BART number embeddings for ca and c3 before limiting\")\n",
    "    print(f\"Number of embeddings for 'ca' in SMILES: {len([atype for atype in atomtype_embedding_perelem_dict_smiles_bart['c'][0] if atype == 'ca'])}\")\n",
    "    #print(f\"Number of embeddings for 'ca' in SELFIES: {len([atype for atype in atomtype_embedding_perelem_dict_selfies_bart['c'][0] if atype == 'ca'])}\")\n",
    "    print(f\"Number of embeddings for 'c3' in SMILES: {len([atype for atype in atomtype_embedding_perelem_dict_smiles_bart['c'][0] if atype == 'c3'])}\")\n",
    "    #print(f\"Number of embeddings for 'c3' in SELFIES: {len([atype for atype in atomtype_embedding_perelem_dict_selfies_bart['c'][0] if atype == 'c3'])}\")\n",
    "    limit_embeddings_SMILESonly(atomtype_embedding_perelem_dict_smiles_bart, 'c', 'ca', limit)\n",
    "    # Print the lengths to verify\n",
    "    limit_embeddings_SMILESonly(atomtype_embedding_perelem_dict_smiles_bart, 'c', 'c3', limit)\n",
    "    print(f\"Number of embeddings for 'ca' in SMILES: {len([atype for atype in atomtype_embedding_perelem_dict_smiles_bart['c'][0] if atype == 'ca'])}\")\n",
    "    #print(f\"Number of embeddings for 'ca' in SELFIES: {len([atype for atype in atomtype_embedding_perelem_dict_selfies_bart['c'][0] if atype == 'ca'])}\")\n",
    "    print(f\"Number of embeddings for 'c3' in SMILES: {len([atype for atype in atomtype_embedding_perelem_dict_smiles_bart['c'][0] if atype == 'c3'])}\")\n",
    "    #print(f\"Number of embeddings for 'c3' in SELFIES: {len([atype for atype in atomtype_embedding_perelem_dict_selfies_bart['c'][0] if atype == 'c3'])}\")\n",
    "\n",
    "    print(\"RoBERTa number embeddings for ca and c3 before limiting\")\n",
    "    # same for RoBERTa\n",
    "    print(f\"Number of embeddings for 'ca' in SMILES: {len([atype for atype in atomtype_embedding_perelem_dict_smiles_roberta['c'][0] if atype == 'ca'])}\")\n",
    "    #print(f\"Number of embeddings for 'ca' in SELFIES: {len([atype for atype in atomtype_embedding_perelem_dict_selfies_roberta['c'][0] if atype == 'ca'])}\")\n",
    "    print(f\"Number of embeddings for 'c3' in SMILES: {len([atype for atype in atomtype_embedding_perelem_dict_smiles_roberta['c'][0] if atype == 'c3'])}\")\n",
    "    #print(f\"Number of embeddings for 'c3' in SELFIES: {len([atype for atype in atomtype_embedding_perelem_dict_selfies_roberta['c'][0] if atype == 'c3'])}\")\n",
    "    limit_embeddings_SMILESonly(atomtype_embedding_perelem_dict_smiles_roberta, 'c', 'ca', limit)\n",
    "\n",
    "    limit_embeddings_SMILESonly(atomtype_embedding_perelem_dict_smiles_roberta, 'c', 'c3', limit)\n",
    "    print(f\"Number of embeddings for 'ca' in SMILES: {len([atype for atype in atomtype_embedding_perelem_dict_smiles_roberta['c'][0] if atype == 'ca'])}\")\n",
    "    #print(f\"Number of embeddings for 'ca' in SELFIES: {len([atype for atype in atomtype_embedding_perelem_dict_selfies_roberta['c'][0] if atype == 'ca'])}\")\n",
    "    print(f\"Number of embeddings for 'c3' in SMILES: {len([atype for atype in atomtype_embedding_perelem_dict_smiles_roberta['c'][0] if atype == 'c3'])}\")\n",
    "    #print(f\"Number of embeddings for 'c3' in SELFIES: {len([atype for atype in atomtype_embedding_perelem_dict_selfies_roberta['c'][0] if atype == 'c3'])}\")\n",
    "    \n",
    "    print(\"Random Bart number embeddings for ca and c3 before limiting\")\n",
    "    # same for random BART\n",
    "    print(f\"Number of embeddings for 'ca' in SMILES: {len([atype for atype in atomtype_embedding_perelem_dict_smiles_random['c'][0] if atype == 'ca'])}\")\n",
    "    print(f\"Number of embeddings for 'ca' in SELFIES: {len([atype for atype in atomtype_embedding_perelem_dict_smiles_random['c'][0] if atype == 'ca'])}\")\n",
    "    print(f\"Number of embeddings for 'c3' in SMILES: {len([atype for atype in atomtype_embedding_perelem_dict_smiles_random['c'][0] if atype == 'c3'])}\")\n",
    "    print(f\"Number of embeddings for 'c3' in SELFIES: {len([atype for atype in atomtype_embedding_perelem_dict_smiles_random['c'][0] if atype == 'c3'])}\")\n",
    "    limit_embeddings_SMILESonly(atomtype_embedding_perelem_dict_smiles_random, 'c', 'ca', limit)\n",
    "\n",
    "    limit_embeddings_SMILESonly(atomtype_embedding_perelem_dict_smiles_random, 'c', 'c3', limit)\n",
    "    print(f\"Number of embeddings for 'ca' in SMILES: {len([atype for atype in atomtype_embedding_perelem_dict_smiles_random['c'][0] if atype == 'ca'])}\")\n",
    "    print(f\"Number of embeddings for 'ca' in SELFIES: {len([atype for atype in atomtype_embedding_perelem_dict_smiles_random['c'][0] if atype == 'ca'])}\")\n",
    "    print(f\"Number of embeddings for 'c3' in SMILES: {len([atype for atype in atomtype_embedding_perelem_dict_smiles_random['c'][0] if atype == 'c3'])}\")\n",
    "    print(f\"Number of embeddings for 'c3' in SELFIES: {len([atype for atype in atomtype_embedding_perelem_dict_smiles_random['c'][0] if atype == 'c3'])}\")\n",
    "    \n",
    "    print(f\"--Number of embeddings for 'c1' in SELFIES: {len([atype for atype in atomtype_embedding_perelem_dict_smiles_random['c'][0] if atype == 'c1'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_legend(legend_elements, save_path, alpha, color):\n",
    "    cm = 1/2.54 \n",
    "    fig, ax = plt.subplots(figsize=(8.3*cm, 2*cm))  # Adjust the size as needed\n",
    "    ax.legend(handles=legend_elements, loc='center', fontsize=12, ncol=len(legend_elements), markerscale=2)\n",
    "    ax.axis('off')\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(f\"{save_path}_LEGEND_alpha{alpha}_col{color}_newnoyelligblu.svg\", format=\"svg\", dpi=600, bbox_inches='tight')\n",
    "    fig.clf()\n",
    "    \n",
    "def build_legend(data, markerdict, labels):\n",
    "    \"\"\"\n",
    "    Build a legend for matplotlib plt from dict\n",
    "    \"\"\"\n",
    "    # Count how often which label appears in the dataset\n",
    "    label_counts = {label: labels.count(label) for label in set(labels)}\n",
    "    legend_elements = []\n",
    "    for key in data:\n",
    "        if key not in labels:\n",
    "            continue\n",
    "        # Create label for legend that contains number of occurrences\n",
    "        legend_label = f\"{key} ({label_counts.get(key, 0)})\"\n",
    "        legend_elements.append(Line2D([0], [0], marker=markerdict[key], color='w', label=legend_label,\n",
    "                                      markerfacecolor=data[key], markersize=9))\n",
    "    return legend_elements\n",
    "\n",
    "def plot_pca_with_markers_seaborn(embeddings, labels, marker_dict, colours_dict, save_path, namestring, color, alpha):\n",
    "    \"\"\"Performing PCA and plotting it\n",
    "\n",
    "    Args:\n",
    "        embeddings (_list[float]_): Embeddings of one element or a subgroup\n",
    "        labels (_list[string]_): List of assigned atom types\n",
    "        colours_dict (_dict[string][int]_): Dictionary of colors linking atomtypes to colors\n",
    "        save_path (_string_): Path where to save plot\n",
    "        alpha (float, optional): Level of opacity.\n",
    "    \"\"\"\n",
    "    #inches to centimeters\n",
    "    cm = 1/2.54 \n",
    "    logging.info(\"Started plotting PCA\")\n",
    "    #before PCA I want to make sure not to plot atom types where less them 10 of those exist\n",
    "    # filter out atom types and emebddings where less than 10 of those exist\n",
    "    filtered_labels = []\n",
    "    filtered_embeddings = []\n",
    "    for label, emb in zip(labels, embeddings):\n",
    "        if labels.count(label) > 10: #and label!=\"ca\":\n",
    "            filtered_labels.append(label)\n",
    "            filtered_embeddings.append(emb)\n",
    "    \n",
    "    labels = filtered_labels\n",
    "    embeddings = filtered_embeddings\n",
    "    \n",
    "    print(\"markerdict:\",marker_dict)\n",
    "    os.makedirs(save_path.parent, exist_ok=True)\n",
    "    pca = PCA(n_components=2, random_state=SEED + 6541)\n",
    "    pca_embeddings = pca.fit_transform(embeddings)\n",
    "    logging.info(\n",
    "        f\"{save_path} has the explained variance of {pca.explained_variance_ratio_}\"\n",
    "    )\n",
    "    explained_variance_percentages = [f\"{var:.2%}\" for var in pca.explained_variance_ratio_]  # Format as percentages\n",
    "    \n",
    "    # sort labels so plotting most frequent first and least frequent will be plotted on top\n",
    "    label_counts = Counter(labels)\n",
    "    sorted_labels = sorted(label_counts, key=label_counts.get, reverse=True)\n",
    "    print(\"sorted labels:\",sorted_labels)\n",
    "    #rep = save_path.name.split('pca')[0]\n",
    "    #fig, ax = plt.subplots(1, figsize=(8.3*cm, 8.3*cm)) #8.3cm is one column figure (10cm x 8.3 cm)\n",
    "    \n",
    "    # Create a DataFrame for Seaborn\n",
    "    import pandas as pd\n",
    "    data = pd.DataFrame({\n",
    "        'PC1': pca_embeddings[:, 0],\n",
    "        'PC2': pca_embeddings[:, 1],\n",
    "        'label': labels\n",
    "    })\n",
    "    \n",
    "    # Create a JointGrid\n",
    "    g = sns.JointGrid(data=data, x='PC1', y='PC2', height=8.5*cm)\n",
    "    \n",
    "    print(f\"colours dict: {colours_dict}\")\n",
    "    # Plot the scatter plot with markers\n",
    "    for label in sorted_labels:\n",
    "        marker = marker_dict[label]\n",
    "        idx = data['label'] == label\n",
    "        col = colours_dict[label]\n",
    "        sns.scatterplot(data=data[idx], x='PC1', y='PC2', ax=g.ax_joint, marker=marker, color=col, alpha=alpha)\n",
    "        \n",
    "    # Add KDE plots on the sides for each label\n",
    "    for label in sorted_labels:\n",
    "        idx = data['label'] == label\n",
    "        col = colours_dict[label]\n",
    "        sns.kdeplot(data=data[idx], x='PC1', ax=g.ax_marg_x, color=col, alpha=alpha)\n",
    "        sns.kdeplot(data=data[idx], y='PC2', ax=g.ax_marg_y, color=col, alpha=alpha)\n",
    "        \n",
    "    # Set labels and title\n",
    "    g.ax_joint.set_xlabel(f\"PC 1 ({explained_variance_percentages[0]})\", fontsize=12)\n",
    "    g.ax_joint.set_ylabel(f\"PC 2 ({explained_variance_percentages[1]})\", fontsize=12)\n",
    "    #g.ax_joint.set_title(f\"PCA for {rep} of element {namestring}\", fontsize=12)\n",
    "    \n",
    "    # Customize spines\n",
    "    g.ax_joint.spines['right'].set_visible(False)\n",
    "    g.ax_joint.spines['top'].set_visible(False)\n",
    "    g.ax_joint.spines['left'].set_linewidth(2)\n",
    "    g.ax_joint.spines['bottom'].set_linewidth(2)\n",
    "    # Set tick parameters\n",
    "    g.ax_joint.tick_params(length=8, width=1, labelsize=12)\n",
    "    \n",
    "    # Set major tick locators\n",
    "    g.ax_joint.xaxis.set_major_locator(MultipleLocator(10))\n",
    "    g.ax_joint.yaxis.set_major_locator(MultipleLocator(10))\n",
    "    \n",
    "    # Set aspect ratio\n",
    "    g.ax_joint.set_aspect('equal', adjustable='box')\n",
    "    \n",
    "    # Calculate the limits for the axes\n",
    "    x_min, x_max = data['PC1'].min(), data['PC1'].max()\n",
    "    y_min, y_max = data['PC2'].min(), data['PC2'].max()\n",
    "    axis_min = min(x_min, y_min)\n",
    "    axis_max = max(x_max, y_max)\n",
    "    margin = 0.08 * (axis_max - axis_min) + 0.1 # 5% margin\n",
    "    \n",
    "    # Set the same limits for both axes with margins\n",
    "    g.ax_joint.set_xlim(axis_min - margin, axis_max + margin)\n",
    "    g.ax_joint.set_ylim(axis_min - margin, axis_max + margin)\n",
    "    \n",
    "    legend_elements = build_legend(colours_dict, marker_dict, labels)\n",
    "    plot_legend(legend_elements, save_path, alpha, color)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    #plt.subplots_adjust(bottom=0.4)\n",
    "     # Set ticks every 10 units\n",
    "    g.savefig(f\"{save_path}_withmarkers_alpha{alpha}_col{color})_sns_noyelligblu.svg\", format=\"svg\", dpi=600, bbox_inches='tight')\n",
    "    #plt.close()\n",
    "\n",
    "\n",
    "\n",
    "def plot_umap_pca_lda(p_f_cl_list_embs, p_f_cl_list_assigs, namestring, save_path_prefix, markerdict, atomtype2color, min_dist, n_neighbors, color, alpha):\n",
    "    #create paths on what to name the plots\n",
    "\n",
    "    pathway_pca = Path(str(save_path_prefix) + f\"pca_{namestring}\")\n",
    "    \n",
    "    print(\"subplot func\")\n",
    "    plot_pca_with_markers_seaborn(p_f_cl_list_embs, p_f_cl_list_assigs, markerdict[namestring],\n",
    "             atomtype2color[namestring], pathway_pca, namestring, color, alpha)\n",
    "\n",
    "\n",
    "def plot_plots(atomtype_embedding_perelem_dict, markerdict, colordict, min_dist, n_neighbors, alpha, save_path_prefix, color):\n",
    "    #namestring=\"c\"\n",
    "    #plot_umap_pca_lda(atomtype_embedding_perelem_dict[namestring][1], atomtype_embedding_perelem_dict[namestring][0], namestring, save_path_prefix, colordict, min_dist, n_neighbors, alpha)\n",
    "    \n",
    "    #namestring=\"p f cl o s\"\n",
    "    #plot_umap_pca_lda(atomtype_embedding_perelem_dict[namestring][1], atomtype_embedding_perelem_dict[namestring][0], namestring, save_path_prefix, colordict, min_dist, n_neighbors, alpha)\n",
    "    \n",
    "    #namestring = \"c o\"\n",
    "    #plot_umap_pca_lda(atomtype_embedding_perelem_dict[namestring][1], atomtype_embedding_perelem_dict[namestring][0], namestring, save_path_prefix, colordict, min_dist, n_neighbors, alpha)\n",
    "    \n",
    "    print(\"Plotting................................BY ELEMENT\")\n",
    "    # plot all atomtypes of one element only\n",
    "    for key in colordict.keys():\n",
    "        print(\"key\",key)\n",
    "        if len(key)<=2 and key!='cl' and key in atomtype_embedding_perelem_dict.keys():\n",
    "            print(f\"#######KEY {key}\\n\")\n",
    "            pathway_umap = Path(str(save_path_prefix) +\n",
    "                                f\"umap_{min_dist}_{n_neighbors}_{key}.svg\")\n",
    "            pathway_pca = Path(str(save_path_prefix) + f\"pca_{key}.svg\")\n",
    "            pathway_lda = Path(str(save_path_prefix) + f\"lda_{key}.svg\")\n",
    "            embeddings = atomtype_embedding_perelem_dict[key][1]\n",
    "            assignments = atomtype_embedding_perelem_dict[key][0]\n",
    "            print(\"ASSIGNMENTS: \", assignments)\n",
    "            #atomtype2color, set_list = getcolorstoatomtype(set(assignments.copy()))\n",
    "\n",
    "            try:\n",
    "                assert len(embeddings) == (len(assignments)), \"Assignments and embeddings do not have same length.\"\n",
    "                assert len(embeddings)>10, \"Not enough embeddings for plotting\"\n",
    "                print(f\"len embeddings of key {key}: {len(embeddings)}\")\n",
    "                plot_umap_pca_lda(embeddings, assignments, key, save_path_prefix, markerdict, colordict, min_dist, n_neighbors, color, alpha)\n",
    "                #plot_pca(embeddings, assignments, atomtype2color, pathway_pca, alpha)\n",
    "                #plot_lda(embeddings, assignments, atomtype2color, pathway_lda, alpha)\n",
    "                #plot_umap(embeddings, assignments, atomtype2color, pathway_umap, min_dist, n_neighbors, alpha)\n",
    "            except AssertionError as e:\n",
    "                print(f\"Assertion error occurred for element {key}: {e}\")\n",
    "                continue \n",
    "\n",
    "def create_plotsperelem(atomtype_embedding_perelem_dict_smiles, atomtype_embedding_perelem_dict_selfies, markerdict, colordict, min_dist, n_neighbors, alpha, save_path_prefix, color, model):\n",
    "    \"\"\"Create plot per element and for all element subsets\n",
    "\n",
    "    Args:\n",
    "        dikt (_dict_): Dictionary of atom mappings etc\n",
    "        colordict (_dict[string][dict[string],[color]]): Dictionary that maps atom types to colors\n",
    "        penalty_threshold (_float_): Threshold for max penalty score\n",
    "        min_dist (_float_): Number of min dist to use in UMAP\n",
    "        n_neighbors (_int_): Number of neighbors to use in UMAP\n",
    "        alpha (_int_): Level of opacity\n",
    "        save_path_prefix (_string_): Path prefix where to save output plot\n",
    "    \"\"\"\n",
    "    print(colordict.keys())\n",
    "    #plot for SMILES\n",
    "    plot_plots(atomtype_embedding_perelem_dict_smiles, markerdict, colordict, min_dist, n_neighbors, alpha, f\"{save_path_prefix}{model}_smiles\", color)\n",
    "    # plot for SELFIES\n",
    "    plot_plots(atomtype_embedding_perelem_dict_selfies, markerdict, colordict, min_dist, n_neighbors, alpha, f\"{save_path_prefix}{model}_selfies\", color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dicts_on_parmchk_penalty_threshold(merged_dict_bart, merged_dict_roberta, penalty_threshold):\n",
    "    \n",
    "    #filter both dicts at the same time based on penalty threshold\n",
    "    filtered_merged_dict_bart = {smiles: info for smiles, info in merged_dict_bart.items() if info['max_penalty'] is not None and info['max_penalty'] < penalty_threshold}\n",
    "    filtered_merged_dict_roberta = {smiles: merged_dict_roberta[smiles] for smiles in filtered_merged_dict_bart if smiles in merged_dict_roberta}\n",
    "     \n",
    "    print(f\"Len before filtering - bart_dict: {len(merged_dict_bart.keys())} and after threshold filtering: {len(filtered_merged_dict_bart.keys())}\")\n",
    "    print(f\"Len before filtering - roberta_dict: {len(merged_dict_roberta.keys())} and after threshold filtering: {len(filtered_merged_dict_roberta.keys())}\")\n",
    "    \n",
    "    # there are less SELFIES embeddings that could be mapped to the SMILES and thereby the atomtypes, so the number of embeddings is less\n",
    "    # therefore filter filtered_dict further on what is available for SELFIES and also what is available for SMILES\n",
    "    filtered_embs_bart_dict = {smiles: info for smiles, info in filtered_merged_dict_bart.items() if info['atomtype_to_embedding'] is not None and info['atomtype_to_clean_selfies_embedding'] is not None}\n",
    "    filtered_embs_roberta_dict = {smiles: filtered_merged_dict_roberta[smiles] for smiles in filtered_embs_bart_dict if smiles in filtered_merged_dict_roberta}\n",
    "    \n",
    "    \n",
    "    print(f\"Len before filtering - bart_dict: {len(filtered_merged_dict_bart.keys())} and after embedding filtering: {len(filtered_embs_bart_dict.keys())}\")\n",
    "    print(f\"Len before filtering - roberta_dict: {len(filtered_merged_dict_roberta.keys())} and after embedding filtering: {len(filtered_embs_roberta_dict.keys())}\")\n",
    "    \n",
    "    \n",
    "    return filtered_embs_bart_dict, filtered_embs_roberta_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dicts_on_parmchk_penalty_threshold_and_None(merged_dict_bart, merged_dict_roberta, merged_dict_random, penalty_threshold):\n",
    "    \n",
    "    #filter both dicts at the same time based on penalty threshold\n",
    "    filtered_merged_dict_bart = {smiles: info for smiles, info in merged_dict_bart.items() if info['max_penalty'] is not None and info['max_penalty'] < penalty_threshold}\n",
    "    filtered_merged_dict_roberta = {smiles: merged_dict_roberta[smiles] for smiles in filtered_merged_dict_bart if smiles in merged_dict_roberta}\n",
    "    filtered_merged_dict_random = {smiles: merged_dict_random[smiles] for smiles in filtered_merged_dict_bart if smiles in merged_dict_random}\n",
    "     \n",
    "    print(f\"Len before filtering - bart_dict: {len(merged_dict_bart.keys())} and after threshold filtering: {len(filtered_merged_dict_bart.keys())}\")\n",
    "    print(f\"Len before filtering - roberta_dict: {len(merged_dict_roberta.keys())} and after threshold filtering: {len(filtered_merged_dict_roberta.keys())}\")\n",
    "    print(f\"Len before filtering - random_dict: {len(merged_dict_random.keys())} and after threshold filtering: {len(filtered_merged_dict_random.keys())}\")\n",
    "    print()\n",
    "    \n",
    "    # there are less SELFIES embeddings that could be mapped to the SMILES and thereby the atomtypes, so the number of embeddings is less\n",
    "    # therefore filter filtered_dict further on what is available for SELFIES and also what is available for SMILES\n",
    "    filtered_embs_bart_dict = {smiles: info for smiles, info in filtered_merged_dict_bart.items() if info['atomtype_to_embedding'] is not None}\n",
    "    filtered_embs_roberta_dict = {smiles: filtered_merged_dict_roberta[smiles] for smiles in filtered_embs_bart_dict if smiles in filtered_merged_dict_roberta}\n",
    "    filtered_embs_random_dict = {smiles: filtered_merged_dict_random[smiles] for smiles in filtered_embs_bart_dict if smiles in filtered_merged_dict_random}\n",
    "    \n",
    "    #count None type embeddings in filtered_embs_random_dict\n",
    "    #count_None = 0\n",
    "    #for smiles, info in filtered_embs_random_dict.items():\n",
    "    #    print(info['atomtype_to_clean_selfies_embedding'])\n",
    "    #    if info['atomtype_to_clean_selfies_embedding'] is None:\n",
    "    #        count_None += 1\n",
    "    #print(f\"Number of None type embeddings in filtered_embs_random_dict: {count_None}\")\n",
    "    \n",
    "    #count None type embeddings in filtered_embs_random_dict\n",
    "    count_None = 0\n",
    "    for smiles, info in filtered_embs_roberta_dict.items():\n",
    "        if info['atomtype_to_embedding'] is None:\n",
    "            count_None += 1\n",
    "    print(f\"Number of None type embeddings in filtered_embs_roberta_dict: {count_None}\")\n",
    "    \n",
    "    #count None type embeddings in filtered_embs_random_dict\n",
    "    count_None = 0\n",
    "    for smiles, info in filtered_embs_bart_dict.items():\n",
    "        if info['atomtype_to_embedding'] is None:\n",
    "            count_None += 1\n",
    "    print(f\"Number of None type embeddings in filtered_embs_bart_dict: {count_None}\")\n",
    "    \n",
    "    print(f\"Len before filtering - bart_dict: {len(filtered_merged_dict_bart.keys())} and after embedding filtering: {len(filtered_embs_bart_dict.keys())}\")\n",
    "    print(f\"Len before filtering - roberta_dict: {len(filtered_merged_dict_roberta.keys())} and after embedding filtering: {len(filtered_embs_roberta_dict.keys())}\")\n",
    "    print(f\"Len before filtering - random_dict: {len(filtered_merged_dict_random.keys())} and after embedding filtering: {len(filtered_embs_random_dict.keys())}\")\n",
    "    \n",
    "    \n",
    "    return filtered_embs_bart_dict, filtered_embs_roberta_dict, filtered_embs_random_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_colorsubdict(filtered_embs_bart_dict, color):\n",
    "    # create \n",
    "    unique_atomtype_set = set(chain.from_iterable(filtered_embs_bart_dict[key]['atom_types'] for key in filtered_embs_bart_dict if filtered_embs_bart_dict[key].get('atom_types') is not None))\n",
    "    basic_atomtypes_to_elems_dict = {'c': ['c', 'c1', 'c2', 'c3', 'ca'], 'o': ['o', 'oh', 'os'], 'n': ['n', 'n1', 'n2', 'n3', 'n4', 'na', 'nh', 'no']}\n",
    "    atomtypes_to_elems_dict = create_elementsubsets(unique_atomtype_set)\n",
    "    # get colors for atomtypes by element and element groups\n",
    "    colordict, markerdict = colorstoatomtypesbyelement(basic_atomtypes_to_elems_dict)\n",
    "    #print(colordict['sash']['c'])\n",
    "    print(markerdict)\n",
    "    colorsubdict=colordict[color]\n",
    "    return colorsubdict, markerdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dicts=False\n",
    "traintype=\"pretrained\"\n",
    "if create_dicts:\n",
    "    \n",
    "    model =\"BART\"\n",
    "    merged_dikt_bart = create_merged_dikt(model, traintype)\n",
    "\n",
    "    model =\"roberta\"\n",
    "    merged_dikt_roberta = create_merged_dikt(model, traintype)\n",
    "    \n",
    "    # create random model embeddings\n",
    "    model =\"random\"\n",
    "    traintype=\"pretrained\"\n",
    "    merged_dikt_random = create_merged_dikt(model, traintype)\n",
    "\n",
    "    print(merged_dikt_bart.keys())\n",
    "    print(len(merged_dikt_bart.keys()))\n",
    "\n",
    "    print(merged_dikt_roberta.keys())\n",
    "    print(len(merged_dikt_roberta.keys()))\n",
    "\n",
    "    print(merged_dikt_random.keys())\n",
    "    print(len(merged_dikt_random.keys()))\n",
    "\n",
    "    #confirm it's the same keys please \n",
    "    for key in merged_dikt_bart.keys():\n",
    "        if key not in merged_dikt_roberta.keys():\n",
    "            print(f\"key {key} not in roberta\")\n",
    "    for key,val in zip(merged_dikt_bart.keys(),merged_dikt_roberta.keys()):\n",
    "        #print(key, val)\n",
    "        if key!=val:\n",
    "            print(f\"key {key} not equal to val {val}\")\n",
    "            \n",
    "    #confirm it's the same keys please \n",
    "    for key in merged_dikt_bart.keys():\n",
    "        if key not in merged_dikt_random.keys():\n",
    "            print(f\"key {key} not in roberta\")\n",
    "    for key,val in zip(merged_dikt_bart.keys(),merged_dikt_random.keys()):\n",
    "        #print(key, val)\n",
    "        if key!=val:\n",
    "            print(f\"key {key} not equal to val {val}\")\n",
    "\n",
    "    # save merged dicts\n",
    "    import json\n",
    "    with open('./assignment_dicts/kekulized/merged_dikt_bart_bigC_5_5.json', 'w') as f:\n",
    "        json.dump(merged_dikt_bart, f)\n",
    "    with open('./assignment_dicts/kekulized/merged_dikt_roberta_bigC_5_5.json', 'w') as f:\n",
    "        json.dump(merged_dikt_roberta, f)\n",
    "    with open('./assignment_dicts/kekulized/merged_dikt_random_bart_bigC_5_5.json', 'w') as f:\n",
    "        json.dump(merged_dikt_random, f)\n",
    "else:\n",
    "    print(\"Merged dicts were already created, no need to create them again.\")\n",
    "    with open('./assignment_dicts/kekulized/merged_dikt_bart_bigC_5_5.json', 'r') as f:\n",
    "        merged_dikt_bart = json.load(f)\n",
    "    with open('./assignment_dicts/kekulized/merged_dikt_roberta_bigC_5_5.json', 'r') as f:\n",
    "        merged_dikt_roberta = json.load(f)\n",
    "    with open('./assignment_dicts/kekulized/merged_dikt_random_bart_bigC_5_5.json', 'r') as f:\n",
    "        merged_dikt_random = json.load(f)\n",
    "        \n",
    "print(len(merged_dikt_bart.keys()))\n",
    "print(len(merged_dikt_roberta.keys()))\n",
    "print(len(merged_dikt_random.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "penalty_threshold = 300\n",
    "filtered_embs_bart_dict, filtered_merged_dict_roberta, filtered_merged_dict_random = filter_dicts_on_parmchk_penalty_threshold_and_None(merged_dikt_bart, merged_dikt_roberta, merged_dikt_random, penalty_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get colordict\n",
    "color=\"tol\"\n",
    "colorsubdict, markerdict = get_colorsubdict(filtered_embs_bart_dict, color)\n",
    "\n",
    "for key, val in colorsubdict.items():\n",
    "    print(key, val)\n",
    "    \n",
    "dark_blue = '#1965B0' #--> turn aromatic c to dark blue?\n",
    "#perfect colors in oxygen plot with orange, blue, purple\n",
    "# need to change that red and green are in same plot for nitrogen\n",
    "# change color of nh to orange\n",
    "orange = '#f1932d'\n",
    "colorsubdict['c']['ca']=dark_blue\n",
    "colorsubdict['n']['nh']=orange\n",
    "print(colorsubdict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter on parmchk penalty threshold\n",
    "\n",
    "#penalty_threshold = 300\n",
    "#filtered_embs_bart_dict, filtered_merged_dict_roberta = filter_dicts_on_parmchk_penalty_threshold_and_None(merged_dikt_bart, merged_dikt_roberta, merged_dikt_random, penalty_threshold)\n",
    "\n",
    "# get colordict\n",
    "#color=\"tol\"\n",
    "#colorsubdict, markerdict = get_colorsubdict(filtered_embs_bart_dict, color)\n",
    "\n",
    "# get all atomtype_to_embeddings dicts\n",
    "atomtype_embedding_perelem_dict_smiles_bart = get_atomtype_embedding_perelem_dict(filtered_embs_bart_dict, colorsubdict, 'atomtype_to_embedding')\n",
    "#atomtype_embedding_perelem_dict_selfies_bart = get_atomtype_embedding_perelem_dict(filtered_embs_bart_dict, colorsubdict, 'atomtype_to_clean_selfies_embedding')\n",
    "\n",
    "atomtype_embedding_perelem_dict_smiles_roberta = get_atomtype_embedding_perelem_dict(filtered_merged_dict_roberta, colorsubdict, 'atomtype_to_embedding')\n",
    "#atomtype_embedding_perelem_dict_selfies_roberta = get_atomtype_embedding_perelem_dict(filtered_merged_dict_roberta, colorsubdict, 'atomtype_to_clean_selfies_embedding')\n",
    "\n",
    "atomtype_embedding_perelem_dict_smiles_random = get_atomtype_embedding_perelem_dict(filtered_merged_dict_random, colorsubdict, 'atomtype_to_embedding')\n",
    "#atomtype_embedding_perelem_dict_selfies_random = get_atomtype_embedding_perelem_dict(filtered_merged_dict_random, colorsubdict, 'atomtype_to_clean_selfies_embedding')\n",
    "#the random model BART is only trained on SMILES, so no SELFIES embeddings to take from, just use the SMILES embeddings for SELFIES again to keep up with the same structure\n",
    "\n",
    "limit = 300\n",
    "limit_all_atomtypes_to_threshold_SMILESonly(limit, atomtype_embedding_perelem_dict_smiles_bart, atomtype_embedding_perelem_dict_smiles_roberta, atomtype_embedding_perelem_dict_smiles_random, atomtype_embedding_perelem_dict_smiles_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traintype=\"pretrained\"\n",
    "final_validkeys = len([key for key in filtered_embs_bart_dict.keys() if filtered_embs_bart_dict[key]['posToKeep'] is not None and filtered_embs_bart_dict[key]['atomtype_to_embedding'] is not None and filtered_embs_bart_dict[key]['atomtype_to_clean_selfies_embedding'] is not None])\n",
    "save_path_prefix = f\"./5_5__PaulTolColor_{traintype}_delaney_bbbp_clearance_lipo_mols{final_validkeys}_thresh{penalty_threshold}_ctypelimit{limit}_bigC/\"\n",
    "min_dist = 0.1\n",
    "n_neighbors = 15\n",
    "alpha = 0.6\n",
    "\n",
    "model=\"BART\"\n",
    "print(f\"PLOT for model {model}\")\n",
    "create_plotsperelem(atomtype_embedding_perelem_dict_smiles_bart, atomtype_embedding_perelem_dict_selfies_bart, markerdict, colorsubdict, min_dist, n_neighbors, alpha, save_path_prefix, color, model)\n",
    "\n",
    "model=\"roberta\"\n",
    "print(f\"PLOT for model {model}\")\n",
    "create_plotsperelem(atomtype_embedding_perelem_dict_smiles_roberta, atomtype_embedding_perelem_dict_selfies_roberta, markerdict, colorsubdict, min_dist, n_neighbors, alpha, save_path_prefix, color, model)\n",
    "\n",
    "model=\"random\"\n",
    "print(f\"PLOT for model {model}\")\n",
    "create_plotsperelem(atomtype_embedding_perelem_dict_smiles_random, atomtype_embedding_perelem_dict_smiles_random, markerdict, colorsubdict, min_dist, n_neighbors, alpha, save_path_prefix, color, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fairseq_git2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
