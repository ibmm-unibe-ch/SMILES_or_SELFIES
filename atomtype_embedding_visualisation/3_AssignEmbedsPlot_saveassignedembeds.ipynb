{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Saving embeddings of atom types for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ifender/miniconda3/envs/fairseq_git2/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Skipped loading some Tensorflow models, missing a dependency. No module named 'tensorflow'\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'torch_geometric'\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. cannot import name 'DMPNN' from 'deepchem.models.torch_models' (/home/ifender/miniconda3/envs/fairseq_git2/lib/python3.10/site-packages/deepchem/models/torch_models/__init__.py)\n",
      "Skipped loading modules with pytorch-lightning dependency, missing a dependency. No module named 'pytorch_lightning'\n",
      "Skipped loading some Jax models, missing a dependency. No module named 'jax'\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import json\n",
    "import logging\n",
    "from typing import List\n",
    "from tokenisation import tokenize_dataset, get_tokenizer\n",
    "from pathlib import Path\n",
    "from fairseq_utils2 import compute_model_output, compute_model_output_RoBERTa, load_dataset, load_model\n",
    "from fairseq.data import Dictionary\n",
    "from SMILES_to_SELFIES_mapping import canonize_smiles, generate_mapping, generate_mappings_for_task_SMILES_to_SELFIES\n",
    "from itertools import chain\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from constants import SEED\n",
    "import numpy as np\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "from collections import Counter\n",
    "\n",
    "from constants import (\n",
    "    TASK_PATH,\n",
    "    MOLNET_DIRECTORY,\n",
    "    TOKENIZER_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_atomtype_embedding_perelem_dict(filtered_dict, colordict, entry_name_atomtype_to_embedding):\n",
    "    atomtype_to_embedding_lists = [value[entry_name_atomtype_to_embedding] for value in filtered_dict.values() if entry_name_atomtype_to_embedding in value and value[entry_name_atomtype_to_embedding] is not None]\n",
    "    print(\"len atomtype to embedding list smiles: \",len(atomtype_to_embedding_lists))\n",
    "    \n",
    "    # sort embeddings according to atomtype, I checked it visually and the mapping works\n",
    "    embeddings_by_atomtype = {}  # Dictionary to hold lists of embeddings for each atom type\n",
    "    #listembeddings = list()\n",
    "    for atom_type_list in atomtype_to_embedding_lists:\n",
    "        # go through single dictionary\n",
    "        for tuple in atom_type_list:\n",
    "           # print(f\"atomtype {atom_type} embeddings {embeddings[1]}\")\n",
    "            if tuple[0] not in embeddings_by_atomtype:\n",
    "                embeddings_by_atomtype[tuple[0]] = []\n",
    "            # extend the list of embeddings for this atom type(, but ONLY by the embedding not the attached token)\n",
    "            embeddings_by_atomtype[tuple[0]].append(tuple[1][0])\n",
    "            #print(\"\\ntuple01\",len(tuple[1][0]),tuple[1][0])\n",
    "            #print(len(embeddings[0]))\n",
    "    print(\"embeddings c\",len(embeddings_by_atomtype['c']))\n",
    "    \n",
    "    # sort dictionary that is mapping embeddings to atomtypes to elements so that e.g. all carbon atom types can be accessed at once in one list\n",
    "    #atom_types_repeated = []\n",
    "    #embeddings_list = []\n",
    "    atomtype_embedding_perelem_dict = dict()\n",
    "    ctr = 0\n",
    "    for key in colordict.keys():\n",
    "        print(f\"key {key}\")\n",
    "        for atype in colordict[key]:\n",
    "            print(atype) \n",
    "            if atype in embeddings_by_atomtype.keys():\n",
    "                embsofatype = embeddings_by_atomtype[atype]\n",
    "                atypes = [atype] * len(embeddings_by_atomtype[atype])\n",
    "                assert len(embsofatype) == len(atypes), \"Length of embeddings and atom types do not match.\"\n",
    "                if key not in atomtype_embedding_perelem_dict:\n",
    "                    atomtype_embedding_perelem_dict[key] = ([],[])\n",
    "                if key in atomtype_embedding_perelem_dict:\n",
    "                    atomtype_embedding_perelem_dict[key][0].extend(atypes)\n",
    "                    atomtype_embedding_perelem_dict[key][1].extend(embsofatype)\n",
    "    \n",
    "    print(atomtype_embedding_perelem_dict.keys())\n",
    "    return atomtype_embedding_perelem_dict\n",
    "\n",
    "def colorstoatomtypesbyelement(atomtoelems_dict):\n",
    "    \"\"\"Generating a dictionary of colors given a dictionary that maps atomtypes to elements\n",
    "\n",
    "    Args:\n",
    "        atomtoelems_dict (_dict_): Dictionary that maps atom types to elements\n",
    "\n",
    "    Returns:\n",
    "        _dict,: Dictionary that maps atom types to colors\n",
    "    \"\"\"\n",
    "    # https://sashamaps.net/docs/resources/20-colors/ #95% accessible only, subject to change, no white\n",
    "    sash = ['#e6194B', '#3cb44b', '#ffe119', '#4363d8', '#f58231', '#911eb4', '#42d4f4', '#f032e6', '#bfef45', '#fabed4',\n",
    "                   '#469990', '#dcbeff', '#9A6324', '#fffac8', '#800000', '#aaffc3', '#808000', '#ffd8b1', '#000075', '#a9a9a9', '#000000']\n",
    "    #viridis\n",
    "    viridis = plt.cm.get_cmap('viridis', 8)  # Get the Viridis colormap\n",
    "    viridis_colors = [mcolors.to_hex(viridis(i)) for i in range(8)]  # Convert to hex \n",
    "    # tol colorscale https://personal.sron.nl/~pault/ \n",
    "    # 10 14 15 17 18 26\n",
    "    tol = ['#882e72','#1965B0', '#7bafde', '#4eb265', '#cae0ab', '#f7f056', '#f1932d', '#dc050c'] \n",
    "    \n",
    "    markers = [\"o\", \"s\", \"^\", \"v\", \"D\", \"P\", \"X\", \"*\"]\n",
    "    \n",
    "    # create dict for colors\n",
    "    colornames=['sash', 'tol', 'viridis']\n",
    "    colors = [sash, tol, viridis_colors]\n",
    "    colordict = dict()\n",
    "\n",
    "    for colourname, colour in zip(colornames,colors):\n",
    "        colordict[colourname]={}\n",
    "        for key in atomtoelems_dict.keys():\n",
    "            atypes = atomtoelems_dict[key]\n",
    "            keycoldict=dict()\n",
    "            for at, col in zip(atypes, colour[0:len(atypes)]):\n",
    "                keycoldict[at]=col    \n",
    "            colordict[colourname][key]=keycoldict \n",
    "    print(colordict.items())\n",
    "    print(colordict['sash'])\n",
    "    \n",
    "    # create dict for markers\n",
    "    markerdict = dict()\n",
    "    for key in atomtoelems_dict.keys():\n",
    "            atypes = atomtoelems_dict[key]\n",
    "            markerdictdict=dict()\n",
    "            i=0\n",
    "            for at in atypes:\n",
    "                markerdictdict[at]=markers[i]\n",
    "                i+=1  \n",
    "            markerdict[key]=markerdictdict\n",
    "    print(markerdict.items())\n",
    "    \n",
    "    \n",
    "    \n",
    "    # now instead for each element, get colors for a combination of atomtypes\n",
    "    # p f cl o s\n",
    "    #key='p f cl o s'\n",
    "    #pfclos_types = atomtoelems_dict['p']+atomtoelems_dict['f']+atomtoelems_dict['cl']+atomtoelems_dict['o']+atomtoelems_dict['s']\n",
    "    #keycoldicti=dict()\n",
    "    #for at, col in zip(pfclos_types, colors_sash[0:len(pfclos_types)]):\n",
    "    #    keycoldicti[at]=col\n",
    "    #colordict[key]=keycoldicti \n",
    "    # c o\n",
    "    #key='c o'\n",
    "    #pfclos_types = atomtoelems_dict['c']+atomtoelems_dict['o']\n",
    "    #keycoldicti=dict()\n",
    "    #for at, col in zip(pfclos_types, colors_sash[0:len(pfclos_types)]):\n",
    "    #    keycoldicti[at]=col\n",
    "    #colordict[key]=keycoldicti \n",
    "    #print(colordict.keys())\n",
    "    #print(colordict.items())\n",
    "    return colordict, markerdict\n",
    "    \n",
    "\n",
    "def create_elementsubsets(atomtype_set):\n",
    "    \"\"\"Creation of element subsets according to alphabet\n",
    "\n",
    "    Args:\n",
    "        big_set (_set_): Set of atom types\n",
    "    Returns:\n",
    "        _list,dict[string][list[float],list[string]]_: List of keys (elements), dictionary that contains atomtypes sorted by element\n",
    "    \"\"\"\n",
    "    atomtype_set=sorted(atomtype_set)\n",
    "    element_dict = dict()\n",
    "    elements = list()\n",
    "    ctr=0\n",
    "    last_firstval = ''\n",
    "    for atype in atomtype_set:\n",
    "        if ctr==0:\n",
    "            last_firstval = atype[0]\n",
    "        if not atype.startswith('cl') and atype not in element_dict.items() and atype[0]==last_firstval:\n",
    "            #print(elements)\n",
    "            elements.append(atype)\n",
    "            element_dict[last_firstval] = elements\n",
    "        elif last_firstval != atype[0] and atype != 'cl' and atype != 'br':\n",
    "            element_dict[last_firstval] = elements\n",
    "            elements = list()\n",
    "            elements.append(atype)\n",
    "            last_firstval = atype[0]\n",
    "        ctr+=1\n",
    "    element_dict['cl']=['cl']\n",
    "    element_dict['br']=['br']\n",
    "    return element_dict\n",
    "\n",
    "def map_selfies_embeddings_to_smiles(embeds_selfies, smiles_to_selfies_mapping, dikt):\n",
    "    \"\"\"Map  clean SELFIES embeddings to their corresponding SMILES and atomtypes\n",
    "    Args:\n",
    "        embeds_selfies (_list_): List of lists of SELFIES embeddings\n",
    "        smiles_to_selfies_mapping (_dict_): Dictionary that maps SMILES to SELFIES and SELFIES tokens to SMILES tokens (mappings[smiles]['selfiesstr_tok_map'] = (selfies_str,tokenised_selfies,mapping))\n",
    "        dikt (_dict_): Dictionary of atom mappings etc\n",
    "    Returns:\n",
    "        adds SELFIES embeddings to atomtype mappings to dikt\n",
    "    \"\"\"\n",
    "    # get embeddings for SELFIES that have a mapping to SMILES and map to SMILES in smiles_to_selfies_mapping\n",
    "    for emb, smiles in zip(embeds_selfies[0], smiles_to_selfies_mapping.keys()):\n",
    "        # Check if the mapping for the current smiles has a non-None value at index 2 for mapping of SELFIES to SMILES\n",
    "        if smiles_to_selfies_mapping[smiles]['selfiesstr_tok_map'][2] is not None:\n",
    "            # If so, set 'selfies_emb' to emb, otherwise set it to None\n",
    "            smiles_to_selfies_mapping[smiles].setdefault(\"raw_selfies_emb\", emb)\n",
    "        else:\n",
    "            smiles_to_selfies_mapping[smiles].setdefault(\"raw_selfies_emb\", None)\n",
    "\n",
    "    print(\"within\", len(dikt.keys()))\n",
    "    for key,val in dikt.items():\n",
    "        #print(\"smiles:\",key, val['atomtype_to_embedding'][0])\n",
    "        if key in smiles_to_selfies_mapping.keys():\n",
    "            # get list with positions to keep from dikt\n",
    "            #if assignment failed posToKeep will be empty, then there is no need to map anything\n",
    "            posToKeep = dikt[key][\"posToKeep\"]\n",
    "            if posToKeep is not None:\n",
    "               # print(\"selfies:\", smiles_to_selfies_mapping[key]['selfiesstr_tok_map'][0], smiles_to_selfies_mapping[key]['selfiesstr_tok_map'][1])\n",
    "                # if mapping exists\n",
    "                if smiles_to_selfies_mapping[key]['selfiesstr_tok_map'][2] is not None and smiles_to_selfies_mapping[key]['raw_selfies_emb'] is not None:\n",
    "                   # print(\"key:\",key)\n",
    "                   # print(\"1111111111: \",smiles_to_selfies_mapping[key]['selfiesstr_tok_map'][1])\n",
    "                   # print(\"1111111111: \",smiles_to_selfies_mapping[key]['selfiesstr_tok_map'][2].keys())\n",
    "                    # 1 atom mappings and number of embeddings do not have to be even because branch, ring and overloaded tokens cannot be mapped to tokens in canonized SMILES\n",
    "                    # 1 keep only the mebeddings that have a mapping\n",
    "                    embs_with_mapping = []\n",
    "                    for x, val in smiles_to_selfies_mapping[key]['selfiesstr_tok_map'][2].items():\n",
    "                        token_id = x[0]\n",
    "                        #print(\"\\ttoken:\",x[1])\n",
    "                        #print(\"\\ttoken id:\",x[0]) \n",
    "                        #print(\"\\t in embedding: \",smiles_to_selfies_mapping[key]['raw_selfies_emb'][token_id][1])\n",
    "                        #print()\n",
    "                        #print(\"maps to smiles id: \",val)\n",
    "                        #print()\n",
    "                        assert x[1]==smiles_to_selfies_mapping[key]['raw_selfies_emb'][token_id][1], f\"Token {x[1]} does not match token {smiles_to_selfies_mapping[key]['raw_selfies_emb'][token_id][1]}\"\n",
    "                        embs_with_mapping.append((val, smiles_to_selfies_mapping[key]['raw_selfies_emb'][token_id]))\n",
    "                        #embs_with_mapping.append(smiles_to_selfies_mapping[key]['raw_selfies_emb'][token_id])\n",
    "                    # 2 resort embeddings according to their position in the SMILES string\n",
    "                    embs_with_mapping = sorted(embs_with_mapping, key=lambda item: item[0])\n",
    "                    #print(\"sorted \", [key for key, _ in embs_with_mapping])\n",
    "                   ## for _, value in embs_with_mapping:\n",
    "                     #   print(value[1])\n",
    "                    # 3 only keep embeddings with smiles id that belong to id that is in posToKeepList\n",
    "                    filtered_embs = [(key, value) for key, value in embs_with_mapping if key in posToKeep]\n",
    "                    # 4 assert that the length of the filtered embeddings is the same as the length of the posToKeep list\n",
    "                    assert len(filtered_embs) == len(posToKeep), f\"Length of filtered embeddings {len(filtered_embs)} and posToKeep list {len(posToKeep)} do not agree.\"\n",
    "                    # 5 map the filtered embeddings to the atom types\n",
    "                    atomtypes= dikt[key]['atom_types']\n",
    "                    assert len(atomtypes) == len(filtered_embs), f\"Length of atom types {len(atomtypes)} and filtered embeddings {len(filtered_embs)} do not agree.\"\n",
    "                    atomtypes_to_selfies_embs = []\n",
    "                    for atomtype, emb in zip(atomtypes, filtered_embs):\n",
    "                        atomtypes_to_selfies_embs.append((atomtype, emb[1]))  \n",
    "                        # assert letters of atomtype and token of embedding match\n",
    "                        # checked visually, looks good\n",
    "                        #print(\"----------------------------------------------------\")\n",
    "                        #print(f\"atomtype {atomtype} emb {emb[1][1]}\")\n",
    "                        \n",
    "                    # 6 attach this dictionary with name 'atomtype_to_clean_selfies_embedding' to the dikt\n",
    "                    dikt[key].setdefault(\"atomtype_to_clean_selfies_embedding\", atomtypes_to_selfies_embs) \n",
    "                else:\n",
    "                    dikt[key].setdefault(\"atomtype_to_clean_selfies_embedding\", None)\n",
    "            else:\n",
    "                dikt[key].setdefault(\"atomtype_to_clean_selfies_embedding\", None)\n",
    "        else:\n",
    "            dikt[key].setdefault(\"atomtype_to_clean_selfies_embedding\", None)    \n",
    "\n",
    "def map_embeddings_to_atomtypes(dikt,task_SMILES):\n",
    "    for SMILES in task_SMILES:\n",
    "        if dikt[SMILES][\"posToKeep\"] is not None:\n",
    "            atomtype_to_embedding = {}\n",
    "            atom_types = dikt[SMILES]['atom_types']\n",
    "            embeddings = dikt[SMILES]['clean_embedding']\n",
    "            type_to_emb_tuples_list = list()\n",
    "            for atom_type, embedding in zip(atom_types, embeddings):\n",
    "                type_to_emb_tuples_list.append((atom_type, embedding))\n",
    "                #atomtype_to_embedding.setdefault(atom_type, []).append(embedding)\n",
    "                #type_to_emb_dict[atom_type] = embedding\n",
    "                # assert to check whether atom type is the same as the first letter of the embedding\n",
    "                assert(atom_type.lower() if atom_type.lower() =='cl' or atom_type.lower() =='br' else atom_type[0].lower()==(embedding[1][1].lower() if embedding[1].startswith(\"[\") else embedding[1]).lower()), f\"Atom assignment failed: {atom_type} != {embedding[1]}\"\n",
    "            dikt[SMILES][\"atomtype_to_embedding\"] = type_to_emb_tuples_list\n",
    "        else:\n",
    "            dikt[SMILES][\"atomtype_to_embedding\"]= (None,None)\n",
    "    logging.info(\"Embeddings mapped to atom types, all checks passed\")\n",
    "\n",
    "def get_clean_embeds(embeds, dikt, creation_assignment_fails, task_SMILES):\n",
    "    \"\"\"Clean embeddings of embeddings that encode for digits, hydrogens, or structural tokens\n",
    "\n",
    "    Args:\n",
    "        embeds (_List[List[float]_): Embeddings of a SMILES\n",
    "        failedSmiPos (_list_): Positions of SMILES in list where no file and/or assignment could be generated\n",
    "        posToKeep_list (_list_): List of positions in a SMILES according to tokens that need to be kept (not digits, hydrogens, or structural tokens)\n",
    "\n",
    "    Returns:\n",
    "        _list[float]_: Embeddings that do not encode hydrogens, digits, or structural tokens, but only atoms\n",
    "    \"\"\"\n",
    "    # some sanity checks on embeddings per SMILES\n",
    "    assert (len(dikt.keys())) == (len(\n",
    "        embeds[0])), f\"Number of SMILES and embeddings do not agree. Number of SMILES: {len(dikt.keys())} of which {creation_assignment_fails} failures and Number of embeddings: {len(embeds[0])}\"\n",
    "    print(f\"Number of SMILES: {len(dikt.keys())} with {creation_assignment_fails} failures and Number of embeddings: {len(embeds[0])}\")\n",
    "    \n",
    "    none_embeddings = sum([1 for emb in embeds[0] if emb is None])\n",
    "    print(\"Sum of NONE embeddings:\",none_embeddings)\n",
    "    \n",
    "    none_sth =0\n",
    "    #only keep embeddings for SMILES where atoms could be assigned to types\n",
    "    embeds_clean = list()\n",
    "    for smi, emb in zip(task_SMILES, embeds[0]):\n",
    "        posToKeep = dikt[smi][\"posToKeep\"]\n",
    "        # new: embeddings can be none too\n",
    "        if posToKeep is not None and emb is not None:\n",
    "            embeds_clean.append(emb)\n",
    "            dikt[smi][\"orig_embedding\"]=emb\n",
    "        else:\n",
    "            dikt[smi][\"orig_embedding\"]=None\n",
    "            none_sth+=1\n",
    "    \n",
    "    logging.info(\n",
    "        f\"Length embeddings before removal: {len(embeds[0])}, after removal where atom assignment failed or embedding is None: {len(embeds_clean)}\")\n",
    "    creation_assignment_fails_AND_none_embeddings =creation_assignment_fails+none_embeddings\n",
    "    numberdel_embs = (len(embeds[0])-len(embeds_clean))\n",
    "    assert none_sth == (len(\n",
    "        embeds[0])-len(embeds_clean)), f\"Assignment fails ({creation_assignment_fails}) plus none embeddings {none_embeddings} (leads to: {none_sth}) and number of deleted embeddings do not agree ({numberdel_embs}).\"\n",
    "\n",
    "    embeds_cleaner = []\n",
    "    #assert len(embeds_clean) == (len([item for item in posToKeep_list if item is not None])\n",
    "     #                            ), f\"Not the same amount of embeddings as assigned SMILES. {len(embeds_clean)} embeddings vs. {len([item for item in posToKeep_list if item is not None])} SMILES with positions\"\n",
    "    # only keep embeddings that belong to atoms\n",
    "    for SMILES in task_SMILES:\n",
    "        poslist = dikt[SMILES][\"posToKeep\"]\n",
    "        emb_clean = dikt[SMILES][\"orig_embedding\"]\n",
    "\n",
    "        if poslist is not None and emb_clean is not None:\n",
    "            newembsforsmi = []\n",
    "            newembsforsmi = [emb_clean[pos] for pos in poslist]\n",
    "            embeds_cleaner.append(newembsforsmi)\n",
    "            dikt[SMILES][\"clean_embedding\"]=newembsforsmi  \n",
    "        else:\n",
    "            # if original embeddings is None, make clean embedding None too\n",
    "            dikt[SMILES][\"clean_embedding\"]=None   \n",
    "            # also set posToKeep to None\n",
    "            dikt[SMILES][\"posToKeep\"]=None\n",
    "\n",
    "    # sanity check that length of embeddings to keep is the same as length of embeddings to keep\n",
    "    posToKeep_list = [value[\"posToKeep\"] for value in dikt.values() if value[\"posToKeep\"] is not None]\n",
    "    # sanity check that the lengths agree\n",
    "    for smiemb, pos_list in zip(embeds_cleaner, posToKeep_list):\n",
    "        assert len(smiemb) == len(\n",
    "            pos_list), \"Final selected embeddings for assigned atoms do not have same length as list of assigned atoms.\"\n",
    "        #print(len(smiemb), pos_list)\n",
    "        \n",
    "    # sanity check that length of assigned atoms map to length of clean embeddings\n",
    "    for SMILES in task_SMILES:\n",
    "        smi_clean=dikt[SMILES][\"smi_clean\"]\n",
    "        emb_clean = dikt[SMILES][\"clean_embedding\"]\n",
    "        if dikt[SMILES][\"posToKeep\"] is not None and emb_clean is not None:\n",
    "            assert len(smi_clean) == len(\n",
    "                emb_clean), \"SMILES and embeddings do not have same length.\"\n",
    "            for sm, em in zip(smi_clean,emb_clean):\n",
    "                #print(f\"sm {sm} em {em[1]}\")\n",
    "                assert(sm==em[1]), f\"Atom assignment failed: {sm} != {em[1]}\"\n",
    "    logging.info(\"Cleaning embeddings finished, all checks passed\")\n",
    "    return embeds_cleaner\n",
    "\n",
    "def check_lengths(smi_toks, embeds):\n",
    "    \"\"\"Check that number of tokens corresponds to number of embeddings per SMILES, otherwise sth went wrong\n",
    "     new: if sth went wrong turn that embedding to None and return the embeddings\n",
    "\n",
    "    Args:\n",
    "        smi_toks (_list[string]_): SMILES tokens for a SMILES\n",
    "        embeds (_list[float]_): Embeddings\n",
    "    \"\"\"\n",
    "    samenums = 0\n",
    "    diffnums = 0\n",
    "    smismaller = 0\n",
    "    new_embs = list()\n",
    "    for smi, embs in zip(smi_toks, embeds[0]):\n",
    "        # only compare when both are not None)\n",
    "        if embs is not None and smi is not None:\n",
    "            if len(smi) == len(embs):\n",
    "                samenums += 1\n",
    "                new_embs.append(embs)\n",
    "            else:\n",
    "                print(f\"smilen: {len(smi)} emblen: {len(embs)}\")\n",
    "                embs_signs = [emb1 for (emb0,emb1) in embs]\n",
    "                print(f\"smi: {smi} \\nemb: {embs_signs} \\nwith len diff {len(smi)-len(embs)}\")\n",
    "                diffnums += 1\n",
    "                new_embs.append(None)\n",
    "                if len(smi) < len(embs):\n",
    "                    smismaller += 1\n",
    "    embeds[0]=new_embs\n",
    "    if diffnums == 0:\n",
    "        return embeds\n",
    "    else:\n",
    "        print(\n",
    "            f\"same numbers between tokens and embeddings: {samenums} and different number betqween tokens and embeddings: {diffnums} of which smiles tokens have smaller length: {smismaller}\")\n",
    "        perc = (diffnums/(diffnums+samenums))*100\n",
    "        print(\n",
    "            \"percentage of embeddings not correct compared to smiles: {:.2f}%\".format(perc))\n",
    "        return embeds\n",
    "\n",
    "def get_embeddings(task: str, specific_model_path: str, data_path: str, cuda: int, task_reps: List[str]):\n",
    "    \"\"\"Generate the embeddings dict of a task\n",
    "    Args:\n",
    "        task (str): Task to find attention of\n",
    "        cuda (int): CUDA device to use\n",
    "    Returns:\n",
    "        Tuple[List[List[float]], np.ndarray]: attention, labels\n",
    "    \"\"\"\n",
    "    #task_SMILES, task_labels = load_molnet_test_set(task)\n",
    "\n",
    "    #data_path = \"/data/jgut/SMILES_or_SELFIES/task/delaney/smiles_atom_isomers\"\n",
    "    model = load_model(specific_model_path, data_path, cuda)\n",
    "    #print(\"model loaded\")\n",
    "    model.zero_grad()\n",
    "    data_path = data_path / \"input0\" / \"test\"\n",
    "    # True for classification, false for regression\n",
    "    dataset = load_dataset(data_path, True)\n",
    "    source_dictionary = Dictionary.load(str(data_path.parent / \"dict.txt\"))\n",
    "\n",
    "    assert len(task_reps) == len(\n",
    "        dataset\n",
    "    ), f\"Real and filtered dataset {task} do not have same length: len(task_reps): {len(task_reps)} vs. len(dataset):{len(dataset)} .\"\n",
    "    \n",
    "\n",
    "    #text = [canonize_smile(smile) for smile in task_SMILES]\n",
    "    text = [rep for rep in task_reps]\n",
    "    embeds= []\n",
    "    tokenizer = None\n",
    "    if \"bart\" in str(specific_model_path):\n",
    "        embeds.append(\n",
    "            compute_model_output(\n",
    "                dataset,\n",
    "                model,\n",
    "                text, #this is very important to be in same order as task_SMILES which it is\n",
    "                source_dictionary,\n",
    "                False,\n",
    "                False,\n",
    "                True,  # true for embeddings\n",
    "                True,  # true for eos_embeddings\n",
    "                tokenizer,\n",
    "            )[2]\n",
    "        )\n",
    "    if \"roberta\" in str(specific_model_path):\n",
    "        embeds.append(\n",
    "            compute_model_output_RoBERTa(\n",
    "                dataset,\n",
    "                model,\n",
    "                text,\n",
    "                source_dictionary,\n",
    "                False,\n",
    "                False,\n",
    "                True,  # true for embeddings\n",
    "                True,  # true for eos_embeddings\n",
    "                tokenizer,\n",
    "            )[2]\n",
    "        )\n",
    "   # print(\"attention encodings\",len(attention_encodings[0]))\n",
    "   # print(len(attention_encodings))\n",
    "    #output = list(zip(*embeds))\n",
    "    #labels = np.array(task_labels).transpose()[0]\n",
    "    # print(\"labels\",labels)\n",
    "    # print(len(labels))\n",
    "    return embeds\n",
    "\n",
    "def get_embeddings_from_model(task, traintype, model, rep, reps, listoftokenisedreps):\n",
    "    # ----------------------specific model paths for Delaney for BART and RoBERTa-------------------------\n",
    "    finetuned_TASK_MODEL_PATH = Path(\"/data2/jgut/SoS_models\")\n",
    "    pretrained_TASK_MODEL_PATH = Path(\"/data/jgut/SMILES_or_SELFIES/prediction_models\")\n",
    "    # path to finetuned models\n",
    "    subfolder=\"\"\n",
    "    if rep==\"smiles\":\n",
    "        #subfolder = \"smiles_atom_isomers\"\n",
    "        subfolder = \"smiles_atom_standard\"\n",
    "    elif rep==\"selfies\":\n",
    "        #subfolder=\"selfies_atom_isomers\"\n",
    "        subfolder=\"selfies_atom_standard\"\n",
    "        \n",
    "    if traintype==\"finetuned\":\n",
    "        if model==\"BART\":\n",
    "            # path for BART  \n",
    "            specific_model_path = (\n",
    "            finetuned_TASK_MODEL_PATH\n",
    "            / task\n",
    "            / f\"{subfolder}_bart\"\n",
    "            / \"1e-05_0.2_seed_0\" \n",
    "            / \"checkpoint_best.pt\"\n",
    "            )\n",
    "        else:\n",
    "            if rep=='selfies':\n",
    "                #path for RoBERTa\n",
    "                specific_model_path = (\n",
    "                    finetuned_TASK_MODEL_PATH\n",
    "                    / task\n",
    "                    / f\"{subfolder}_roberta\"\n",
    "                    / \"5e-06_0.2_seed_0\" \n",
    "                    / \"checkpoint_best.pt\"\n",
    "                )\n",
    "            else:\n",
    "                #path for RoBERTa\n",
    "                specific_model_path = (\n",
    "                    finetuned_TASK_MODEL_PATH\n",
    "                    / task\n",
    "                    / f\"{subfolder}_roberta\"\n",
    "                    / \"1e-05_0.2_seed_0\" \n",
    "                    / \"checkpoint_best.pt\"\n",
    "                )\n",
    "    # ----------------------specific model paths for pretrained models of BART and RoBERTa-------------------------\n",
    "    elif traintype==\"pretrained\":\n",
    "        if model==\"BART\":\n",
    "            # path for BART   \n",
    "            specific_model_path = (\n",
    "                pretrained_TASK_MODEL_PATH\n",
    "                / f\"{subfolder}_bart\"\n",
    "                / \"checkpoint_last.pt\"\n",
    "            ) \n",
    "        else:\n",
    "            #path for RoBERTa\n",
    "            specific_model_path = (\n",
    "            pretrained_TASK_MODEL_PATH\n",
    "            / f\"{subfolder}_roberta\"\n",
    "            / \"checkpoint_last.pt\"\n",
    "            )\n",
    "    print(\"specific model path: \",specific_model_path)\n",
    "    data_path = TASK_PATH / task / f\"{subfolder}\"\n",
    "    \n",
    "    embeds = []\n",
    "    embeds = get_embeddings(task, specific_model_path, data_path, False, reps) #works for BART model with newest version of fairseq on github, see fairseq_git.yaml file\n",
    "    checked_embeds = check_lengths(listoftokenisedreps, embeds) #, \"Length of SMILES_tokens and embeddings do not agree.\"\n",
    "    print(\"got the embeddings\")\n",
    "    return checked_embeds\n",
    "\n",
    "\n",
    "def get_tokenized_SMILES(task_SMILES: List[str]):\n",
    "    \"\"\"Tokenize SMILES string\n",
    "\n",
    "    Args:\n",
    "        input_list of strings (str): List of SMILES input string\n",
    "\n",
    "    Returns:\n",
    "        dict: dictionary that links canonize SMILES string\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer = get_tokenizer(TOKENIZER_PATH)\n",
    "    print(f\"tokenizer {tokenizer}\")\n",
    "    smi_toks = tokenize_dataset(tokenizer, task_SMILES, False)\n",
    "    smi_toks = [smi_tok.split() for smi_tok in smi_toks]\n",
    "    print(f\"SMILES tokens: {smi_toks[0]}\")\n",
    "    smiles_dict = dict(zip(task_SMILES,smi_toks))\n",
    "    return smiles_dict\n",
    "\n",
    "def load_dictsandinfo_from_jsonfolder(input_folder):\n",
    "    \"\"\"\n",
    "    Load atom assignments and info on failed assignments from folder that contains dictionaries on antechamber atom assignments and info files on failed assignments\n",
    "    :param input_folder: folder that contains atom assignments and info on failed assignments\n",
    "    :return: dict of tasks with dictionary with atom assignments, total number of failed assignments, list of failed SMILES and positions that failed, list of positions that should be kept\n",
    "    \"\"\"\n",
    "    task_dikt = {}\n",
    "    task_totalfails = {}\n",
    "    for file in os.listdir(input_folder):\n",
    "        if file.endswith(\".json\"):\n",
    "            if file.startswith(\"dikt\"):\n",
    "                task = file.split(\"_\")[1].split(\".\")[0]\n",
    "                if task==\"bace\":\n",
    "                    task = \"bace classification\"\n",
    "                with open(os.path.join(input_folder, file), 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                    task_dikt[task] = data\n",
    "                    #totalfails += data['totalfails']\n",
    "                    #failedSmiPos.extend(data['failedSmiPos'])\n",
    "                    #posToKeep_list.extend(data['posToKeep'])\n",
    "            elif file.startswith(\"assignment_info\"):\n",
    "                task=file.split(\".\")[0].split(\"_\")[2]\n",
    "                if task==\"bace\":\n",
    "                    task = \"bace classification\"\n",
    "                with open(os.path.join(input_folder, file), 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                    task_totalfails[task] = data\n",
    "                    #failedSmiPos.extend(data['failedSmiPos'])\n",
    "                    #posToKeep_list.extend(data['posToKeep'])\n",
    "                \n",
    "    return task_dikt, task_totalfails    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_merged_dikt(model, traintype):\n",
    "        # get atom assignments from folder that contains antechamber atom assignments and info files on failed assignments\n",
    "    input_folder = \"./assignment_dicts\"\n",
    "    task_dikt, task_totalfails = load_dictsandinfo_from_jsonfolder(input_folder)\n",
    "    #currently available processed tasks \n",
    "    #task = \"delaney\" --> regression\n",
    "    #task = \"bace_classification\" --> only fails\n",
    "    #task=\"bbbp\" --> classification\n",
    "    #task=\"clearance\" --> regression\n",
    "    onlyworkingtasks=[\"delaney\", \"clearance\", \"bbbp\",\"lipo\"]\n",
    "    test_tasks=[\"delaney\"]\n",
    "    \n",
    "\n",
    "    merged_dikt = {}\n",
    "    #print(f\"totalfails: {totalfails} and total assigned molecules: {len(dikt.keys())}\")\n",
    "    for key, val in task_dikt.items():\n",
    "        print(\"TASK: \",key)\n",
    "        task=key\n",
    "        if task not in onlyworkingtasks:\n",
    "            continue\n",
    "        #print(f\"SMILES task: {key} \\nwith dict_keys {val.keys()}\")\n",
    "        #print(task_dikt[key].keys())\n",
    "        dikt=task_dikt[key]\n",
    "        totalfails = task_totalfails[key]['totalfails']\n",
    "        task_SMILES=dikt.keys()\n",
    "        \n",
    "        # have to tokenize SMILES just like previously in e.g. 2_AssignEmbedsPlot.py\n",
    "        smiles_dict = get_tokenized_SMILES(task_SMILES)\n",
    "        #for key2, val2 in val.items():\n",
    "        #    print(f\"{key2}: {val2}\")\n",
    "        percentagefailures = (totalfails/len(dikt.keys()))*100\n",
    "        print(f\"total fails for task {task}: {totalfails} out of {len(dikt.keys())} SMILES ({percentagefailures:.2f}%) \")\n",
    "        #get embeddings from model\n",
    "        #model = \"ROBERTA\"\n",
    "        #model=\"BART\"\n",
    "        #traintype = \"pretrained\"\n",
    "        #rep = \"smiles\"\n",
    "        # task needs specifiyng for loading of finetuned model\n",
    "        task = key\n",
    "        ########################## Get embeddings from model for SMILES ############################################\n",
    "        try:\n",
    "            rep=\"smiles\"\n",
    "            print(\"get embeddings\")\n",
    "            embeds = get_embeddings_from_model(task, traintype, model, rep, smiles_dict.keys(), smiles_dict.values())\n",
    "            #get rid of embeddings that encode for digits or hydrogens\n",
    "            embeds_clean = get_clean_embeds(embeds, dikt, totalfails, task_SMILES)\n",
    "            # within the dikt, map embeddings to atom types\n",
    "            map_embeddings_to_atomtypes(dikt,task_SMILES)\n",
    "            print()\n",
    "            \n",
    "            ## SELFIES------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "            # get SELFIES equivalent of SMILES and mapping between them\n",
    "            smiles_to_selfies_mapping = generate_mappings_for_task_SMILES_to_SELFIES(task_SMILES)\n",
    "            \n",
    "            selfies_tokenised = []\n",
    "            selfies = []\n",
    "            maps_num = 0\n",
    "            for key in smiles_to_selfies_mapping.keys():\n",
    "                print(f\"SMILES: {key} SELFIES: {smiles_to_selfies_mapping[key]}\")\n",
    "                selfies_tokenised.append(smiles_to_selfies_mapping[key]['selfiesstr_tok_map'][1])\n",
    "                selfies.append(smiles_to_selfies_mapping[key]['selfiesstr_tok_map'][0])\n",
    "                if smiles_to_selfies_mapping[key]['selfiesstr_tok_map'][2] is not None:\n",
    "                    maps_num +=1\n",
    "                    for key2,val in smiles_to_selfies_mapping[key]['selfiesstr_tok_map'][2].items():\n",
    "                        print(f\"SELFIES index:{key2[0]} with token:{key2[1]}\\tmaps to SMILES token at pos: {val}\")\n",
    "                print()\n",
    "                \n",
    "            print(f\"list of tokenised selfies: {selfies_tokenised}\")\n",
    "            print(f\"selfies {selfies} \\nwith len() {len(selfies)}\")\n",
    "            print(f\"mappings {maps_num}\")\n",
    "            \n",
    "            rep=\"selfies\"\n",
    "            # traintype and model speicfied above\n",
    "            embeds_selfies = get_embeddings_from_model(task, traintype, model, rep, selfies, selfies_tokenised)\n",
    "            \n",
    "            # map selfies embeddings to smiles in smiles_dict\n",
    "            map_selfies_embeddings_to_smiles(embeds_selfies, smiles_to_selfies_mapping, dikt)\n",
    "            for key, val in dikt.items():\n",
    "                dikt[key]['task']=task\n",
    "            \n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            continue\n",
    "        merged_dikt.update(dikt)\n",
    "        \n",
    "    print(len(merged_dikt.keys()))\n",
    "    print(merged_dikt.keys())\n",
    "    valid_keys_count = len([key for key in merged_dikt.keys() if merged_dikt[key]['posToKeep'] is not None and merged_dikt[key]['atomtype_to_embedding'] is not None and merged_dikt[key]['atomtype_to_clean_selfies_embedding'] is not None])\n",
    "    print(\"==============================================================================================================================================\")\n",
    "    print(f\"Number of valid keys in final merged_dikt: {valid_keys_count}\")\n",
    "    return merged_dikt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged dicts were already created, no need to create them again.\n"
     ]
    }
   ],
   "source": [
    "create_dicts=False\n",
    "if create_dicts:\n",
    "    traintype=\"pretrained\"\n",
    "    model =\"BART\"\n",
    "    merged_dikt_bart = create_merged_dikt(model, traintype)\n",
    "\n",
    "    model =\"roberta\"\n",
    "    merged_dikt_roberta = create_merged_dikt(model, traintype)\n",
    "\n",
    "    print(merged_dikt_bart.keys())\n",
    "    print(len(merged_dikt_bart.keys()))\n",
    "\n",
    "    print(merged_dikt_roberta.keys())\n",
    "    print(len(merged_dikt_roberta.keys()))\n",
    "\n",
    "    #confirm it's the same keys please \n",
    "    for key in merged_dikt_bart.keys():\n",
    "        if key not in merged_dikt_roberta.keys():\n",
    "            print(f\"key {key} not in roberta\")\n",
    "    for key,val in zip(merged_dikt_bart.keys(),merged_dikt_roberta.keys()):\n",
    "        #print(key, val)\n",
    "        if key!=val:\n",
    "            print(f\"key {key} not equal to val {val}\")\n",
    "\n",
    "    # save merged dicts\n",
    "    import json\n",
    "    with open('./assignment_dicts/merged_dikt_bart.json', 'w') as f:\n",
    "        json.dump(merged_dikt_bart, f)\n",
    "    with open('./assignment_dicts/merged_dikt_roberta.json', 'w') as f:\n",
    "        json.dump(merged_dikt_roberta, f)\n",
    "else:\n",
    "    print(\"Merged dicts were already created, no need to create them again.\")\n",
    "    with open('./assignment_dicts/merged_dikt_bart.json', 'r') as f:\n",
    "        merged_dikt_bart = json.load(f)\n",
    "    with open('./assignment_dicts/merged_dikt_roberta.json', 'r') as f:\n",
    "        merged_dikt_roberta = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "791\n",
      "791\n"
     ]
    }
   ],
   "source": [
    "print(len(merged_dikt_bart.keys()))\n",
    "print(len(merged_dikt_roberta.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dicts_on_parmchk_penalty_threshold(merged_dict_bart, merged_dict_roberta):\n",
    "    \n",
    "    # filter on parmchk penalty threshold\n",
    "    penalty_threshold = 300\n",
    "    \n",
    "    \n",
    "    #filter both dicts at the same time based on penalty threshold\n",
    "    filtered_merged_dict_bart = {smiles: info for smiles, info in merged_dict_bart.items() if info['max_penalty'] is not None and info['max_penalty'] < penalty_threshold}\n",
    "    filtered_merged_dict_roberta = {smiles: merged_dict_roberta[smiles] for smiles in filtered_merged_dict_bart if smiles in merged_dict_roberta}\n",
    "     \n",
    "    print(f\"Len before filtering - bart_dict: {len(merged_dict_bart.keys())} and after threshold filtering: {len(filtered_merged_dict_bart.keys())}\")\n",
    "    print(f\"Len before filtering - roberta_dict: {len(merged_dict_roberta.keys())} and after threshold filtering: {len(filtered_merged_dict_roberta.keys())}\")\n",
    "    \n",
    "    # there are less SELFIES embeddings that could be mapped to the SMILES and thereby the atomtypes, so the number of embeddings is less\n",
    "    # therefore filter filtered_dict further on what is available for SELFIES and also what is available for SMILES\n",
    "    filtered_embs_bart_dict = {smiles: info for smiles, info in filtered_merged_dict_bart.items() if info['atomtype_to_embedding'] is not None and info['atomtype_to_clean_selfies_embedding'] is not None}\n",
    "    filtered_embs_roberta_dict = {smiles: filtered_merged_dict_roberta[smiles] for smiles in filtered_embs_bart_dict if smiles in filtered_merged_dict_roberta}\n",
    "    \n",
    "    \n",
    "    print(f\"Len before filtering - bart_dict: {len(filtered_merged_dict_bart.keys())} and after embedding filtering: {len(filtered_embs_bart_dict.keys())}\")\n",
    "    print(f\"Len before filtering - roberta_dict: {len(filtered_merged_dict_roberta.keys())} and after embedding filtering: {len(filtered_embs_roberta_dict.keys())}\")\n",
    "    \n",
    "    \n",
    "    return filtered_merged_dict_bart, filtered_merged_dict_roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len before filtering - bart_dict: 791 and after threshold filtering: 479\n",
      "Len before filtering - roberta_dict: 791 and after threshold filtering: 479\n",
      "Len before filtering - bart_dict: 479 and after embedding filtering: 143\n",
      "Len before filtering - roberta_dict: 479 and after embedding filtering: 143\n"
     ]
    }
   ],
   "source": [
    "filtered_embs_bart_dict, filtered_merged_dict_roberta = filter_dicts_on_parmchk_penalty_threshold(merged_dikt_bart, merged_dikt_roberta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('sash', {'c': {'c': '#e6194B', 'c1': '#3cb44b', 'c2': '#ffe119', 'c3': '#4363d8', 'ca': '#f58231'}, 'o': {'o': '#e6194B', 'oh': '#3cb44b', 'os': '#ffe119'}, 'n': {'n': '#e6194B', 'n1': '#3cb44b', 'n2': '#ffe119', 'n3': '#4363d8', 'n4': '#f58231', 'na': '#911eb4', 'nh': '#42d4f4', 'no': '#f032e6'}}), ('tol', {'c': {'c': '#882e72', 'c1': '#1965B0', 'c2': '#7bafde', 'c3': '#4eb265', 'ca': '#cae0ab'}, 'o': {'o': '#882e72', 'oh': '#1965B0', 'os': '#7bafde'}, 'n': {'n': '#882e72', 'n1': '#1965B0', 'n2': '#7bafde', 'n3': '#4eb265', 'n4': '#cae0ab', 'na': '#f7f056', 'nh': '#f1932d', 'no': '#dc050c'}}), ('viridis', {'c': {'c': '#440154', 'c1': '#46327e', 'c2': '#365c8d', 'c3': '#277f8e', 'ca': '#1fa187'}, 'o': {'o': '#440154', 'oh': '#46327e', 'os': '#365c8d'}, 'n': {'n': '#440154', 'n1': '#46327e', 'n2': '#365c8d', 'n3': '#277f8e', 'n4': '#1fa187', 'na': '#4ac16d', 'nh': '#a0da39', 'no': '#fde725'}})])\n",
      "{'c': {'c': '#e6194B', 'c1': '#3cb44b', 'c2': '#ffe119', 'c3': '#4363d8', 'ca': '#f58231'}, 'o': {'o': '#e6194B', 'oh': '#3cb44b', 'os': '#ffe119'}, 'n': {'n': '#e6194B', 'n1': '#3cb44b', 'n2': '#ffe119', 'n3': '#4363d8', 'n4': '#f58231', 'na': '#911eb4', 'nh': '#42d4f4', 'no': '#f032e6'}}\n",
      "dict_items([('c', {'c': 'o', 'c1': 's', 'c2': '^', 'c3': 'v', 'ca': 'D'}), ('o', {'o': 'o', 'oh': 's', 'os': '^'}), ('n', {'n': 'o', 'n1': 's', 'n2': '^', 'n3': 'v', 'n4': 'D', 'na': 'P', 'nh': 'X', 'no': '*'})])\n",
      "{'c': {'c': 'o', 'c1': 's', 'c2': '^', 'c3': 'v', 'ca': 'D'}, 'o': {'o': 'o', 'oh': 's', 'os': '^'}, 'n': {'n': 'o', 'n1': 's', 'n2': '^', 'n3': 'v', 'n4': 'D', 'na': 'P', 'nh': 'X', 'no': '*'}}\n"
     ]
    }
   ],
   "source": [
    "# create \n",
    "unique_atomtype_set = set(chain.from_iterable(filtered_embs_bart_dict[key]['atom_types'] for key in filtered_embs_bart_dict if filtered_embs_bart_dict[key].get('atom_types') is not None))\n",
    "basic_atomtypes_to_elems_dict = {'c': ['c', 'c1', 'c2', 'c3', 'ca'], 'o': ['o', 'oh', 'os'], 'n': ['n', 'n1', 'n2', 'n3', 'n4', 'na', 'nh', 'no']}\n",
    "atomtypes_to_elems_dict = create_elementsubsets(unique_atomtype_set)\n",
    "\n",
    "\n",
    "#colors=\"tol\"\n",
    "# get colors for atomtypes by element and element groups\n",
    "colordict, markerdict = colorstoatomtypesbyelement(basic_atomtypes_to_elems_dict)\n",
    "#print(colordict['sash']['c'])\n",
    "print(markerdict)\n",
    "\n",
    "color=\"tol\"\n",
    "colorsubdict=colordict[color]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len atomtype to embedding list smiles:  479\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m atomtype_embedding_perelem_dict_smiles_bart \u001b[38;5;241m=\u001b[39m \u001b[43mget_atomtype_embedding_perelem_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiltered_embs_bart_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolorsubdict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43matomtype_to_embedding\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m atomtype_embedding_perelem_dict_selfies_bart \u001b[38;5;241m=\u001b[39m get_atomtype_embedding_perelem_dict(filtered_embs_bart_dict, colorsubdict, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124matomtype_to_clean_selfies_embedding\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m atomtype_embedding_perelem_dict_smiles_roberta \u001b[38;5;241m=\u001b[39m get_atomtype_embedding_perelem_dict(filtered_merged_dict_roberta, colorsubdict, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124matomtype_to_embedding\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 12\u001b[0m, in \u001b[0;36mget_atomtype_embedding_perelem_dict\u001b[0;34m(filtered_dict, colordict, entry_name_atomtype_to_embedding)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m atom_type_list \u001b[38;5;129;01min\u001b[39;00m atomtype_to_embedding_lists:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# go through single dictionary\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m \u001b[38;5;129;01min\u001b[39;00m atom_type_list:\n\u001b[1;32m     11\u001b[0m        \u001b[38;5;66;03m# print(f\"atomtype {atom_type} embeddings {embeddings[1]}\")\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m embeddings_by_atomtype:\n\u001b[1;32m     13\u001b[0m             embeddings_by_atomtype[\u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;241m0\u001b[39m]] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;66;03m# extend the list of embeddings for this atom type(, but ONLY by the embedding not the attached token)\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "atomtype_embedding_perelem_dict_smiles_bart = get_atomtype_embedding_perelem_dict(filtered_embs_bart_dict, colorsubdict, 'atomtype_to_embedding')\n",
    "atomtype_embedding_perelem_dict_selfies_bart = get_atomtype_embedding_perelem_dict(filtered_embs_bart_dict, colorsubdict, 'atomtype_to_clean_selfies_embedding')\n",
    "\n",
    "atomtype_embedding_perelem_dict_smiles_roberta = get_atomtype_embedding_perelem_dict(filtered_merged_dict_roberta, colorsubdict, 'atomtype_to_embedding')\n",
    "atomtype_embedding_perelem_dict_selfies_roberta = get_atomtype_embedding_perelem_dict(filtered_merged_dict_roberta, colorsubdict, 'atomtype_to_clean_selfies_embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "    # following this, the dict looks as follows: atomtype_to_dict should be a list of tuples with atomtype and embeddings    \n",
    "    # dikt[SMILES] with dict_keys(['posToKeep', 'smi_clean', 'atom_types', 'max_penalty', 'orig_embedding', 'clean_embedding', 'atomtype_to_embedding', 'atomtype_to_clean_selfies_embedding'])\n",
    "\n",
    "    # SELFIES embeddings mapped to atomtypes-------------------------------------------------------------------------------------------------------------------------     \n",
    "    # following this, the dict looks as follows: atomtype_to_dict should be a list of tuples with atomtype and embeddings    \n",
    "    # dikt[SMILES] with dict_keys(['posToKeep', 'smi_clean', 'atom_types', 'max_penalty', 'orig_embedding', 'clean_embedding', 'atomtype_to_embedding', 'atomtype_to_clean_selfies_embedding'])\n",
    "    \n",
    "    unique_atomtype_set = set(chain.from_iterable(merged_dikt[key]['atom_types'] for key in merged_dikt if merged_dikt[key].get('atom_types') is not None))\n",
    "    basic_atomtypes_to_elems_dict = {'c': ['c', 'c1', 'c2', 'c3', 'ca'], 'o': ['o', 'oh', 'os'], 'n': ['n', 'n1', 'n2', 'n3', 'n4', 'na', 'nh', 'no']}\n",
    "    atomtypes_to_elems_dict = create_elementsubsets(unique_atomtype_set)\n",
    "\n",
    "    \n",
    "    #colors=\"tol\"\n",
    "    # get colors for atomtypes by element and element groups\n",
    "    colordict, markerdict = colorstoatomtypesbyelement(basic_atomtypes_to_elems_dict)\n",
    "    #print(colordict['sash']['c'])\n",
    "    print(markerdict)\n",
    "    \n",
    "    penalty_threshold = 300\n",
    "    # Assuming 'dikt' is your dictionary and each value has a 'penalty_score' key\n",
    "    filtered_dict_filterthresh = {smiles: info for smiles, info in merged_dikt.items() if info['max_penalty'] is not None and info['max_penalty'] < penalty_threshold}\n",
    "    \n",
    "    # there are less SELFIES embeddings that could be mapped to the SMILES and thereby the atomtypes, so the number of embeddings is less\n",
    "    # therefore filter filtered_dict further on what is available for SELFIES and also what is available for SMILES\n",
    "    filtered_dict = {smiles: info for smiles, info in filtered_dict_filterthresh.items() if info['atomtype_to_embedding'] is not None and info['atomtype_to_clean_selfies_embedding'] is not None}\n",
    "    print(\"keys in filtered dict:\",len(filtered_dict.keys()))\n",
    "    print(\"keys in filtered dict:\",filtered_dict.keys())\n",
    "    \n",
    "    save_path_prefix = f\"./TEST_10_2_{model}_{traintype}_delaney_bbbp_clearance_lipo_mols{valid_keys_count}_thresh{penalty_threshold}/\"\n",
    "    \n",
    "    # only plot using color scheme tol\n",
    "    color=\"tol\"\n",
    "    colorsubdict=colordict[color]\n",
    "    min_dist = 0.1\n",
    "    n_neighbors = 15\n",
    "    alpha = 0.6\n",
    "    \n",
    "        # -------------------------SMILES\n",
    "    atomtype_embedding_perelem_dict_smiles = get_atomtype_embedding_perelem_dict(filtered_dict, colorsubdict, 'atomtype_to_embedding')\n",
    "    #print(f\"len of atomtype embs per elem smiles: {len(atomtype_embedding_perelem_dict_smiles['sash'])}\")\n",
    "    print(\"keys: \",atomtype_embedding_perelem_dict_smiles.keys())\n",
    "    #print(\"keys: \",atomtype_embedding_perelem_dict_smiles['sash']['c'][0])\n",
    "    print('=======================')\n",
    "    # for x in atomtype_embedding_perelem_dict_smiles['sash']:\n",
    "    #     print(x)\n",
    "    # print('=======================')\n",
    "    #first_items = [tup for tup in atomtype_embedding_perelem_dict_smiles['sash']]\n",
    "    #print(\"First items: \", len(first_items))\n",
    "    #print(\"First items: \", first_items)\n",
    "    #------------------------------SELFIES \n",
    "    # print(\"plotting SELFIES\")\n",
    "    atomtype_embedding_perelem_dict_selfies = get_atomtype_embedding_perelem_dict(filtered_dict, colorsubdict, 'atomtype_to_clean_selfies_embedding')\n",
    "    #print(f\"len of atomtype embs per elem selfies: {len(atomtype_embedding_perelem_dict_selfies)}\")\n",
    "    #print(\"keys: \", atomtype_embedding_perelem_dict_selfies.keys())\n",
    "\n",
    "    #create_plotsperelem(atomtype_embedding_perelem_dict_smiles, atomtype_embedding_perelem_dict_selfies, markerdict, colorsubdict, min_dist, n_neighbors, alpha, save_path_prefix, color)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fairseq_git2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
