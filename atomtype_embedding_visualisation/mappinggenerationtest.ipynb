{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6f305b0",
   "metadata": {},
   "source": [
    "# Testing mapping generation of SMILES to SELFIES WITHOUT canonization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32603596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selfies\n",
    "from rdkit import Chem\n",
    "import re\n",
    "\n",
    "#from constants\n",
    "PARSING_REGEX = r\"(<unk>|\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\|\\/|:|~|@|\\?|>>?|\\*|\\$|\\%[0-9]{2}|[0-9])\"\n",
    "\n",
    "#from preprocessing\n",
    "def canonize_smiles(input_str: str, remove_identities: bool = True) -> str:\n",
    "    \"\"\"Canonize SMILES string\n",
    "\n",
    "    Args:\n",
    "        input_str (str): SMILES input string\n",
    "\n",
    "    Returns:\n",
    "        str: canonize SMILES string\n",
    "    \"\"\"\n",
    "    mol = Chem.MolFromSmiles(input_str)\n",
    "    if mol is None:\n",
    "        return None\n",
    "    # not sure remove_identities is neccessary for generate mapping, cannot see a difference\n",
    "    if remove_identities:\n",
    "        [a.SetAtomMapNum(0) for a in mol.GetAtoms()]\n",
    "\n",
    "    return Chem.MolToSmiles(mol)\n",
    "\n",
    "def generate_mapping(smiles, debug=False):\n",
    "    # this mapping function is build on the basis that \n",
    "    # 1. the SELFIES string is generated from a canonized SMILES string\n",
    "    # 2. the order of the atoms in both strings is the same\n",
    "    # see: https://github.com/aspuru-guzik-group/selfies/blob/master/README.md \n",
    "    #canon_smiles = canonize_smiles(smiles)\n",
    "    #assert smiles==canon_smiles, f\"SMILES input was not canonized: {smiles} -> {canon_smiles}\"\n",
    "    \n",
    "    selfies_str, attr = selfies.encoder(smiles, attribute=True)\n",
    "    #print(attr)\n",
    "    #attr is a list of AttributionMaps containing the output token, its index, and input tokens that led to it.\n",
    "    # e.g AttributionMap(index=0, token='[O]', attribution=[Attribution(index=0, token='O')]),\n",
    "    ind_list = [att.index for att in attr]\n",
    "    index_dict = {elem: ind_list.count(elem) for elem in set(ind_list)}\n",
    "    tokenised_smiles = [elem for elem in re.split(PARSING_REGEX,canonize_smiles(smiles)) if elem]\n",
    "    tokenised_selfies = list(selfies.split_selfies(selfies_str))\n",
    "    # Items map indices and tokens from the SELFIES representation to indices in the tokenized SMILES string. \n",
    "    # This mapping only includes tokens that do not represent rings or branches, occur exactly once (index_dict[att.index]==1), and match the corresponding token in the tokenized SELFIES string.\n",
    "    # here att.index is the index of the token in the SELFIES string, att token is token in SELFIES string, and att.attribution[0].index is the index of the token in the SMILES string\n",
    "    items = {(att.index, att.token):att.attribution[0].index for att in attr if (att.attribution and not ((\"Ring\" in att.token) or (\"Branch\" in att.token)) and index_dict[att.index]==1 and att.token == tokenised_selfies[att.index])}\n",
    "    if debug:\n",
    "        print(smiles)\n",
    "        print(selfies_str)\n",
    "        print(attr)\n",
    "        print(index_dict)\n",
    "        print(items)\n",
    "        for key, value in items.items():\n",
    "            print(f\"{list(selfies.split_selfies(selfies_str))[key[0]]} to {tokenised_smiles[value]}, {key} to {value}\")\n",
    "        print(\"missing SMILES\")\n",
    "        print([(val,tokenised_smiles[val]) for val in range(len(tokenised_smiles)) if not (val in list(items.values())) ] )\n",
    "        print([(key,tokenised_selfies[key]) for key in range(len(tokenised_selfies)) if not (key in list(items.keys())) ] )\n",
    "    else:\n",
    "        # Checks that there are no duplicate indices in the SELFIES tokens part of the mapping. \n",
    "        # #This is done by comparing the length of the set of SELFIES indices (which removes duplicates) with the length of the list of these indices. If the lengths are equal, there are no duplicates.\n",
    "        no_dupes_i = len(set([key[0] for key in items.keys()]))==len([key[0] for key in items.keys()])\n",
    "        # Similar to no_dupes_i, but checks for no duplicate indices in the SMILES part of the mapping.\n",
    "        no_dupes_ii = len(set([val for val in items.values()]))==len([val for val in items.values()])\n",
    "        # Ensures there are no alphabetic SMILES tokens left unmapped. \n",
    "        # It checks if any alphabetic token in the tokenized SMILES string is not part of the mapping (items.values()), indicating a potentially incomplete mapping.\n",
    "        no_leftover_smiles = not any([tokenised_smiles[val].isalpha() for val in range(len(tokenised_smiles)) if not (val in list(items.values()))])\n",
    "        correct_letter = not any([\"\".join([v.upper() for v in tokenised_smiles[val] if v.isalpha() or v==\"@\"]) not in key[1] for (key, val) in list(items.items()) if any([v.isalpha() for v in tokenised_smiles[val]])])\n",
    "        if no_dupes_i and no_dupes_ii and no_leftover_smiles and correct_letter:\n",
    "            return selfies_str,tokenised_selfies,items\n",
    "        else:\n",
    "            return selfies_str,tokenised_selfies,None\n",
    "        \n",
    "def generate_mappings_for_task_SMILES_to_SELFIES(task_SMILES):\n",
    "    mappings = {}\n",
    "    for smiles in task_SMILES:\n",
    "        selfies_str,tokenised_selfies,mapping = generate_mapping(smiles)\n",
    "        mappings[smiles] = {}\n",
    "        mappings[smiles]['selfiesstr_tok_map'] = (selfies_str,tokenised_selfies,mapping)\n",
    "    return mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c30f9435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CCOC(=O)[C@H](CCC1=CC=CC=C1)N[C@@H](C)C(=O)N1CCC[C@H]1C(=O)O\n",
      "[C][C][O][C][=Branch1][C][=O][C@H1][Branch1][O][C][C][C][=C][C][=C][C][=C][Ring1][=Branch1][N][C@@H1][Branch1][C][C][C][=Branch1][C][=O][N][C][C][C][C@H1][Ring1][Branch1][C][=Branch1][C][=O][O]\n",
      "AttributionMap(index=0, token='[C]', attribution=[Attribution(index=0, token='C')])\n",
      "AttributionMap(index=1, token='[C]', attribution=[Attribution(index=1, token='C')])\n",
      "AttributionMap(index=2, token='[O]', attribution=[Attribution(index=2, token='O')])\n",
      "AttributionMap(index=3, token='[C]', attribution=[Attribution(index=3, token='C')])\n",
      "AttributionMap(index=6, token='[=O]', attribution=[Attribution(index=6, token='O')])\n",
      "AttributionMap(index=5, token='[C]', attribution=[Attribution(index=6, token='O')])\n",
      "AttributionMap(index=5, token='[=Branch1]', attribution=[Attribution(index=6, token='O')])\n",
      "AttributionMap(index=7, token='[C@H1]', attribution=[Attribution(index=8, token='[C@H]')])\n",
      "AttributionMap(index=10, token='[C]', attribution=[Attribution(index=10, token='C')])\n",
      "AttributionMap(index=11, token='[C]', attribution=[Attribution(index=11, token='C')])\n",
      "AttributionMap(index=12, token='[C]', attribution=[Attribution(index=12, token='C')])\n",
      "AttributionMap(index=13, token='[=C]', attribution=[Attribution(index=15, token='C')])\n",
      "AttributionMap(index=14, token='[C]', attribution=[Attribution(index=16, token='C')])\n",
      "AttributionMap(index=15, token='[=C]', attribution=[Attribution(index=18, token='C')])\n",
      "AttributionMap(index=16, token='[C]', attribution=[Attribution(index=19, token='C')])\n",
      "AttributionMap(index=17, token='[=C]', attribution=[Attribution(index=21, token='C')])\n",
      "AttributionMap(index=18, token='[Ring1]', attribution=None)\n",
      "AttributionMap(index=19, token='[=Branch1]', attribution=None)\n",
      "AttributionMap(index=9, token='[O]', attribution=[Attribution(index=10, token='C')])\n",
      "AttributionMap(index=9, token='[Branch1]', attribution=[Attribution(index=10, token='C')])\n",
      "AttributionMap(index=20, token='[N]', attribution=[Attribution(index=24, token='N')])\n",
      "AttributionMap(index=21, token='[C@@H1]', attribution=[Attribution(index=25, token='[C@@H]')])\n",
      "AttributionMap(index=24, token='[C]', attribution=[Attribution(index=27, token='C')])\n",
      "AttributionMap(index=23, token='[C]', attribution=[Attribution(index=27, token='C')])\n",
      "AttributionMap(index=23, token='[Branch1]', attribution=[Attribution(index=27, token='C')])\n",
      "AttributionMap(index=25, token='[C]', attribution=[Attribution(index=29, token='C')])\n",
      "AttributionMap(index=28, token='[=O]', attribution=[Attribution(index=32, token='O')])\n",
      "AttributionMap(index=27, token='[C]', attribution=[Attribution(index=32, token='O')])\n",
      "AttributionMap(index=27, token='[=Branch1]', attribution=[Attribution(index=32, token='O')])\n",
      "AttributionMap(index=29, token='[N]', attribution=[Attribution(index=34, token='N')])\n",
      "AttributionMap(index=30, token='[C]', attribution=[Attribution(index=36, token='C')])\n",
      "AttributionMap(index=31, token='[C]', attribution=[Attribution(index=37, token='C')])\n",
      "AttributionMap(index=32, token='[C]', attribution=[Attribution(index=38, token='C')])\n",
      "AttributionMap(index=33, token='[C@H1]', attribution=[Attribution(index=39, token='[C@H]')])\n",
      "AttributionMap(index=34, token='[Ring1]', attribution=None)\n",
      "AttributionMap(index=35, token='[Branch1]', attribution=None)\n",
      "AttributionMap(index=36, token='[C]', attribution=[Attribution(index=41, token='C')])\n",
      "AttributionMap(index=39, token='[=O]', attribution=[Attribution(index=44, token='O')])\n",
      "AttributionMap(index=38, token='[C]', attribution=[Attribution(index=44, token='O')])\n",
      "AttributionMap(index=38, token='[=Branch1]', attribution=[Attribution(index=44, token='O')])\n",
      "AttributionMap(index=40, token='[O]', attribution=[Attribution(index=46, token='O')])\n",
      "[AttributionMap(index=0, token='[C]', attribution=[Attribution(index=0, token='C')]), AttributionMap(index=1, token='[C]', attribution=[Attribution(index=1, token='C')]), AttributionMap(index=2, token='[O]', attribution=[Attribution(index=2, token='O')]), AttributionMap(index=3, token='[C]', attribution=[Attribution(index=3, token='C')]), AttributionMap(index=6, token='[=O]', attribution=[Attribution(index=6, token='O')]), AttributionMap(index=5, token='[C]', attribution=[Attribution(index=6, token='O')]), AttributionMap(index=5, token='[=Branch1]', attribution=[Attribution(index=6, token='O')]), AttributionMap(index=7, token='[C@H1]', attribution=[Attribution(index=8, token='[C@H]')]), AttributionMap(index=10, token='[C]', attribution=[Attribution(index=10, token='C')]), AttributionMap(index=11, token='[C]', attribution=[Attribution(index=11, token='C')]), AttributionMap(index=12, token='[C]', attribution=[Attribution(index=12, token='C')]), AttributionMap(index=13, token='[=C]', attribution=[Attribution(index=15, token='C')]), AttributionMap(index=14, token='[C]', attribution=[Attribution(index=16, token='C')]), AttributionMap(index=15, token='[=C]', attribution=[Attribution(index=18, token='C')]), AttributionMap(index=16, token='[C]', attribution=[Attribution(index=19, token='C')]), AttributionMap(index=17, token='[=C]', attribution=[Attribution(index=21, token='C')]), AttributionMap(index=18, token='[Ring1]', attribution=None), AttributionMap(index=19, token='[=Branch1]', attribution=None), AttributionMap(index=9, token='[O]', attribution=[Attribution(index=10, token='C')]), AttributionMap(index=9, token='[Branch1]', attribution=[Attribution(index=10, token='C')]), AttributionMap(index=20, token='[N]', attribution=[Attribution(index=24, token='N')]), AttributionMap(index=21, token='[C@@H1]', attribution=[Attribution(index=25, token='[C@@H]')]), AttributionMap(index=24, token='[C]', attribution=[Attribution(index=27, token='C')]), AttributionMap(index=23, token='[C]', attribution=[Attribution(index=27, token='C')]), AttributionMap(index=23, token='[Branch1]', attribution=[Attribution(index=27, token='C')]), AttributionMap(index=25, token='[C]', attribution=[Attribution(index=29, token='C')]), AttributionMap(index=28, token='[=O]', attribution=[Attribution(index=32, token='O')]), AttributionMap(index=27, token='[C]', attribution=[Attribution(index=32, token='O')]), AttributionMap(index=27, token='[=Branch1]', attribution=[Attribution(index=32, token='O')]), AttributionMap(index=29, token='[N]', attribution=[Attribution(index=34, token='N')]), AttributionMap(index=30, token='[C]', attribution=[Attribution(index=36, token='C')]), AttributionMap(index=31, token='[C]', attribution=[Attribution(index=37, token='C')]), AttributionMap(index=32, token='[C]', attribution=[Attribution(index=38, token='C')]), AttributionMap(index=33, token='[C@H1]', attribution=[Attribution(index=39, token='[C@H]')]), AttributionMap(index=34, token='[Ring1]', attribution=None), AttributionMap(index=35, token='[Branch1]', attribution=None), AttributionMap(index=36, token='[C]', attribution=[Attribution(index=41, token='C')]), AttributionMap(index=39, token='[=O]', attribution=[Attribution(index=44, token='O')]), AttributionMap(index=38, token='[C]', attribution=[Attribution(index=44, token='O')]), AttributionMap(index=38, token='[=Branch1]', attribution=[Attribution(index=44, token='O')]), AttributionMap(index=40, token='[O]', attribution=[Attribution(index=46, token='O')])]\n"
     ]
    }
   ],
   "source": [
    "smiles = 'CCOC(=O)[C@H](CCC1=CC=CC=C1)N[C@@H](C)C(=O)N1CCC[C@H]1C(=O)O'\n",
    "print(smiles)\n",
    "selfies_str, attr = selfies.encoder(smiles, attribute=True)\n",
    "print(selfies_str)\n",
    "for pc in attr:\n",
    "    print(pc)\n",
    "print(attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a1ae272",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m selfies_str,tokenised_selfies,mapping \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_mapping\u001b[49m\u001b[43m(\u001b[49m\u001b[43msmiles\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 67\u001b[0m, in \u001b[0;36mgenerate_mapping\u001b[0;34m(smiles, debug)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Ensures there are no alphabetic SMILES tokens left unmapped. \u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# It checks if any alphabetic token in the tokenized SMILES string is not part of the mapping (items.values()), indicating a potentially incomplete mapping.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m no_leftover_smiles \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m([tokenised_smiles[val]\u001b[38;5;241m.\u001b[39misalpha() \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(tokenised_smiles)) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(items\u001b[38;5;241m.\u001b[39mvalues()))])\n\u001b[0;32m---> 67\u001b[0m correct_letter \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([v\u001b[38;5;241m.\u001b[39mupper() \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m tokenised_smiles[val] \u001b[38;5;28;01mif\u001b[39;00m v\u001b[38;5;241m.\u001b[39misalpha() \u001b[38;5;129;01mor\u001b[39;00m v\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m@\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m key[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m (key, val) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(items\u001b[38;5;241m.\u001b[39mitems()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m([v\u001b[38;5;241m.\u001b[39misalpha() \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m tokenised_smiles[val]])])\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m no_dupes_i \u001b[38;5;129;01mand\u001b[39;00m no_dupes_ii \u001b[38;5;129;01mand\u001b[39;00m no_leftover_smiles \u001b[38;5;129;01mand\u001b[39;00m correct_letter:\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m selfies_str,tokenised_selfies,items\n",
      "Cell \u001b[0;32mIn[15], line 67\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Ensures there are no alphabetic SMILES tokens left unmapped. \u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# It checks if any alphabetic token in the tokenized SMILES string is not part of the mapping (items.values()), indicating a potentially incomplete mapping.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m no_leftover_smiles \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m([tokenised_smiles[val]\u001b[38;5;241m.\u001b[39misalpha() \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(tokenised_smiles)) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(items\u001b[38;5;241m.\u001b[39mvalues()))])\n\u001b[0;32m---> 67\u001b[0m correct_letter \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([v\u001b[38;5;241m.\u001b[39mupper() \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m tokenised_smiles[val] \u001b[38;5;28;01mif\u001b[39;00m v\u001b[38;5;241m.\u001b[39misalpha() \u001b[38;5;129;01mor\u001b[39;00m v\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m@\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m key[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m (key, val) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(items\u001b[38;5;241m.\u001b[39mitems()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m([v\u001b[38;5;241m.\u001b[39misalpha() \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtokenised_smiles\u001b[49m\u001b[43m[\u001b[49m\u001b[43mval\u001b[49m\u001b[43m]\u001b[49m])])\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m no_dupes_i \u001b[38;5;129;01mand\u001b[39;00m no_dupes_ii \u001b[38;5;129;01mand\u001b[39;00m no_leftover_smiles \u001b[38;5;129;01mand\u001b[39;00m correct_letter:\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m selfies_str,tokenised_selfies,items\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "selfies_str,tokenised_selfies,mapping = generate_mapping(smiles)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fairseq_git2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
