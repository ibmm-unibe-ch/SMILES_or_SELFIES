{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "889c6e79",
   "metadata": {},
   "source": [
    "# Scoring report\n",
    "## Gather scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f622a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jgut/miniconda3/envs/SMILES_OR_SELFIES/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Skipped loading some Tensorflow models, missing a dependency. No module named 'tensorflow'\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'torch_geometric'\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. cannot import name 'DMPNN' from 'deepchem.models.torch_models' (/home/jgut/miniconda3/envs/SMILES_OR_SELFIES/lib/python3.10/site-packages/deepchem/models/torch_models/__init__.py)\n",
      "Skipped loading modules with pytorch-lightning dependency, missing a dependency. No module named 'pytorch_lightning'\n",
      "Skipped loading some Jax models, missing a dependency. No module named 'jax'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data2/jgut/SoS_models/bace_classification/smiles_atom_isomers_bart\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_atom_isomers_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_atom_isomers_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_atom_isomers_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_atom_isomers_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_atom_isomers_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_atom_isomers_roberta\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_atom_isomers_roberta/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_atom_isomers_roberta/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_atom_isomers_roberta/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_atom_isomers_roberta/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_atom_isomers_roberta/5e-06_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_atom_isomers_bart\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_atom_isomers_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_atom_isomers_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_atom_isomers_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_atom_isomers_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_atom_isomers_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_atom_isomers_roberta\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_atom_isomers_roberta/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_atom_isomers_roberta/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_atom_isomers_roberta/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_atom_isomers_roberta/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_atom_isomers_roberta/5e-06_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_atom_isomers_bart\n",
      "/data2/jgut/SoS_models/bbbp/smiles_atom_isomers_bart/1e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_atom_isomers_bart/1e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_atom_isomers_bart/1e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_atom_isomers_bart/1e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_atom_isomers_bart/1e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_atom_isomers_roberta\n",
      "/data2/jgut/SoS_models/bbbp/smiles_atom_isomers_roberta/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_atom_isomers_roberta/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_atom_isomers_roberta/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_atom_isomers_roberta/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_atom_isomers_roberta/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_atom_isomers_bart\n",
      "/data2/jgut/SoS_models/clearance/smiles_atom_isomers_bart/1e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_atom_isomers_bart/1e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_atom_isomers_bart/1e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_atom_isomers_bart/1e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_atom_isomers_bart/1e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_atom_isomers_roberta\n",
      "/data2/jgut/SoS_models/clearance/smiles_atom_isomers_roberta/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_atom_isomers_roberta/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_atom_isomers_roberta/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_atom_isomers_roberta/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_atom_isomers_roberta/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_atom_isomers_bart\n",
      "/data2/jgut/SoS_models/clintox/smiles_atom_isomers_bart/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_atom_isomers_bart/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_atom_isomers_bart/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_atom_isomers_bart/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_atom_isomers_bart/5e-06_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_atom_isomers_roberta\n",
      "/data2/jgut/SoS_models/clintox/smiles_atom_isomers_roberta/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_atom_isomers_roberta/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_atom_isomers_roberta/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_atom_isomers_roberta/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_atom_isomers_roberta/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_atom_isomers_bart\n",
      "/data2/jgut/SoS_models/delaney/smiles_atom_isomers_bart/1e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_atom_isomers_bart/1e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_atom_isomers_bart/1e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_atom_isomers_bart/1e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_atom_isomers_bart/1e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_atom_isomers_roberta\n",
      "/data2/jgut/SoS_models/delaney/smiles_atom_isomers_roberta/1e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_atom_isomers_roberta/1e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_atom_isomers_roberta/1e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_atom_isomers_roberta/1e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_atom_isomers_roberta/1e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_atom_isomers_bart\n",
      "/data2/jgut/SoS_models/hiv/smiles_atom_isomers_bart/1e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_atom_isomers_bart/1e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_atom_isomers_bart/1e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_atom_isomers_bart/1e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_atom_isomers_bart/1e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_atom_isomers_roberta\n",
      "/data2/jgut/SoS_models/hiv/smiles_atom_isomers_roberta/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_atom_isomers_roberta/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_atom_isomers_roberta/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_atom_isomers_roberta/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_atom_isomers_roberta/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_atom_isomers_bart\n",
      "/data2/jgut/SoS_models/lipo/smiles_atom_isomers_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_atom_isomers_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_atom_isomers_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_atom_isomers_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_atom_isomers_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_atom_isomers_roberta\n",
      "/data2/jgut/SoS_models/lipo/smiles_atom_isomers_roberta/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_atom_isomers_roberta/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_atom_isomers_roberta/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_atom_isomers_roberta/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_atom_isomers_roberta/5e-06_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_atom_isomers_bart\n",
      "/data2/jgut/SoS_models/tox21/smiles_atom_isomers_bart/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_atom_isomers_bart/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_atom_isomers_bart/5e-06_0.2_seed_2/scores.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data2/jgut/SoS_models/tox21/smiles_atom_isomers_bart/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_atom_isomers_bart/5e-06_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_atom_isomers_roberta\n",
      "/data2/jgut/SoS_models/tox21/smiles_atom_isomers_roberta/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_atom_isomers_roberta/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_atom_isomers_roberta/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_atom_isomers_roberta/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_atom_isomers_roberta/5e-06_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_atom_standard_bart\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_atom_standard_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_atom_standard_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_atom_standard_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_atom_standard_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_atom_standard_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_atom_standard_roberta\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_atom_standard_roberta/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_atom_standard_roberta/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_atom_standard_roberta/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_atom_standard_roberta/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_atom_standard_roberta/5e-06_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_atom_standard_bart\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_atom_standard_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_atom_standard_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_atom_standard_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_atom_standard_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_atom_standard_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_atom_standard_roberta\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_atom_standard_roberta/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_atom_standard_roberta/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_atom_standard_roberta/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_atom_standard_roberta/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_atom_standard_roberta/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_atom_standard_bart\n",
      "/data2/jgut/SoS_models/bbbp/smiles_atom_standard_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_atom_standard_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_atom_standard_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_atom_standard_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_atom_standard_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_atom_standard_roberta\n",
      "/data2/jgut/SoS_models/bbbp/smiles_atom_standard_roberta/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_atom_standard_roberta/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_atom_standard_roberta/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_atom_standard_roberta/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_atom_standard_roberta/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_atom_standard_bart\n",
      "/data2/jgut/SoS_models/clearance/smiles_atom_standard_bart/1e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_atom_standard_bart/1e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_atom_standard_bart/1e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_atom_standard_bart/1e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_atom_standard_bart/1e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_atom_standard_roberta\n",
      "/data2/jgut/SoS_models/clearance/smiles_atom_standard_roberta/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_atom_standard_roberta/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_atom_standard_roberta/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_atom_standard_roberta/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_atom_standard_roberta/5e-06_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_atom_standard_bart\n",
      "/data2/jgut/SoS_models/clintox/smiles_atom_standard_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_atom_standard_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_atom_standard_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_atom_standard_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_atom_standard_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_atom_standard_roberta\n",
      "/data2/jgut/SoS_models/clintox/smiles_atom_standard_roberta/1e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_atom_standard_roberta/1e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_atom_standard_roberta/1e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_atom_standard_roberta/1e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_atom_standard_roberta/1e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_atom_standard_bart\n",
      "/data2/jgut/SoS_models/delaney/smiles_atom_standard_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_atom_standard_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_atom_standard_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_atom_standard_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_atom_standard_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_atom_standard_roberta\n",
      "/data2/jgut/SoS_models/delaney/smiles_atom_standard_roberta/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_atom_standard_roberta/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_atom_standard_roberta/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_atom_standard_roberta/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_atom_standard_roberta/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_atom_standard_bart\n",
      "/data2/jgut/SoS_models/hiv/smiles_atom_standard_bart/1e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_atom_standard_bart/1e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_atom_standard_bart/1e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_atom_standard_bart/1e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_atom_standard_bart/1e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_atom_standard_roberta\n",
      "/data2/jgut/SoS_models/hiv/smiles_atom_standard_roberta/1e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_atom_standard_roberta/1e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_atom_standard_roberta/1e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_atom_standard_roberta/1e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_atom_standard_roberta/1e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_atom_standard_bart\n",
      "/data2/jgut/SoS_models/lipo/smiles_atom_standard_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_atom_standard_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_atom_standard_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_atom_standard_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_atom_standard_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_atom_standard_roberta\n",
      "/data2/jgut/SoS_models/lipo/smiles_atom_standard_roberta/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_atom_standard_roberta/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_atom_standard_roberta/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_atom_standard_roberta/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_atom_standard_roberta/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_atom_standard_bart\n",
      "/data2/jgut/SoS_models/tox21/smiles_atom_standard_bart/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_atom_standard_bart/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_atom_standard_bart/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_atom_standard_bart/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_atom_standard_bart/5e-06_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_atom_standard_roberta\n",
      "/data2/jgut/SoS_models/tox21/smiles_atom_standard_roberta/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_atom_standard_roberta/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_atom_standard_roberta/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_atom_standard_roberta/5e-05_0.2_seed_3/scores.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data2/jgut/SoS_models/tox21/smiles_atom_standard_roberta/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_trained_isomers_bart\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_trained_isomers_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_trained_isomers_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_trained_isomers_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_trained_isomers_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_trained_isomers_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_trained_isomers_roberta\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_trained_isomers_roberta/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_trained_isomers_roberta/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_trained_isomers_roberta/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_trained_isomers_roberta/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_trained_isomers_roberta/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_trained_isomers_bart\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_trained_isomers_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_trained_isomers_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_trained_isomers_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_trained_isomers_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_trained_isomers_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_trained_isomers_roberta\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_trained_isomers_roberta/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_trained_isomers_roberta/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_trained_isomers_roberta/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_trained_isomers_roberta/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_trained_isomers_roberta/5e-06_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_trained_isomers_bart\n",
      "/data2/jgut/SoS_models/bbbp/smiles_trained_isomers_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_trained_isomers_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_trained_isomers_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_trained_isomers_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_trained_isomers_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_trained_isomers_roberta\n",
      "/data2/jgut/SoS_models/bbbp/smiles_trained_isomers_roberta/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_trained_isomers_roberta/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_trained_isomers_roberta/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_trained_isomers_roberta/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_trained_isomers_roberta/5e-06_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_trained_isomers_bart\n",
      "/data2/jgut/SoS_models/clearance/smiles_trained_isomers_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_trained_isomers_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_trained_isomers_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_trained_isomers_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_trained_isomers_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_trained_isomers_roberta\n",
      "/data2/jgut/SoS_models/clearance/smiles_trained_isomers_roberta/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_trained_isomers_roberta/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_trained_isomers_roberta/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_trained_isomers_roberta/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_trained_isomers_roberta/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_trained_isomers_bart\n",
      "/data2/jgut/SoS_models/clintox/smiles_trained_isomers_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_trained_isomers_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_trained_isomers_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_trained_isomers_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_trained_isomers_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_trained_isomers_roberta\n",
      "/data2/jgut/SoS_models/clintox/smiles_trained_isomers_roberta/1e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_trained_isomers_roberta/1e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_trained_isomers_roberta/1e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_trained_isomers_roberta/1e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_trained_isomers_roberta/1e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_trained_isomers_bart\n",
      "/data2/jgut/SoS_models/delaney/smiles_trained_isomers_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_trained_isomers_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_trained_isomers_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_trained_isomers_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_trained_isomers_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_trained_isomers_roberta\n",
      "/data2/jgut/SoS_models/delaney/smiles_trained_isomers_roberta/1e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_trained_isomers_roberta/1e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_trained_isomers_roberta/1e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_trained_isomers_roberta/1e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_trained_isomers_roberta/1e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_trained_isomers_bart\n",
      "/data2/jgut/SoS_models/hiv/smiles_trained_isomers_bart/1e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_trained_isomers_bart/1e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_trained_isomers_bart/1e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_trained_isomers_bart/1e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_trained_isomers_bart/1e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_trained_isomers_roberta\n",
      "/data2/jgut/SoS_models/hiv/smiles_trained_isomers_roberta/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_trained_isomers_roberta/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_trained_isomers_roberta/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_trained_isomers_roberta/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_trained_isomers_roberta/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_trained_isomers_bart\n",
      "/data2/jgut/SoS_models/lipo/smiles_trained_isomers_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_trained_isomers_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_trained_isomers_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_trained_isomers_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_trained_isomers_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_trained_isomers_roberta\n",
      "/data2/jgut/SoS_models/lipo/smiles_trained_isomers_roberta/1e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_trained_isomers_roberta/1e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_trained_isomers_roberta/1e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_trained_isomers_roberta/1e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_trained_isomers_roberta/1e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_trained_isomers_bart\n",
      "/data2/jgut/SoS_models/tox21/smiles_trained_isomers_bart/1e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_trained_isomers_bart/1e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_trained_isomers_bart/1e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_trained_isomers_bart/1e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_trained_isomers_bart/1e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_trained_isomers_roberta\n",
      "/data2/jgut/SoS_models/tox21/smiles_trained_isomers_roberta/1e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_trained_isomers_roberta/1e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_trained_isomers_roberta/1e-05_0.2_seed_2/scores.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data2/jgut/SoS_models/tox21/smiles_trained_isomers_roberta/1e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_trained_isomers_roberta/1e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_trained_standard_bart\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_trained_standard_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_trained_standard_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_trained_standard_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_trained_standard_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_trained_standard_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_trained_standard_roberta\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_trained_standard_roberta/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_trained_standard_roberta/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_trained_standard_roberta/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_trained_standard_roberta/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_trained_standard_roberta/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_trained_standard_bart\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_trained_standard_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_trained_standard_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_trained_standard_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_trained_standard_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_trained_standard_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_trained_standard_roberta\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_trained_standard_roberta/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_trained_standard_roberta/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_trained_standard_roberta/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_trained_standard_roberta/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_trained_standard_roberta/5e-06_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_trained_standard_bart\n",
      "/data2/jgut/SoS_models/bbbp/smiles_trained_standard_bart/1e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_trained_standard_bart/1e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_trained_standard_bart/1e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_trained_standard_bart/1e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_trained_standard_bart/1e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_trained_standard_roberta\n",
      "/data2/jgut/SoS_models/bbbp/smiles_trained_standard_roberta/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_trained_standard_roberta/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_trained_standard_roberta/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_trained_standard_roberta/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_trained_standard_roberta/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_trained_standard_bart\n",
      "/data2/jgut/SoS_models/clearance/smiles_trained_standard_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_trained_standard_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_trained_standard_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_trained_standard_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_trained_standard_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_trained_standard_roberta\n",
      "/data2/jgut/SoS_models/clearance/smiles_trained_standard_roberta/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_trained_standard_roberta/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_trained_standard_roberta/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_trained_standard_roberta/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_trained_standard_roberta/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_trained_standard_bart\n",
      "/data2/jgut/SoS_models/clintox/smiles_trained_standard_bart/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_trained_standard_bart/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_trained_standard_bart/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_trained_standard_bart/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_trained_standard_bart/5e-06_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_trained_standard_roberta\n",
      "/data2/jgut/SoS_models/clintox/smiles_trained_standard_roberta/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_trained_standard_roberta/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_trained_standard_roberta/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_trained_standard_roberta/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_trained_standard_roberta/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_trained_standard_bart\n",
      "/data2/jgut/SoS_models/delaney/smiles_trained_standard_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_trained_standard_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_trained_standard_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_trained_standard_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_trained_standard_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_trained_standard_roberta\n",
      "/data2/jgut/SoS_models/delaney/smiles_trained_standard_roberta/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_trained_standard_roberta/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_trained_standard_roberta/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_trained_standard_roberta/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_trained_standard_roberta/5e-06_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_trained_standard_bart\n",
      "/data2/jgut/SoS_models/hiv/smiles_trained_standard_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_trained_standard_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_trained_standard_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_trained_standard_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_trained_standard_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_trained_standard_roberta\n",
      "/data2/jgut/SoS_models/hiv/smiles_trained_standard_roberta/1e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_trained_standard_roberta/1e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_trained_standard_roberta/1e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_trained_standard_roberta/1e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_trained_standard_roberta/1e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_trained_standard_bart\n",
      "/data2/jgut/SoS_models/lipo/smiles_trained_standard_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_trained_standard_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_trained_standard_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_trained_standard_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_trained_standard_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_trained_standard_roberta\n",
      "/data2/jgut/SoS_models/lipo/smiles_trained_standard_roberta/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_trained_standard_roberta/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_trained_standard_roberta/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_trained_standard_roberta/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_trained_standard_roberta/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_trained_standard_bart\n",
      "/data2/jgut/SoS_models/tox21/smiles_trained_standard_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_trained_standard_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_trained_standard_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_trained_standard_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_trained_standard_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_trained_standard_roberta\n",
      "/data2/jgut/SoS_models/tox21/smiles_trained_standard_roberta/1e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_trained_standard_roberta/1e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_trained_standard_roberta/1e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_trained_standard_roberta/1e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_trained_standard_roberta/1e-05_0.2_seed_4/scores.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data2/jgut/SoS_models/bace_classification/selfies_atom_isomers_bart\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_atom_isomers_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_atom_isomers_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_atom_isomers_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_atom_isomers_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_atom_isomers_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_atom_isomers_roberta\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_atom_isomers_roberta/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_atom_isomers_roberta/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_atom_isomers_roberta/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_atom_isomers_roberta/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_atom_isomers_roberta/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_atom_isomers_bart\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_atom_isomers_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_atom_isomers_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_atom_isomers_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_atom_isomers_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_atom_isomers_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_atom_isomers_roberta\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_atom_isomers_roberta/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_atom_isomers_roberta/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_atom_isomers_roberta/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_atom_isomers_roberta/5e-06_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_atom_isomers_roberta/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_atom_isomers_bart\n",
      "/data2/jgut/SoS_models/bbbp/selfies_atom_isomers_bart/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_atom_isomers_bart/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_atom_isomers_bart/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_atom_isomers_bart/5e-06_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_atom_isomers_bart/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_atom_isomers_roberta\n",
      "/data2/jgut/SoS_models/bbbp/selfies_atom_isomers_roberta/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_atom_isomers_roberta/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_atom_isomers_roberta/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_atom_isomers_roberta/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_atom_isomers_roberta/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_atom_isomers_bart\n",
      "/data2/jgut/SoS_models/clearance/selfies_atom_isomers_bart/1e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_atom_isomers_bart/1e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_atom_isomers_bart/1e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_atom_isomers_bart/1e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_atom_isomers_bart/1e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_atom_isomers_roberta\n",
      "/data2/jgut/SoS_models/clearance/selfies_atom_isomers_roberta/1e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_atom_isomers_roberta/1e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_atom_isomers_roberta/1e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_atom_isomers_roberta/1e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_atom_isomers_roberta/1e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_atom_isomers_bart\n",
      "/data2/jgut/SoS_models/clintox/selfies_atom_isomers_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_atom_isomers_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_atom_isomers_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_atom_isomers_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_atom_isomers_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_atom_isomers_roberta\n",
      "/data2/jgut/SoS_models/clintox/selfies_atom_isomers_roberta/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_atom_isomers_roberta/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_atom_isomers_roberta/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_atom_isomers_roberta/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_atom_isomers_roberta/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_atom_isomers_bart\n",
      "/data2/jgut/SoS_models/delaney/selfies_atom_isomers_bart/1e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_atom_isomers_bart/1e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_atom_isomers_bart/1e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_atom_isomers_bart/1e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_atom_isomers_bart/1e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_atom_isomers_roberta\n",
      "/data2/jgut/SoS_models/delaney/selfies_atom_isomers_roberta/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_atom_isomers_roberta/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_atom_isomers_roberta/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_atom_isomers_roberta/5e-06_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_atom_isomers_roberta/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_atom_isomers_bart\n",
      "/data2/jgut/SoS_models/hiv/selfies_atom_isomers_bart/1e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_atom_isomers_bart/1e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_atom_isomers_bart/1e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_atom_isomers_bart/1e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_atom_isomers_bart/1e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_atom_isomers_roberta\n",
      "/data2/jgut/SoS_models/hiv/selfies_atom_isomers_roberta/1e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_atom_isomers_roberta/1e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_atom_isomers_roberta/1e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_atom_isomers_roberta/1e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_atom_isomers_roberta/1e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_atom_isomers_bart\n",
      "/data2/jgut/SoS_models/lipo/selfies_atom_isomers_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_atom_isomers_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_atom_isomers_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_atom_isomers_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_atom_isomers_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_atom_isomers_roberta\n",
      "/data2/jgut/SoS_models/lipo/selfies_atom_isomers_roberta/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_atom_isomers_roberta/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_atom_isomers_roberta/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_atom_isomers_roberta/5e-06_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_atom_isomers_roberta/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_atom_isomers_bart\n",
      "/data2/jgut/SoS_models/tox21/selfies_atom_isomers_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_atom_isomers_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_atom_isomers_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_atom_isomers_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_atom_isomers_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_atom_isomers_roberta\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data2/jgut/SoS_models/tox21/selfies_atom_isomers_roberta/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_atom_isomers_roberta/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_atom_isomers_roberta/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_atom_isomers_roberta/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_atom_isomers_roberta/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_atom_standard_bart\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_atom_standard_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_atom_standard_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_atom_standard_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_atom_standard_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_atom_standard_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_atom_standard_roberta\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_atom_standard_roberta/1e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_atom_standard_roberta/1e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_atom_standard_roberta/1e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_atom_standard_roberta/1e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_atom_standard_roberta/1e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_atom_standard_bart\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_atom_standard_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_atom_standard_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_atom_standard_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_atom_standard_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_atom_standard_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_atom_standard_roberta\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_atom_standard_roberta/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_atom_standard_roberta/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_atom_standard_roberta/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_atom_standard_roberta/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_atom_standard_roberta/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_atom_standard_bart\n",
      "/data2/jgut/SoS_models/bbbp/selfies_atom_standard_bart/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_atom_standard_bart/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_atom_standard_bart/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_atom_standard_bart/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_atom_standard_bart/5e-06_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_atom_standard_roberta\n",
      "/data2/jgut/SoS_models/bbbp/selfies_atom_standard_roberta/1e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_atom_standard_roberta/1e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_atom_standard_roberta/1e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_atom_standard_roberta/1e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_atom_standard_roberta/1e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_atom_standard_bart\n",
      "/data2/jgut/SoS_models/clearance/selfies_atom_standard_bart/1e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_atom_standard_bart/1e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_atom_standard_bart/1e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_atom_standard_bart/1e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_atom_standard_bart/1e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_atom_standard_roberta\n",
      "/data2/jgut/SoS_models/clearance/selfies_atom_standard_roberta/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_atom_standard_roberta/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_atom_standard_roberta/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_atom_standard_roberta/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_atom_standard_roberta/5e-06_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_atom_standard_bart\n",
      "/data2/jgut/SoS_models/clintox/selfies_atom_standard_bart/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_atom_standard_bart/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_atom_standard_bart/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_atom_standard_bart/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_atom_standard_bart/5e-06_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_atom_standard_roberta\n",
      "/data2/jgut/SoS_models/clintox/selfies_atom_standard_roberta/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_atom_standard_roberta/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_atom_standard_roberta/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_atom_standard_roberta/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_atom_standard_roberta/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_atom_standard_bart\n",
      "/data2/jgut/SoS_models/delaney/selfies_atom_standard_bart/1e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_atom_standard_bart/1e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_atom_standard_bart/1e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_atom_standard_bart/1e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_atom_standard_bart/1e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_atom_standard_roberta\n",
      "/data2/jgut/SoS_models/delaney/selfies_atom_standard_roberta/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_atom_standard_roberta/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_atom_standard_roberta/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_atom_standard_roberta/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_atom_standard_roberta/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_atom_standard_bart\n",
      "/data2/jgut/SoS_models/hiv/selfies_atom_standard_bart/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_atom_standard_bart/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_atom_standard_bart/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_atom_standard_bart/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_atom_standard_bart/5e-06_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_atom_standard_roberta\n",
      "/data2/jgut/SoS_models/hiv/selfies_atom_standard_roberta/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_atom_standard_roberta/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_atom_standard_roberta/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_atom_standard_roberta/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_atom_standard_roberta/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_atom_standard_bart\n",
      "/data2/jgut/SoS_models/lipo/selfies_atom_standard_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_atom_standard_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_atom_standard_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_atom_standard_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_atom_standard_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_atom_standard_roberta\n",
      "/data2/jgut/SoS_models/lipo/selfies_atom_standard_roberta/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_atom_standard_roberta/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_atom_standard_roberta/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_atom_standard_roberta/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_atom_standard_roberta/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_atom_standard_bart\n",
      "/data2/jgut/SoS_models/tox21/selfies_atom_standard_bart/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_atom_standard_bart/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_atom_standard_bart/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_atom_standard_bart/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_atom_standard_bart/5e-06_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_atom_standard_roberta\n",
      "/data2/jgut/SoS_models/tox21/selfies_atom_standard_roberta/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_atom_standard_roberta/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_atom_standard_roberta/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_atom_standard_roberta/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_atom_standard_roberta/5e-06_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_trained_isomers_bart\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_trained_isomers_bart/1e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_trained_isomers_bart/1e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_trained_isomers_bart/1e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_trained_isomers_bart/1e-05_0.2_seed_3/scores.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data2/jgut/SoS_models/bace_classification/selfies_trained_isomers_bart/1e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_trained_isomers_roberta\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_trained_isomers_roberta/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_trained_isomers_roberta/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_trained_isomers_roberta/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_trained_isomers_roberta/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_trained_isomers_roberta/5e-06_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_trained_isomers_bart\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_trained_isomers_bart/1e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_trained_isomers_bart/1e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_trained_isomers_bart/1e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_trained_isomers_bart/1e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_trained_isomers_bart/1e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_trained_isomers_roberta\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_trained_isomers_roberta/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_trained_isomers_roberta/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_trained_isomers_roberta/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_trained_isomers_roberta/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_trained_isomers_roberta/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_trained_isomers_bart\n",
      "/data2/jgut/SoS_models/bbbp/selfies_trained_isomers_bart/1e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_trained_isomers_bart/1e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_trained_isomers_bart/1e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_trained_isomers_bart/1e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_trained_isomers_bart/1e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_trained_isomers_roberta\n",
      "/data2/jgut/SoS_models/bbbp/selfies_trained_isomers_roberta/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_trained_isomers_roberta/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_trained_isomers_roberta/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_trained_isomers_roberta/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_trained_isomers_roberta/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_trained_isomers_bart\n",
      "/data2/jgut/SoS_models/clearance/selfies_trained_isomers_bart/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_trained_isomers_bart/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_trained_isomers_bart/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_trained_isomers_bart/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_trained_isomers_bart/5e-06_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_trained_isomers_roberta\n",
      "/data2/jgut/SoS_models/clearance/selfies_trained_isomers_roberta/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_trained_isomers_roberta/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_trained_isomers_roberta/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_trained_isomers_roberta/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_trained_isomers_roberta/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_trained_isomers_bart\n",
      "/data2/jgut/SoS_models/clintox/selfies_trained_isomers_bart/1e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_trained_isomers_bart/1e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_trained_isomers_bart/1e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_trained_isomers_bart/1e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_trained_isomers_bart/1e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_trained_isomers_roberta\n",
      "/data2/jgut/SoS_models/clintox/selfies_trained_isomers_roberta/1e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_trained_isomers_roberta/1e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_trained_isomers_roberta/1e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_trained_isomers_roberta/1e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_trained_isomers_roberta/1e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_trained_isomers_bart\n",
      "/data2/jgut/SoS_models/delaney/selfies_trained_isomers_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_trained_isomers_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_trained_isomers_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_trained_isomers_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_trained_isomers_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_trained_isomers_roberta\n",
      "/data2/jgut/SoS_models/delaney/selfies_trained_isomers_roberta/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_trained_isomers_roberta/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_trained_isomers_roberta/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_trained_isomers_roberta/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_trained_isomers_roberta/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_trained_isomers_bart\n",
      "/data2/jgut/SoS_models/hiv/selfies_trained_isomers_bart/1e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_trained_isomers_bart/1e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_trained_isomers_bart/1e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_trained_isomers_bart/1e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_trained_isomers_bart/1e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_trained_isomers_roberta\n",
      "/data2/jgut/SoS_models/hiv/selfies_trained_isomers_roberta/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_trained_isomers_roberta/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_trained_isomers_roberta/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_trained_isomers_roberta/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_trained_isomers_roberta/5e-06_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_trained_isomers_bart\n",
      "/data2/jgut/SoS_models/lipo/selfies_trained_isomers_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_trained_isomers_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_trained_isomers_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_trained_isomers_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_trained_isomers_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_trained_isomers_roberta\n",
      "/data2/jgut/SoS_models/lipo/selfies_trained_isomers_roberta/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_trained_isomers_roberta/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_trained_isomers_roberta/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_trained_isomers_roberta/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_trained_isomers_roberta/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_trained_isomers_bart\n",
      "/data2/jgut/SoS_models/tox21/selfies_trained_isomers_bart/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_trained_isomers_bart/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_trained_isomers_bart/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_trained_isomers_bart/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_trained_isomers_bart/5e-06_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_trained_isomers_roberta\n",
      "/data2/jgut/SoS_models/tox21/selfies_trained_isomers_roberta/5e-06_0.2_seed_0/scores.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data2/jgut/SoS_models/tox21/selfies_trained_isomers_roberta/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_trained_isomers_roberta/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_trained_isomers_roberta/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_trained_isomers_roberta/5e-06_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_trained_standard_bart\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_trained_standard_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_trained_standard_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_trained_standard_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_trained_standard_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_trained_standard_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_trained_standard_roberta\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_trained_standard_roberta/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_trained_standard_roberta/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_trained_standard_roberta/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_trained_standard_roberta/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_trained_standard_roberta/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_trained_standard_bart\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_trained_standard_bart/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_trained_standard_bart/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_trained_standard_bart/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_trained_standard_bart/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_trained_standard_bart/5e-06_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_trained_standard_roberta\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_trained_standard_roberta/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_trained_standard_roberta/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_trained_standard_roberta/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_trained_standard_roberta/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_trained_standard_roberta/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_trained_standard_bart\n",
      "/data2/jgut/SoS_models/bbbp/selfies_trained_standard_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_trained_standard_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_trained_standard_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_trained_standard_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_trained_standard_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_trained_standard_roberta\n",
      "/data2/jgut/SoS_models/bbbp/selfies_trained_standard_roberta/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_trained_standard_roberta/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_trained_standard_roberta/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_trained_standard_roberta/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_trained_standard_roberta/5e-06_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_trained_standard_bart\n",
      "/data2/jgut/SoS_models/clearance/selfies_trained_standard_bart/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_trained_standard_bart/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_trained_standard_bart/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_trained_standard_bart/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_trained_standard_bart/5e-06_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_trained_standard_roberta\n",
      "/data2/jgut/SoS_models/clearance/selfies_trained_standard_roberta/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_trained_standard_roberta/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_trained_standard_roberta/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_trained_standard_roberta/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_trained_standard_roberta/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_trained_standard_bart\n",
      "/data2/jgut/SoS_models/clintox/selfies_trained_standard_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_trained_standard_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_trained_standard_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_trained_standard_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_trained_standard_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_trained_standard_roberta\n",
      "/data2/jgut/SoS_models/clintox/selfies_trained_standard_roberta/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_trained_standard_roberta/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_trained_standard_roberta/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_trained_standard_roberta/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_trained_standard_roberta/5e-06_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_trained_standard_bart\n",
      "/data2/jgut/SoS_models/delaney/selfies_trained_standard_bart/1e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_trained_standard_bart/1e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_trained_standard_bart/1e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_trained_standard_bart/1e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_trained_standard_bart/1e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_trained_standard_roberta\n",
      "/data2/jgut/SoS_models/delaney/selfies_trained_standard_roberta/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_trained_standard_roberta/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_trained_standard_roberta/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_trained_standard_roberta/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_trained_standard_roberta/5e-06_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_trained_standard_bart\n",
      "/data2/jgut/SoS_models/hiv/selfies_trained_standard_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_trained_standard_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_trained_standard_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_trained_standard_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_trained_standard_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_trained_standard_roberta\n",
      "/data2/jgut/SoS_models/hiv/selfies_trained_standard_roberta/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_trained_standard_roberta/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_trained_standard_roberta/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_trained_standard_roberta/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_trained_standard_roberta/5e-06_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_trained_standard_bart\n",
      "/data2/jgut/SoS_models/lipo/selfies_trained_standard_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_trained_standard_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_trained_standard_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_trained_standard_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_trained_standard_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_trained_standard_roberta\n",
      "/data2/jgut/SoS_models/lipo/selfies_trained_standard_roberta/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_trained_standard_roberta/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_trained_standard_roberta/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_trained_standard_roberta/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_trained_standard_roberta/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_trained_standard_bart\n",
      "/data2/jgut/SoS_models/tox21/selfies_trained_standard_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_trained_standard_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_trained_standard_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_trained_standard_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_trained_standard_bart/5e-05_0.2_seed_4/scores.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data2/jgut/SoS_models/tox21/selfies_trained_standard_roberta\n",
      "/data2/jgut/SoS_models/tox21/selfies_trained_standard_roberta/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_trained_standard_roberta/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_trained_standard_roberta/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_trained_standard_roberta/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_trained_standard_roberta/5e-06_0.2_seed_4/scores.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from constants import TASK_MODEL_PATH, TOKENIZER_SUFFIXES, MOLNET_DIRECTORY, PROJECT_PATH, DESCRIPTORS\n",
    "import scipy\n",
    "from copy import copy\n",
    "\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "def compute_zscore(column, value=None):\n",
    "    if value:\n",
    "        return (value-column.mean())/column.std()\n",
    "    return (column-column.mean())/column.std()\n",
    "\n",
    "classification_scores = []\n",
    "regression_scores = []\n",
    "for tokenizer_suffix in TOKENIZER_SUFFIXES:\n",
    "    for task in MOLNET_DIRECTORY.keys():\n",
    "        for model in [\"bart\", \"roberta\"]:\n",
    "            task_tokenizer_path = TASK_MODEL_PATH/task/(tokenizer_suffix+\"_\"+model)\n",
    "            print(task_tokenizer_path)\n",
    "            if task_tokenizer_path.exists():\n",
    "                for hyperparameter_path in glob(str(task_tokenizer_path) + \"/*\", recursive=True):\n",
    "                    scores_path = hyperparameter_path+\"/scores.csv\"\n",
    "                    if not Path(scores_path).is_file() or not \"seed\" in str(scores_path):\n",
    "                        continue\n",
    "                    print(scores_path)\n",
    "                    new_score_df = pd.read_csv(scores_path)\n",
    "                    if list(new_score_df.task_type)[0] == \"classification\":\n",
    "                        classification_scores.append(new_score_df)\n",
    "                    else:\n",
    "                        regression_scores.append(new_score_df)\n",
    "regression_scores = pd.concat(regression_scores, axis = 0, sort = False)\n",
    "classification_scores = pd.concat(classification_scores, axis = 0, sort = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cce8ea1",
   "metadata": {},
   "source": [
    "## Most drilled down regression scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "084e6084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">rectified_mean_squared_error</th>\n",
       "      <th>z_rectified_mean_squared_error_bace_regression</th>\n",
       "      <th>z_rectified_mean_squared_error_clearance</th>\n",
       "      <th>z_rectified_mean_squared_error_delaney</th>\n",
       "      <th>z_rectified_mean_squared_error_lipo</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>task</th>\n",
       "      <th>bace_regression</th>\n",
       "      <th>clearance</th>\n",
       "      <th>delaney</th>\n",
       "      <th>lipo</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embedding</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>dataset</th>\n",
       "      <th>architecture</th>\n",
       "      <th>seed</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"40\" valign=\"top\">selfies</th>\n",
       "      <th rowspan=\"20\" valign=\"top\">atom</th>\n",
       "      <th rowspan=\"10\" valign=\"top\">isomers</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">bart</th>\n",
       "      <th>0</th>\n",
       "      <td>0.867640</td>\n",
       "      <td>1.131814</td>\n",
       "      <td>0.588621</td>\n",
       "      <td>0.732742</td>\n",
       "      <td>-0.627229</td>\n",
       "      <td>-1.076476</td>\n",
       "      <td>1.500931</td>\n",
       "      <td>1.129169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.766853</td>\n",
       "      <td>1.122419</td>\n",
       "      <td>0.589391</td>\n",
       "      <td>0.701290</td>\n",
       "      <td>-1.215783</td>\n",
       "      <td>-1.244779</td>\n",
       "      <td>1.516322</td>\n",
       "      <td>0.116635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.840789</td>\n",
       "      <td>1.168559</td>\n",
       "      <td>0.558832</td>\n",
       "      <td>0.695183</td>\n",
       "      <td>-0.784028</td>\n",
       "      <td>-0.418236</td>\n",
       "      <td>0.905429</td>\n",
       "      <td>-0.079967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.787273</td>\n",
       "      <td>1.169699</td>\n",
       "      <td>0.561634</td>\n",
       "      <td>0.681776</td>\n",
       "      <td>-1.096542</td>\n",
       "      <td>-0.397809</td>\n",
       "      <td>0.961442</td>\n",
       "      <td>-0.511560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.125730</td>\n",
       "      <td>1.203900</td>\n",
       "      <td>0.593769</td>\n",
       "      <td>0.695180</td>\n",
       "      <td>0.879919</td>\n",
       "      <td>0.214865</td>\n",
       "      <td>1.603845</td>\n",
       "      <td>-0.080073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">roberta</th>\n",
       "      <th>0</th>\n",
       "      <td>1.273632</td>\n",
       "      <td>1.140953</td>\n",
       "      <td>0.459100</td>\n",
       "      <td>0.760557</td>\n",
       "      <td>1.743605</td>\n",
       "      <td>-0.912763</td>\n",
       "      <td>-1.088321</td>\n",
       "      <td>2.024608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.909233</td>\n",
       "      <td>1.169139</td>\n",
       "      <td>0.458983</td>\n",
       "      <td>0.697795</td>\n",
       "      <td>-0.384343</td>\n",
       "      <td>-0.407843</td>\n",
       "      <td>-1.090650</td>\n",
       "      <td>0.004124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.161970</td>\n",
       "      <td>1.211294</td>\n",
       "      <td>0.526965</td>\n",
       "      <td>0.729796</td>\n",
       "      <td>1.091542</td>\n",
       "      <td>0.347325</td>\n",
       "      <td>0.268367</td>\n",
       "      <td>1.034300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.211683</td>\n",
       "      <td>1.291416</td>\n",
       "      <td>0.492840</td>\n",
       "      <td>0.734060</td>\n",
       "      <td>1.381849</td>\n",
       "      <td>1.782627</td>\n",
       "      <td>-0.413808</td>\n",
       "      <td>1.171599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.237530</td>\n",
       "      <td>1.392441</td>\n",
       "      <td>0.460964</td>\n",
       "      <td>0.734539</td>\n",
       "      <td>1.532786</td>\n",
       "      <td>3.592365</td>\n",
       "      <td>-1.051050</td>\n",
       "      <td>1.186997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">standard</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">bart</th>\n",
       "      <th>0</th>\n",
       "      <td>1.168866</td>\n",
       "      <td>1.205083</td>\n",
       "      <td>0.658529</td>\n",
       "      <td>0.695181</td>\n",
       "      <td>1.131813</td>\n",
       "      <td>0.236067</td>\n",
       "      <td>2.898472</td>\n",
       "      <td>-0.080038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.779775</td>\n",
       "      <td>1.183429</td>\n",
       "      <td>0.602495</td>\n",
       "      <td>0.689594</td>\n",
       "      <td>-1.140327</td>\n",
       "      <td>-0.151853</td>\n",
       "      <td>1.778292</td>\n",
       "      <td>-0.259894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.762166</td>\n",
       "      <td>1.144793</td>\n",
       "      <td>0.627679</td>\n",
       "      <td>0.656210</td>\n",
       "      <td>-1.243156</td>\n",
       "      <td>-0.843977</td>\n",
       "      <td>2.281739</td>\n",
       "      <td>-1.334623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.821160</td>\n",
       "      <td>1.184509</td>\n",
       "      <td>0.569119</td>\n",
       "      <td>0.669667</td>\n",
       "      <td>-0.898651</td>\n",
       "      <td>-0.132509</td>\n",
       "      <td>1.111066</td>\n",
       "      <td>-0.901392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.732206</td>\n",
       "      <td>1.194193</td>\n",
       "      <td>0.580774</td>\n",
       "      <td>0.680026</td>\n",
       "      <td>-1.418107</td>\n",
       "      <td>0.040981</td>\n",
       "      <td>1.344058</td>\n",
       "      <td>-0.567904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">roberta</th>\n",
       "      <th>0</th>\n",
       "      <td>1.242868</td>\n",
       "      <td>1.206969</td>\n",
       "      <td>0.474809</td>\n",
       "      <td>0.669837</td>\n",
       "      <td>1.563954</td>\n",
       "      <td>0.269839</td>\n",
       "      <td>-0.774265</td>\n",
       "      <td>-0.895939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.225243</td>\n",
       "      <td>1.212885</td>\n",
       "      <td>0.447627</td>\n",
       "      <td>0.718285</td>\n",
       "      <td>1.461031</td>\n",
       "      <td>0.375823</td>\n",
       "      <td>-1.317673</td>\n",
       "      <td>0.663752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.776240</td>\n",
       "      <td>1.188664</td>\n",
       "      <td>0.459117</td>\n",
       "      <td>0.685333</td>\n",
       "      <td>-1.160966</td>\n",
       "      <td>-0.058075</td>\n",
       "      <td>-1.087981</td>\n",
       "      <td>-0.397081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.835992</td>\n",
       "      <td>1.172775</td>\n",
       "      <td>0.430063</td>\n",
       "      <td>0.690933</td>\n",
       "      <td>-0.812039</td>\n",
       "      <td>-0.342711</td>\n",
       "      <td>-1.668782</td>\n",
       "      <td>-0.216780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.225987</td>\n",
       "      <td>1.217195</td>\n",
       "      <td>0.459247</td>\n",
       "      <td>0.700320</td>\n",
       "      <td>1.465376</td>\n",
       "      <td>0.453038</td>\n",
       "      <td>-1.085381</td>\n",
       "      <td>0.085390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"20\" valign=\"top\">trained</th>\n",
       "      <th rowspan=\"10\" valign=\"top\">isomers</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">bart</th>\n",
       "      <th>0</th>\n",
       "      <td>1.094915</td>\n",
       "      <td>1.138284</td>\n",
       "      <td>0.500634</td>\n",
       "      <td>0.680783</td>\n",
       "      <td>0.699968</td>\n",
       "      <td>-0.960572</td>\n",
       "      <td>-0.258013</td>\n",
       "      <td>-0.543549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.107150</td>\n",
       "      <td>1.208899</td>\n",
       "      <td>0.480643</td>\n",
       "      <td>0.674653</td>\n",
       "      <td>0.771419</td>\n",
       "      <td>0.304410</td>\n",
       "      <td>-0.657647</td>\n",
       "      <td>-0.740891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.325104</td>\n",
       "      <td>1.167810</td>\n",
       "      <td>0.486166</td>\n",
       "      <td>0.652172</td>\n",
       "      <td>2.044182</td>\n",
       "      <td>-0.431648</td>\n",
       "      <td>-0.547230</td>\n",
       "      <td>-1.464621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.084281</td>\n",
       "      <td>1.182405</td>\n",
       "      <td>0.525232</td>\n",
       "      <td>0.665512</td>\n",
       "      <td>0.637868</td>\n",
       "      <td>-0.170185</td>\n",
       "      <td>0.233736</td>\n",
       "      <td>-1.035155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.193118</td>\n",
       "      <td>1.177850</td>\n",
       "      <td>0.542705</td>\n",
       "      <td>0.679922</td>\n",
       "      <td>1.273435</td>\n",
       "      <td>-0.251796</td>\n",
       "      <td>0.583032</td>\n",
       "      <td>-0.571251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">roberta</th>\n",
       "      <th>0</th>\n",
       "      <td>0.792596</td>\n",
       "      <td>1.184835</td>\n",
       "      <td>0.520051</td>\n",
       "      <td>0.748091</td>\n",
       "      <td>-1.065458</td>\n",
       "      <td>-0.126656</td>\n",
       "      <td>0.130152</td>\n",
       "      <td>1.623277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.831032</td>\n",
       "      <td>1.170689</td>\n",
       "      <td>0.514270</td>\n",
       "      <td>0.731457</td>\n",
       "      <td>-0.841001</td>\n",
       "      <td>-0.380070</td>\n",
       "      <td>0.014595</td>\n",
       "      <td>1.087797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.904313</td>\n",
       "      <td>1.311176</td>\n",
       "      <td>0.530704</td>\n",
       "      <td>0.737419</td>\n",
       "      <td>-0.413072</td>\n",
       "      <td>2.136593</td>\n",
       "      <td>0.343119</td>\n",
       "      <td>1.279704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.203126</td>\n",
       "      <td>1.178287</td>\n",
       "      <td>0.506735</td>\n",
       "      <td>0.735168</td>\n",
       "      <td>1.331879</td>\n",
       "      <td>-0.243964</td>\n",
       "      <td>-0.136042</td>\n",
       "      <td>1.207261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.778719</td>\n",
       "      <td>1.187267</td>\n",
       "      <td>0.491517</td>\n",
       "      <td>0.720044</td>\n",
       "      <td>-1.146490</td>\n",
       "      <td>-0.083101</td>\n",
       "      <td>-0.440255</td>\n",
       "      <td>0.720374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">standard</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">bart</th>\n",
       "      <th>0</th>\n",
       "      <td>1.146895</td>\n",
       "      <td>1.231815</td>\n",
       "      <td>0.527737</td>\n",
       "      <td>0.703292</td>\n",
       "      <td>1.003514</td>\n",
       "      <td>0.714926</td>\n",
       "      <td>0.283813</td>\n",
       "      <td>0.181074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.870793</td>\n",
       "      <td>1.251535</td>\n",
       "      <td>0.525161</td>\n",
       "      <td>0.697981</td>\n",
       "      <td>-0.608814</td>\n",
       "      <td>1.068203</td>\n",
       "      <td>0.232317</td>\n",
       "      <td>0.010102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.930218</td>\n",
       "      <td>1.199987</td>\n",
       "      <td>0.506908</td>\n",
       "      <td>0.701003</td>\n",
       "      <td>-0.261794</td>\n",
       "      <td>0.144765</td>\n",
       "      <td>-0.132593</td>\n",
       "      <td>0.107408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.967492</td>\n",
       "      <td>1.227695</td>\n",
       "      <td>0.564684</td>\n",
       "      <td>0.717069</td>\n",
       "      <td>-0.044128</td>\n",
       "      <td>0.641127</td>\n",
       "      <td>1.022418</td>\n",
       "      <td>0.624606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.281114</td>\n",
       "      <td>1.305747</td>\n",
       "      <td>0.519464</td>\n",
       "      <td>0.705946</td>\n",
       "      <td>1.787297</td>\n",
       "      <td>2.039338</td>\n",
       "      <td>0.118423</td>\n",
       "      <td>0.266525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">roberta</th>\n",
       "      <th>0</th>\n",
       "      <td>0.926161</td>\n",
       "      <td>1.269090</td>\n",
       "      <td>0.496169</td>\n",
       "      <td>0.728459</td>\n",
       "      <td>-0.285486</td>\n",
       "      <td>1.382679</td>\n",
       "      <td>-0.347272</td>\n",
       "      <td>0.991268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.824731</td>\n",
       "      <td>1.213574</td>\n",
       "      <td>0.496941</td>\n",
       "      <td>0.720099</td>\n",
       "      <td>-0.877798</td>\n",
       "      <td>0.388167</td>\n",
       "      <td>-0.331837</td>\n",
       "      <td>0.722128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.849218</td>\n",
       "      <td>1.214858</td>\n",
       "      <td>0.494477</td>\n",
       "      <td>0.819233</td>\n",
       "      <td>-0.734807</td>\n",
       "      <td>0.411173</td>\n",
       "      <td>-0.381089</td>\n",
       "      <td>3.913531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.945388</td>\n",
       "      <td>1.322168</td>\n",
       "      <td>0.523504</td>\n",
       "      <td>0.755256</td>\n",
       "      <td>-0.173211</td>\n",
       "      <td>2.333513</td>\n",
       "      <td>0.199186</td>\n",
       "      <td>1.853942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.912304</td>\n",
       "      <td>1.189672</td>\n",
       "      <td>0.552072</td>\n",
       "      <td>0.734376</td>\n",
       "      <td>-0.366405</td>\n",
       "      <td>-0.040013</td>\n",
       "      <td>0.770285</td>\n",
       "      <td>1.181759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"40\" valign=\"top\">smiles</th>\n",
       "      <th rowspan=\"20\" valign=\"top\">atom</th>\n",
       "      <th rowspan=\"10\" valign=\"top\">isomers</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">bart</th>\n",
       "      <th>0</th>\n",
       "      <td>0.801891</td>\n",
       "      <td>1.166224</td>\n",
       "      <td>0.541236</td>\n",
       "      <td>0.661239</td>\n",
       "      <td>-1.011177</td>\n",
       "      <td>-0.460053</td>\n",
       "      <td>0.553672</td>\n",
       "      <td>-1.172722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.847840</td>\n",
       "      <td>1.165750</td>\n",
       "      <td>0.475719</td>\n",
       "      <td>0.689789</td>\n",
       "      <td>-0.742853</td>\n",
       "      <td>-0.468554</td>\n",
       "      <td>-0.756090</td>\n",
       "      <td>-0.253624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.794942</td>\n",
       "      <td>1.157096</td>\n",
       "      <td>0.512823</td>\n",
       "      <td>0.677712</td>\n",
       "      <td>-1.051758</td>\n",
       "      <td>-0.623568</td>\n",
       "      <td>-0.014343</td>\n",
       "      <td>-0.642401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.798707</td>\n",
       "      <td>1.128919</td>\n",
       "      <td>0.601219</td>\n",
       "      <td>0.665154</td>\n",
       "      <td>-1.029772</td>\n",
       "      <td>-1.128342</td>\n",
       "      <td>1.752793</td>\n",
       "      <td>-1.046691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.964426</td>\n",
       "      <td>1.106637</td>\n",
       "      <td>0.471436</td>\n",
       "      <td>0.666096</td>\n",
       "      <td>-0.062036</td>\n",
       "      <td>-1.527498</td>\n",
       "      <td>-0.841697</td>\n",
       "      <td>-1.016349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">roberta</th>\n",
       "      <th>0</th>\n",
       "      <td>1.218710</td>\n",
       "      <td>1.264383</td>\n",
       "      <td>0.459496</td>\n",
       "      <td>0.683013</td>\n",
       "      <td>1.422881</td>\n",
       "      <td>1.298358</td>\n",
       "      <td>-1.080391</td>\n",
       "      <td>-0.471747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.123317</td>\n",
       "      <td>1.210698</td>\n",
       "      <td>0.423446</td>\n",
       "      <td>0.662252</td>\n",
       "      <td>0.865828</td>\n",
       "      <td>0.336638</td>\n",
       "      <td>-1.801066</td>\n",
       "      <td>-1.140114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.044592</td>\n",
       "      <td>1.090103</td>\n",
       "      <td>0.472359</td>\n",
       "      <td>0.674474</td>\n",
       "      <td>0.406105</td>\n",
       "      <td>-1.823676</td>\n",
       "      <td>-0.823260</td>\n",
       "      <td>-0.746658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.210434</td>\n",
       "      <td>1.297725</td>\n",
       "      <td>0.469502</td>\n",
       "      <td>0.672637</td>\n",
       "      <td>1.374553</td>\n",
       "      <td>1.895641</td>\n",
       "      <td>-0.880360</td>\n",
       "      <td>-0.805768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.830285</td>\n",
       "      <td>1.182601</td>\n",
       "      <td>0.462775</td>\n",
       "      <td>0.671760</td>\n",
       "      <td>-0.845366</td>\n",
       "      <td>-0.166675</td>\n",
       "      <td>-1.014848</td>\n",
       "      <td>-0.834004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">standard</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">bart</th>\n",
       "      <th>0</th>\n",
       "      <td>0.776456</td>\n",
       "      <td>1.189991</td>\n",
       "      <td>0.422833</td>\n",
       "      <td>0.685542</td>\n",
       "      <td>-1.159708</td>\n",
       "      <td>-0.034303</td>\n",
       "      <td>-1.813315</td>\n",
       "      <td>-0.390348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.164304</td>\n",
       "      <td>1.172861</td>\n",
       "      <td>0.453643</td>\n",
       "      <td>0.680725</td>\n",
       "      <td>1.105175</td>\n",
       "      <td>-0.341168</td>\n",
       "      <td>-1.197403</td>\n",
       "      <td>-0.545395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.859977</td>\n",
       "      <td>1.185323</td>\n",
       "      <td>0.471587</td>\n",
       "      <td>0.677326</td>\n",
       "      <td>-0.671978</td>\n",
       "      <td>-0.117924</td>\n",
       "      <td>-0.838680</td>\n",
       "      <td>-0.654843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.775503</td>\n",
       "      <td>1.161450</td>\n",
       "      <td>0.465650</td>\n",
       "      <td>0.700102</td>\n",
       "      <td>-1.165274</td>\n",
       "      <td>-0.545571</td>\n",
       "      <td>-0.957371</td>\n",
       "      <td>0.078401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.691525</td>\n",
       "      <td>1.200358</td>\n",
       "      <td>0.472493</td>\n",
       "      <td>0.643385</td>\n",
       "      <td>-1.655668</td>\n",
       "      <td>0.151414</td>\n",
       "      <td>-0.820580</td>\n",
       "      <td>-1.747479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">roberta</th>\n",
       "      <th>0</th>\n",
       "      <td>0.916855</td>\n",
       "      <td>1.136921</td>\n",
       "      <td>0.477680</td>\n",
       "      <td>0.661339</td>\n",
       "      <td>-0.339830</td>\n",
       "      <td>-0.984987</td>\n",
       "      <td>-0.716877</td>\n",
       "      <td>-1.169508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.156464</td>\n",
       "      <td>1.202012</td>\n",
       "      <td>0.502311</td>\n",
       "      <td>0.640945</td>\n",
       "      <td>1.059391</td>\n",
       "      <td>0.181036</td>\n",
       "      <td>-0.224481</td>\n",
       "      <td>-1.826037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.101127</td>\n",
       "      <td>1.149108</td>\n",
       "      <td>0.462049</td>\n",
       "      <td>0.674422</td>\n",
       "      <td>0.736247</td>\n",
       "      <td>-0.766675</td>\n",
       "      <td>-1.029354</td>\n",
       "      <td>-0.748324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.863637</td>\n",
       "      <td>1.201630</td>\n",
       "      <td>0.454965</td>\n",
       "      <td>0.638983</td>\n",
       "      <td>-0.650601</td>\n",
       "      <td>0.174200</td>\n",
       "      <td>-1.170967</td>\n",
       "      <td>-1.889204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.936036</td>\n",
       "      <td>1.166703</td>\n",
       "      <td>0.444749</td>\n",
       "      <td>0.650069</td>\n",
       "      <td>-0.227820</td>\n",
       "      <td>-0.451485</td>\n",
       "      <td>-1.375203</td>\n",
       "      <td>-1.532317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"20\" valign=\"top\">trained</th>\n",
       "      <th rowspan=\"10\" valign=\"top\">isomers</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">bart</th>\n",
       "      <th>0</th>\n",
       "      <td>0.982850</td>\n",
       "      <td>1.162689</td>\n",
       "      <td>0.599165</td>\n",
       "      <td>0.683260</td>\n",
       "      <td>0.045554</td>\n",
       "      <td>-0.523384</td>\n",
       "      <td>1.711717</td>\n",
       "      <td>-0.463811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.743716</td>\n",
       "      <td>1.190602</td>\n",
       "      <td>0.525818</td>\n",
       "      <td>0.701761</td>\n",
       "      <td>-1.350895</td>\n",
       "      <td>-0.023352</td>\n",
       "      <td>0.245453</td>\n",
       "      <td>0.131808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.781104</td>\n",
       "      <td>1.218825</td>\n",
       "      <td>0.552721</td>\n",
       "      <td>0.699510</td>\n",
       "      <td>-1.132565</td>\n",
       "      <td>0.482237</td>\n",
       "      <td>0.783254</td>\n",
       "      <td>0.059331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.748217</td>\n",
       "      <td>1.139424</td>\n",
       "      <td>0.537931</td>\n",
       "      <td>0.679769</td>\n",
       "      <td>-1.324610</td>\n",
       "      <td>-0.940146</td>\n",
       "      <td>0.487599</td>\n",
       "      <td>-0.576189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.186177</td>\n",
       "      <td>1.149272</td>\n",
       "      <td>0.535864</td>\n",
       "      <td>0.680846</td>\n",
       "      <td>1.232905</td>\n",
       "      <td>-0.763733</td>\n",
       "      <td>0.446280</td>\n",
       "      <td>-0.541499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">roberta</th>\n",
       "      <th>0</th>\n",
       "      <td>0.959269</td>\n",
       "      <td>1.278790</td>\n",
       "      <td>0.524625</td>\n",
       "      <td>0.712037</td>\n",
       "      <td>-0.092150</td>\n",
       "      <td>1.556434</td>\n",
       "      <td>0.221590</td>\n",
       "      <td>0.462592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.961851</td>\n",
       "      <td>1.260700</td>\n",
       "      <td>0.519156</td>\n",
       "      <td>0.732894</td>\n",
       "      <td>-0.077073</td>\n",
       "      <td>1.232378</td>\n",
       "      <td>0.112264</td>\n",
       "      <td>1.134034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.959308</td>\n",
       "      <td>1.177521</td>\n",
       "      <td>0.506804</td>\n",
       "      <td>0.733938</td>\n",
       "      <td>-0.091923</td>\n",
       "      <td>-0.257687</td>\n",
       "      <td>-0.134654</td>\n",
       "      <td>1.167644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.157611</td>\n",
       "      <td>1.205623</td>\n",
       "      <td>0.518697</td>\n",
       "      <td>0.730948</td>\n",
       "      <td>1.066088</td>\n",
       "      <td>0.245733</td>\n",
       "      <td>0.103097</td>\n",
       "      <td>1.071393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.936007</td>\n",
       "      <td>1.224376</td>\n",
       "      <td>0.518875</td>\n",
       "      <td>0.701344</td>\n",
       "      <td>-0.227992</td>\n",
       "      <td>0.581670</td>\n",
       "      <td>0.106650</td>\n",
       "      <td>0.118379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">standard</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">bart</th>\n",
       "      <th>0</th>\n",
       "      <td>1.238199</td>\n",
       "      <td>1.172745</td>\n",
       "      <td>0.638524</td>\n",
       "      <td>0.693035</td>\n",
       "      <td>1.536694</td>\n",
       "      <td>-0.343250</td>\n",
       "      <td>2.498546</td>\n",
       "      <td>-0.149126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.034957</td>\n",
       "      <td>1.184647</td>\n",
       "      <td>0.546989</td>\n",
       "      <td>0.680766</td>\n",
       "      <td>0.349838</td>\n",
       "      <td>-0.130037</td>\n",
       "      <td>0.668671</td>\n",
       "      <td>-0.544100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.835446</td>\n",
       "      <td>1.119353</td>\n",
       "      <td>0.538539</td>\n",
       "      <td>0.712077</td>\n",
       "      <td>-0.815228</td>\n",
       "      <td>-1.299695</td>\n",
       "      <td>0.499758</td>\n",
       "      <td>0.463906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.868344</td>\n",
       "      <td>1.106184</td>\n",
       "      <td>0.529581</td>\n",
       "      <td>0.701846</td>\n",
       "      <td>-0.623118</td>\n",
       "      <td>-1.535615</td>\n",
       "      <td>0.320676</td>\n",
       "      <td>0.134519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.910983</td>\n",
       "      <td>1.085427</td>\n",
       "      <td>0.525551</td>\n",
       "      <td>0.704656</td>\n",
       "      <td>-0.374123</td>\n",
       "      <td>-1.907454</td>\n",
       "      <td>0.240099</td>\n",
       "      <td>0.224982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">roberta</th>\n",
       "      <th>0</th>\n",
       "      <td>0.975692</td>\n",
       "      <td>1.179015</td>\n",
       "      <td>0.518678</td>\n",
       "      <td>0.689169</td>\n",
       "      <td>0.003754</td>\n",
       "      <td>-0.230921</td>\n",
       "      <td>0.102711</td>\n",
       "      <td>-0.273578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.191827</td>\n",
       "      <td>1.302003</td>\n",
       "      <td>0.485532</td>\n",
       "      <td>0.702191</td>\n",
       "      <td>1.265896</td>\n",
       "      <td>1.972266</td>\n",
       "      <td>-0.559919</td>\n",
       "      <td>0.145631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.948245</td>\n",
       "      <td>1.180178</td>\n",
       "      <td>0.491203</td>\n",
       "      <td>0.719507</td>\n",
       "      <td>-0.156527</td>\n",
       "      <td>-0.210079</td>\n",
       "      <td>-0.446551</td>\n",
       "      <td>0.703069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.018445</td>\n",
       "      <td>1.107126</td>\n",
       "      <td>0.512733</td>\n",
       "      <td>0.704742</td>\n",
       "      <td>0.253415</td>\n",
       "      <td>-1.518732</td>\n",
       "      <td>-0.016130</td>\n",
       "      <td>0.227775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.967159</td>\n",
       "      <td>1.125687</td>\n",
       "      <td>0.531926</td>\n",
       "      <td>0.745870</td>\n",
       "      <td>-0.046077</td>\n",
       "      <td>-1.186231</td>\n",
       "      <td>0.367557</td>\n",
       "      <td>1.551766</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               rectified_mean_squared_error  \\\n",
       "task                                                        bace_regression   \n",
       "embedding tokenizer dataset  architecture seed                                \n",
       "selfies   atom      isomers  bart         0                        0.867640   \n",
       "                                          1                        0.766853   \n",
       "                                          2                        0.840789   \n",
       "                                          3                        0.787273   \n",
       "                                          4                        1.125730   \n",
       "                             roberta      0                        1.273632   \n",
       "                                          1                        0.909233   \n",
       "                                          2                        1.161970   \n",
       "                                          3                        1.211683   \n",
       "                                          4                        1.237530   \n",
       "                    standard bart         0                        1.168866   \n",
       "                                          1                        0.779775   \n",
       "                                          2                        0.762166   \n",
       "                                          3                        0.821160   \n",
       "                                          4                        0.732206   \n",
       "                             roberta      0                        1.242868   \n",
       "                                          1                        1.225243   \n",
       "                                          2                        0.776240   \n",
       "                                          3                        0.835992   \n",
       "                                          4                        1.225987   \n",
       "          trained   isomers  bart         0                        1.094915   \n",
       "                                          1                        1.107150   \n",
       "                                          2                        1.325104   \n",
       "                                          3                        1.084281   \n",
       "                                          4                        1.193118   \n",
       "                             roberta      0                        0.792596   \n",
       "                                          1                        0.831032   \n",
       "                                          2                        0.904313   \n",
       "                                          3                        1.203126   \n",
       "                                          4                        0.778719   \n",
       "                    standard bart         0                        1.146895   \n",
       "                                          1                        0.870793   \n",
       "                                          2                        0.930218   \n",
       "                                          3                        0.967492   \n",
       "                                          4                        1.281114   \n",
       "                             roberta      0                        0.926161   \n",
       "                                          1                        0.824731   \n",
       "                                          2                        0.849218   \n",
       "                                          3                        0.945388   \n",
       "                                          4                        0.912304   \n",
       "smiles    atom      isomers  bart         0                        0.801891   \n",
       "                                          1                        0.847840   \n",
       "                                          2                        0.794942   \n",
       "                                          3                        0.798707   \n",
       "                                          4                        0.964426   \n",
       "                             roberta      0                        1.218710   \n",
       "                                          1                        1.123317   \n",
       "                                          2                        1.044592   \n",
       "                                          3                        1.210434   \n",
       "                                          4                        0.830285   \n",
       "                    standard bart         0                        0.776456   \n",
       "                                          1                        1.164304   \n",
       "                                          2                        0.859977   \n",
       "                                          3                        0.775503   \n",
       "                                          4                        0.691525   \n",
       "                             roberta      0                        0.916855   \n",
       "                                          1                        1.156464   \n",
       "                                          2                        1.101127   \n",
       "                                          3                        0.863637   \n",
       "                                          4                        0.936036   \n",
       "          trained   isomers  bart         0                        0.982850   \n",
       "                                          1                        0.743716   \n",
       "                                          2                        0.781104   \n",
       "                                          3                        0.748217   \n",
       "                                          4                        1.186177   \n",
       "                             roberta      0                        0.959269   \n",
       "                                          1                        0.961851   \n",
       "                                          2                        0.959308   \n",
       "                                          3                        1.157611   \n",
       "                                          4                        0.936007   \n",
       "                    standard bart         0                        1.238199   \n",
       "                                          1                        1.034957   \n",
       "                                          2                        0.835446   \n",
       "                                          3                        0.868344   \n",
       "                                          4                        0.910983   \n",
       "                             roberta      0                        0.975692   \n",
       "                                          1                        1.191827   \n",
       "                                          2                        0.948245   \n",
       "                                          3                        1.018445   \n",
       "                                          4                        0.967159   \n",
       "\n",
       "                                                                              \\\n",
       "task                                           clearance   delaney      lipo   \n",
       "embedding tokenizer dataset  architecture seed                                 \n",
       "selfies   atom      isomers  bart         0     1.131814  0.588621  0.732742   \n",
       "                                          1     1.122419  0.589391  0.701290   \n",
       "                                          2     1.168559  0.558832  0.695183   \n",
       "                                          3     1.169699  0.561634  0.681776   \n",
       "                                          4     1.203900  0.593769  0.695180   \n",
       "                             roberta      0     1.140953  0.459100  0.760557   \n",
       "                                          1     1.169139  0.458983  0.697795   \n",
       "                                          2     1.211294  0.526965  0.729796   \n",
       "                                          3     1.291416  0.492840  0.734060   \n",
       "                                          4     1.392441  0.460964  0.734539   \n",
       "                    standard bart         0     1.205083  0.658529  0.695181   \n",
       "                                          1     1.183429  0.602495  0.689594   \n",
       "                                          2     1.144793  0.627679  0.656210   \n",
       "                                          3     1.184509  0.569119  0.669667   \n",
       "                                          4     1.194193  0.580774  0.680026   \n",
       "                             roberta      0     1.206969  0.474809  0.669837   \n",
       "                                          1     1.212885  0.447627  0.718285   \n",
       "                                          2     1.188664  0.459117  0.685333   \n",
       "                                          3     1.172775  0.430063  0.690933   \n",
       "                                          4     1.217195  0.459247  0.700320   \n",
       "          trained   isomers  bart         0     1.138284  0.500634  0.680783   \n",
       "                                          1     1.208899  0.480643  0.674653   \n",
       "                                          2     1.167810  0.486166  0.652172   \n",
       "                                          3     1.182405  0.525232  0.665512   \n",
       "                                          4     1.177850  0.542705  0.679922   \n",
       "                             roberta      0     1.184835  0.520051  0.748091   \n",
       "                                          1     1.170689  0.514270  0.731457   \n",
       "                                          2     1.311176  0.530704  0.737419   \n",
       "                                          3     1.178287  0.506735  0.735168   \n",
       "                                          4     1.187267  0.491517  0.720044   \n",
       "                    standard bart         0     1.231815  0.527737  0.703292   \n",
       "                                          1     1.251535  0.525161  0.697981   \n",
       "                                          2     1.199987  0.506908  0.701003   \n",
       "                                          3     1.227695  0.564684  0.717069   \n",
       "                                          4     1.305747  0.519464  0.705946   \n",
       "                             roberta      0     1.269090  0.496169  0.728459   \n",
       "                                          1     1.213574  0.496941  0.720099   \n",
       "                                          2     1.214858  0.494477  0.819233   \n",
       "                                          3     1.322168  0.523504  0.755256   \n",
       "                                          4     1.189672  0.552072  0.734376   \n",
       "smiles    atom      isomers  bart         0     1.166224  0.541236  0.661239   \n",
       "                                          1     1.165750  0.475719  0.689789   \n",
       "                                          2     1.157096  0.512823  0.677712   \n",
       "                                          3     1.128919  0.601219  0.665154   \n",
       "                                          4     1.106637  0.471436  0.666096   \n",
       "                             roberta      0     1.264383  0.459496  0.683013   \n",
       "                                          1     1.210698  0.423446  0.662252   \n",
       "                                          2     1.090103  0.472359  0.674474   \n",
       "                                          3     1.297725  0.469502  0.672637   \n",
       "                                          4     1.182601  0.462775  0.671760   \n",
       "                    standard bart         0     1.189991  0.422833  0.685542   \n",
       "                                          1     1.172861  0.453643  0.680725   \n",
       "                                          2     1.185323  0.471587  0.677326   \n",
       "                                          3     1.161450  0.465650  0.700102   \n",
       "                                          4     1.200358  0.472493  0.643385   \n",
       "                             roberta      0     1.136921  0.477680  0.661339   \n",
       "                                          1     1.202012  0.502311  0.640945   \n",
       "                                          2     1.149108  0.462049  0.674422   \n",
       "                                          3     1.201630  0.454965  0.638983   \n",
       "                                          4     1.166703  0.444749  0.650069   \n",
       "          trained   isomers  bart         0     1.162689  0.599165  0.683260   \n",
       "                                          1     1.190602  0.525818  0.701761   \n",
       "                                          2     1.218825  0.552721  0.699510   \n",
       "                                          3     1.139424  0.537931  0.679769   \n",
       "                                          4     1.149272  0.535864  0.680846   \n",
       "                             roberta      0     1.278790  0.524625  0.712037   \n",
       "                                          1     1.260700  0.519156  0.732894   \n",
       "                                          2     1.177521  0.506804  0.733938   \n",
       "                                          3     1.205623  0.518697  0.730948   \n",
       "                                          4     1.224376  0.518875  0.701344   \n",
       "                    standard bart         0     1.172745  0.638524  0.693035   \n",
       "                                          1     1.184647  0.546989  0.680766   \n",
       "                                          2     1.119353  0.538539  0.712077   \n",
       "                                          3     1.106184  0.529581  0.701846   \n",
       "                                          4     1.085427  0.525551  0.704656   \n",
       "                             roberta      0     1.179015  0.518678  0.689169   \n",
       "                                          1     1.302003  0.485532  0.702191   \n",
       "                                          2     1.180178  0.491203  0.719507   \n",
       "                                          3     1.107126  0.512733  0.704742   \n",
       "                                          4     1.125687  0.531926  0.745870   \n",
       "\n",
       "                                               z_rectified_mean_squared_error_bace_regression  \\\n",
       "task                                                                                            \n",
       "embedding tokenizer dataset  architecture seed                                                  \n",
       "selfies   atom      isomers  bart         0                                         -0.627229   \n",
       "                                          1                                         -1.215783   \n",
       "                                          2                                         -0.784028   \n",
       "                                          3                                         -1.096542   \n",
       "                                          4                                          0.879919   \n",
       "                             roberta      0                                          1.743605   \n",
       "                                          1                                         -0.384343   \n",
       "                                          2                                          1.091542   \n",
       "                                          3                                          1.381849   \n",
       "                                          4                                          1.532786   \n",
       "                    standard bart         0                                          1.131813   \n",
       "                                          1                                         -1.140327   \n",
       "                                          2                                         -1.243156   \n",
       "                                          3                                         -0.898651   \n",
       "                                          4                                         -1.418107   \n",
       "                             roberta      0                                          1.563954   \n",
       "                                          1                                          1.461031   \n",
       "                                          2                                         -1.160966   \n",
       "                                          3                                         -0.812039   \n",
       "                                          4                                          1.465376   \n",
       "          trained   isomers  bart         0                                          0.699968   \n",
       "                                          1                                          0.771419   \n",
       "                                          2                                          2.044182   \n",
       "                                          3                                          0.637868   \n",
       "                                          4                                          1.273435   \n",
       "                             roberta      0                                         -1.065458   \n",
       "                                          1                                         -0.841001   \n",
       "                                          2                                         -0.413072   \n",
       "                                          3                                          1.331879   \n",
       "                                          4                                         -1.146490   \n",
       "                    standard bart         0                                          1.003514   \n",
       "                                          1                                         -0.608814   \n",
       "                                          2                                         -0.261794   \n",
       "                                          3                                         -0.044128   \n",
       "                                          4                                          1.787297   \n",
       "                             roberta      0                                         -0.285486   \n",
       "                                          1                                         -0.877798   \n",
       "                                          2                                         -0.734807   \n",
       "                                          3                                         -0.173211   \n",
       "                                          4                                         -0.366405   \n",
       "smiles    atom      isomers  bart         0                                         -1.011177   \n",
       "                                          1                                         -0.742853   \n",
       "                                          2                                         -1.051758   \n",
       "                                          3                                         -1.029772   \n",
       "                                          4                                         -0.062036   \n",
       "                             roberta      0                                          1.422881   \n",
       "                                          1                                          0.865828   \n",
       "                                          2                                          0.406105   \n",
       "                                          3                                          1.374553   \n",
       "                                          4                                         -0.845366   \n",
       "                    standard bart         0                                         -1.159708   \n",
       "                                          1                                          1.105175   \n",
       "                                          2                                         -0.671978   \n",
       "                                          3                                         -1.165274   \n",
       "                                          4                                         -1.655668   \n",
       "                             roberta      0                                         -0.339830   \n",
       "                                          1                                          1.059391   \n",
       "                                          2                                          0.736247   \n",
       "                                          3                                         -0.650601   \n",
       "                                          4                                         -0.227820   \n",
       "          trained   isomers  bart         0                                          0.045554   \n",
       "                                          1                                         -1.350895   \n",
       "                                          2                                         -1.132565   \n",
       "                                          3                                         -1.324610   \n",
       "                                          4                                          1.232905   \n",
       "                             roberta      0                                         -0.092150   \n",
       "                                          1                                         -0.077073   \n",
       "                                          2                                         -0.091923   \n",
       "                                          3                                          1.066088   \n",
       "                                          4                                         -0.227992   \n",
       "                    standard bart         0                                          1.536694   \n",
       "                                          1                                          0.349838   \n",
       "                                          2                                         -0.815228   \n",
       "                                          3                                         -0.623118   \n",
       "                                          4                                         -0.374123   \n",
       "                             roberta      0                                          0.003754   \n",
       "                                          1                                          1.265896   \n",
       "                                          2                                         -0.156527   \n",
       "                                          3                                          0.253415   \n",
       "                                          4                                         -0.046077   \n",
       "\n",
       "                                               z_rectified_mean_squared_error_clearance  \\\n",
       "task                                                                                      \n",
       "embedding tokenizer dataset  architecture seed                                            \n",
       "selfies   atom      isomers  bart         0                                   -1.076476   \n",
       "                                          1                                   -1.244779   \n",
       "                                          2                                   -0.418236   \n",
       "                                          3                                   -0.397809   \n",
       "                                          4                                    0.214865   \n",
       "                             roberta      0                                   -0.912763   \n",
       "                                          1                                   -0.407843   \n",
       "                                          2                                    0.347325   \n",
       "                                          3                                    1.782627   \n",
       "                                          4                                    3.592365   \n",
       "                    standard bart         0                                    0.236067   \n",
       "                                          1                                   -0.151853   \n",
       "                                          2                                   -0.843977   \n",
       "                                          3                                   -0.132509   \n",
       "                                          4                                    0.040981   \n",
       "                             roberta      0                                    0.269839   \n",
       "                                          1                                    0.375823   \n",
       "                                          2                                   -0.058075   \n",
       "                                          3                                   -0.342711   \n",
       "                                          4                                    0.453038   \n",
       "          trained   isomers  bart         0                                   -0.960572   \n",
       "                                          1                                    0.304410   \n",
       "                                          2                                   -0.431648   \n",
       "                                          3                                   -0.170185   \n",
       "                                          4                                   -0.251796   \n",
       "                             roberta      0                                   -0.126656   \n",
       "                                          1                                   -0.380070   \n",
       "                                          2                                    2.136593   \n",
       "                                          3                                   -0.243964   \n",
       "                                          4                                   -0.083101   \n",
       "                    standard bart         0                                    0.714926   \n",
       "                                          1                                    1.068203   \n",
       "                                          2                                    0.144765   \n",
       "                                          3                                    0.641127   \n",
       "                                          4                                    2.039338   \n",
       "                             roberta      0                                    1.382679   \n",
       "                                          1                                    0.388167   \n",
       "                                          2                                    0.411173   \n",
       "                                          3                                    2.333513   \n",
       "                                          4                                   -0.040013   \n",
       "smiles    atom      isomers  bart         0                                   -0.460053   \n",
       "                                          1                                   -0.468554   \n",
       "                                          2                                   -0.623568   \n",
       "                                          3                                   -1.128342   \n",
       "                                          4                                   -1.527498   \n",
       "                             roberta      0                                    1.298358   \n",
       "                                          1                                    0.336638   \n",
       "                                          2                                   -1.823676   \n",
       "                                          3                                    1.895641   \n",
       "                                          4                                   -0.166675   \n",
       "                    standard bart         0                                   -0.034303   \n",
       "                                          1                                   -0.341168   \n",
       "                                          2                                   -0.117924   \n",
       "                                          3                                   -0.545571   \n",
       "                                          4                                    0.151414   \n",
       "                             roberta      0                                   -0.984987   \n",
       "                                          1                                    0.181036   \n",
       "                                          2                                   -0.766675   \n",
       "                                          3                                    0.174200   \n",
       "                                          4                                   -0.451485   \n",
       "          trained   isomers  bart         0                                   -0.523384   \n",
       "                                          1                                   -0.023352   \n",
       "                                          2                                    0.482237   \n",
       "                                          3                                   -0.940146   \n",
       "                                          4                                   -0.763733   \n",
       "                             roberta      0                                    1.556434   \n",
       "                                          1                                    1.232378   \n",
       "                                          2                                   -0.257687   \n",
       "                                          3                                    0.245733   \n",
       "                                          4                                    0.581670   \n",
       "                    standard bart         0                                   -0.343250   \n",
       "                                          1                                   -0.130037   \n",
       "                                          2                                   -1.299695   \n",
       "                                          3                                   -1.535615   \n",
       "                                          4                                   -1.907454   \n",
       "                             roberta      0                                   -0.230921   \n",
       "                                          1                                    1.972266   \n",
       "                                          2                                   -0.210079   \n",
       "                                          3                                   -1.518732   \n",
       "                                          4                                   -1.186231   \n",
       "\n",
       "                                               z_rectified_mean_squared_error_delaney  \\\n",
       "task                                                                                    \n",
       "embedding tokenizer dataset  architecture seed                                          \n",
       "selfies   atom      isomers  bart         0                                  1.500931   \n",
       "                                          1                                  1.516322   \n",
       "                                          2                                  0.905429   \n",
       "                                          3                                  0.961442   \n",
       "                                          4                                  1.603845   \n",
       "                             roberta      0                                 -1.088321   \n",
       "                                          1                                 -1.090650   \n",
       "                                          2                                  0.268367   \n",
       "                                          3                                 -0.413808   \n",
       "                                          4                                 -1.051050   \n",
       "                    standard bart         0                                  2.898472   \n",
       "                                          1                                  1.778292   \n",
       "                                          2                                  2.281739   \n",
       "                                          3                                  1.111066   \n",
       "                                          4                                  1.344058   \n",
       "                             roberta      0                                 -0.774265   \n",
       "                                          1                                 -1.317673   \n",
       "                                          2                                 -1.087981   \n",
       "                                          3                                 -1.668782   \n",
       "                                          4                                 -1.085381   \n",
       "          trained   isomers  bart         0                                 -0.258013   \n",
       "                                          1                                 -0.657647   \n",
       "                                          2                                 -0.547230   \n",
       "                                          3                                  0.233736   \n",
       "                                          4                                  0.583032   \n",
       "                             roberta      0                                  0.130152   \n",
       "                                          1                                  0.014595   \n",
       "                                          2                                  0.343119   \n",
       "                                          3                                 -0.136042   \n",
       "                                          4                                 -0.440255   \n",
       "                    standard bart         0                                  0.283813   \n",
       "                                          1                                  0.232317   \n",
       "                                          2                                 -0.132593   \n",
       "                                          3                                  1.022418   \n",
       "                                          4                                  0.118423   \n",
       "                             roberta      0                                 -0.347272   \n",
       "                                          1                                 -0.331837   \n",
       "                                          2                                 -0.381089   \n",
       "                                          3                                  0.199186   \n",
       "                                          4                                  0.770285   \n",
       "smiles    atom      isomers  bart         0                                  0.553672   \n",
       "                                          1                                 -0.756090   \n",
       "                                          2                                 -0.014343   \n",
       "                                          3                                  1.752793   \n",
       "                                          4                                 -0.841697   \n",
       "                             roberta      0                                 -1.080391   \n",
       "                                          1                                 -1.801066   \n",
       "                                          2                                 -0.823260   \n",
       "                                          3                                 -0.880360   \n",
       "                                          4                                 -1.014848   \n",
       "                    standard bart         0                                 -1.813315   \n",
       "                                          1                                 -1.197403   \n",
       "                                          2                                 -0.838680   \n",
       "                                          3                                 -0.957371   \n",
       "                                          4                                 -0.820580   \n",
       "                             roberta      0                                 -0.716877   \n",
       "                                          1                                 -0.224481   \n",
       "                                          2                                 -1.029354   \n",
       "                                          3                                 -1.170967   \n",
       "                                          4                                 -1.375203   \n",
       "          trained   isomers  bart         0                                  1.711717   \n",
       "                                          1                                  0.245453   \n",
       "                                          2                                  0.783254   \n",
       "                                          3                                  0.487599   \n",
       "                                          4                                  0.446280   \n",
       "                             roberta      0                                  0.221590   \n",
       "                                          1                                  0.112264   \n",
       "                                          2                                 -0.134654   \n",
       "                                          3                                  0.103097   \n",
       "                                          4                                  0.106650   \n",
       "                    standard bart         0                                  2.498546   \n",
       "                                          1                                  0.668671   \n",
       "                                          2                                  0.499758   \n",
       "                                          3                                  0.320676   \n",
       "                                          4                                  0.240099   \n",
       "                             roberta      0                                  0.102711   \n",
       "                                          1                                 -0.559919   \n",
       "                                          2                                 -0.446551   \n",
       "                                          3                                 -0.016130   \n",
       "                                          4                                  0.367557   \n",
       "\n",
       "                                               z_rectified_mean_squared_error_lipo  \n",
       "task                                                                                \n",
       "embedding tokenizer dataset  architecture seed                                      \n",
       "selfies   atom      isomers  bart         0                               1.129169  \n",
       "                                          1                               0.116635  \n",
       "                                          2                              -0.079967  \n",
       "                                          3                              -0.511560  \n",
       "                                          4                              -0.080073  \n",
       "                             roberta      0                               2.024608  \n",
       "                                          1                               0.004124  \n",
       "                                          2                               1.034300  \n",
       "                                          3                               1.171599  \n",
       "                                          4                               1.186997  \n",
       "                    standard bart         0                              -0.080038  \n",
       "                                          1                              -0.259894  \n",
       "                                          2                              -1.334623  \n",
       "                                          3                              -0.901392  \n",
       "                                          4                              -0.567904  \n",
       "                             roberta      0                              -0.895939  \n",
       "                                          1                               0.663752  \n",
       "                                          2                              -0.397081  \n",
       "                                          3                              -0.216780  \n",
       "                                          4                               0.085390  \n",
       "          trained   isomers  bart         0                              -0.543549  \n",
       "                                          1                              -0.740891  \n",
       "                                          2                              -1.464621  \n",
       "                                          3                              -1.035155  \n",
       "                                          4                              -0.571251  \n",
       "                             roberta      0                               1.623277  \n",
       "                                          1                               1.087797  \n",
       "                                          2                               1.279704  \n",
       "                                          3                               1.207261  \n",
       "                                          4                               0.720374  \n",
       "                    standard bart         0                               0.181074  \n",
       "                                          1                               0.010102  \n",
       "                                          2                               0.107408  \n",
       "                                          3                               0.624606  \n",
       "                                          4                               0.266525  \n",
       "                             roberta      0                               0.991268  \n",
       "                                          1                               0.722128  \n",
       "                                          2                               3.913531  \n",
       "                                          3                               1.853942  \n",
       "                                          4                               1.181759  \n",
       "smiles    atom      isomers  bart         0                              -1.172722  \n",
       "                                          1                              -0.253624  \n",
       "                                          2                              -0.642401  \n",
       "                                          3                              -1.046691  \n",
       "                                          4                              -1.016349  \n",
       "                             roberta      0                              -0.471747  \n",
       "                                          1                              -1.140114  \n",
       "                                          2                              -0.746658  \n",
       "                                          3                              -0.805768  \n",
       "                                          4                              -0.834004  \n",
       "                    standard bart         0                              -0.390348  \n",
       "                                          1                              -0.545395  \n",
       "                                          2                              -0.654843  \n",
       "                                          3                               0.078401  \n",
       "                                          4                              -1.747479  \n",
       "                             roberta      0                              -1.169508  \n",
       "                                          1                              -1.826037  \n",
       "                                          2                              -0.748324  \n",
       "                                          3                              -1.889204  \n",
       "                                          4                              -1.532317  \n",
       "          trained   isomers  bart         0                              -0.463811  \n",
       "                                          1                               0.131808  \n",
       "                                          2                               0.059331  \n",
       "                                          3                              -0.576189  \n",
       "                                          4                              -0.541499  \n",
       "                             roberta      0                               0.462592  \n",
       "                                          1                               1.134034  \n",
       "                                          2                               1.167644  \n",
       "                                          3                               1.071393  \n",
       "                                          4                               0.118379  \n",
       "                    standard bart         0                              -0.149126  \n",
       "                                          1                              -0.544100  \n",
       "                                          2                               0.463906  \n",
       "                                          3                               0.134519  \n",
       "                                          4                               0.224982  \n",
       "                             roberta      0                              -0.273578  \n",
       "                                          1                               0.145631  \n",
       "                                          2                               0.703069  \n",
       "                                          3                               0.227775  \n",
       "                                          4                               1.551766  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression_score_columns = [\"rectified_mean_squared_error\"]\n",
    "regression_pivot = regression_scores.pivot_table(values=regression_score_columns, index=[\"embedding\",\"tokenizer\",\"dataset\",\"architecture\", \"seed\"],columns=[\"task\"])\n",
    "for metric in regression_score_columns:\n",
    "    for task in regression_scores.task.unique():\n",
    "        regression_pivot[f\"z_{metric}_{task}\"] = compute_zscore(regression_pivot[metric][task])\n",
    "regression_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9f4aa39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>z_rectified_mean_squared_error_clearance</th>\n",
       "      <th>z_rectified_mean_squared_error_delaney</th>\n",
       "      <th>z_rectified_mean_squared_error_lipo</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>task</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embedding</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>dataset</th>\n",
       "      <th>architecture</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">selfies</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">atom</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">isomers</th>\n",
       "      <th>bart</th>\n",
       "      <td>-0.584487</td>\n",
       "      <td>1.297594</td>\n",
       "      <td>0.114841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.880342</td>\n",
       "      <td>-0.675092</td>\n",
       "      <td>1.084326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">standard</th>\n",
       "      <th>bart</th>\n",
       "      <td>-0.170258</td>\n",
       "      <td>1.882725</td>\n",
       "      <td>-0.628770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.139583</td>\n",
       "      <td>-1.186816</td>\n",
       "      <td>-0.152132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">trained</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">isomers</th>\n",
       "      <th>bart</th>\n",
       "      <td>-0.301958</td>\n",
       "      <td>-0.129224</td>\n",
       "      <td>-0.871093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.260561</td>\n",
       "      <td>-0.017686</td>\n",
       "      <td>1.183682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">standard</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.921672</td>\n",
       "      <td>0.304876</td>\n",
       "      <td>0.237943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.895104</td>\n",
       "      <td>-0.018145</td>\n",
       "      <td>1.732526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">smiles</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">atom</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">isomers</th>\n",
       "      <th>bart</th>\n",
       "      <td>-0.841603</td>\n",
       "      <td>0.138867</td>\n",
       "      <td>-0.826357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.308057</td>\n",
       "      <td>-1.119985</td>\n",
       "      <td>-0.799658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">standard</th>\n",
       "      <th>bart</th>\n",
       "      <td>-0.177510</td>\n",
       "      <td>-1.125470</td>\n",
       "      <td>-0.651933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>-0.369582</td>\n",
       "      <td>-0.903376</td>\n",
       "      <td>-1.433078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">trained</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">isomers</th>\n",
       "      <th>bart</th>\n",
       "      <td>-0.353675</td>\n",
       "      <td>0.734860</td>\n",
       "      <td>-0.278072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.671705</td>\n",
       "      <td>0.081790</td>\n",
       "      <td>0.790808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">standard</th>\n",
       "      <th>bart</th>\n",
       "      <td>-1.043210</td>\n",
       "      <td>0.845550</td>\n",
       "      <td>0.026036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>-0.234739</td>\n",
       "      <td>-0.110466</td>\n",
       "      <td>0.470933</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          z_rectified_mean_squared_error_clearance  \\\n",
       "task                                                                                 \n",
       "embedding tokenizer dataset  architecture                                            \n",
       "selfies   atom      isomers  bart                                        -0.584487   \n",
       "                             roberta                                      0.880342   \n",
       "                    standard bart                                        -0.170258   \n",
       "                             roberta                                      0.139583   \n",
       "          trained   isomers  bart                                        -0.301958   \n",
       "                             roberta                                      0.260561   \n",
       "                    standard bart                                         0.921672   \n",
       "                             roberta                                      0.895104   \n",
       "smiles    atom      isomers  bart                                        -0.841603   \n",
       "                             roberta                                      0.308057   \n",
       "                    standard bart                                        -0.177510   \n",
       "                             roberta                                     -0.369582   \n",
       "          trained   isomers  bart                                        -0.353675   \n",
       "                             roberta                                      0.671705   \n",
       "                    standard bart                                        -1.043210   \n",
       "                             roberta                                     -0.234739   \n",
       "\n",
       "                                          z_rectified_mean_squared_error_delaney  \\\n",
       "task                                                                               \n",
       "embedding tokenizer dataset  architecture                                          \n",
       "selfies   atom      isomers  bart                                       1.297594   \n",
       "                             roberta                                   -0.675092   \n",
       "                    standard bart                                       1.882725   \n",
       "                             roberta                                   -1.186816   \n",
       "          trained   isomers  bart                                      -0.129224   \n",
       "                             roberta                                   -0.017686   \n",
       "                    standard bart                                       0.304876   \n",
       "                             roberta                                   -0.018145   \n",
       "smiles    atom      isomers  bart                                       0.138867   \n",
       "                             roberta                                   -1.119985   \n",
       "                    standard bart                                      -1.125470   \n",
       "                             roberta                                   -0.903376   \n",
       "          trained   isomers  bart                                       0.734860   \n",
       "                             roberta                                    0.081790   \n",
       "                    standard bart                                       0.845550   \n",
       "                             roberta                                   -0.110466   \n",
       "\n",
       "                                          z_rectified_mean_squared_error_lipo  \n",
       "task                                                                           \n",
       "embedding tokenizer dataset  architecture                                      \n",
       "selfies   atom      isomers  bart                                    0.114841  \n",
       "                             roberta                                 1.084326  \n",
       "                    standard bart                                   -0.628770  \n",
       "                             roberta                                -0.152132  \n",
       "          trained   isomers  bart                                   -0.871093  \n",
       "                             roberta                                 1.183682  \n",
       "                    standard bart                                    0.237943  \n",
       "                             roberta                                 1.732526  \n",
       "smiles    atom      isomers  bart                                   -0.826357  \n",
       "                             roberta                                -0.799658  \n",
       "                    standard bart                                   -0.651933  \n",
       "                             roberta                                -1.433078  \n",
       "          trained   isomers  bart                                   -0.278072  \n",
       "                             roberta                                 0.790808  \n",
       "                    standard bart                                    0.026036  \n",
       "                             roberta                                 0.470933  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression_cols = [\"embedding\",\"tokenizer\",\"dataset\",\"architecture\", \"z_rectified_mean_squared_error_clearance\",\"z_rectified_mean_squared_error_delaney\",\"z_rectified_mean_squared_error_lipo\"]\n",
    "z_df = regression_pivot.groupby(regression_cols[:4]).agg(\"mean\")[regression_cols[4:]]\n",
    "# https://iopscience.iop.org/article/10.1088/2632-2153/ac3ffb\n",
    "chemformer_best_rmse = {\"Clearance\": 1.230, \"Delaney\": 0.633, \"Lipo\": 0.598}\n",
    "molbert_best_rmse = {\"Clearance\": 0.948, \"Delaney\": 0.531, \"Lipo\": 0.561}\n",
    "z_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c02e5e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llllr}\n",
      "\\toprule\n",
      "       &         &          &         &     0 \\\\\n",
      "embedding & tokenizer & dataset & architecture &       \\\\\n",
      "\\midrule\n",
      "selfies & atom & isomers & bart &  0.28 \\\\\n",
      "       &         &          & roberta &  0.43 \\\\\n",
      "       &         & standard & bart &  0.36 \\\\\n",
      "       &         &          & roberta & -0.40 \\\\\n",
      "       & trained & isomers & bart & -0.43 \\\\\n",
      "       &         &          & roberta &  0.48 \\\\\n",
      "       &         & standard & bart &  0.49 \\\\\n",
      "       &         &          & roberta &  0.87 \\\\\n",
      "smiles & atom & isomers & bart & -0.51 \\\\\n",
      "       &         &          & roberta & -0.54 \\\\\n",
      "       &         & standard & bart & -0.65 \\\\\n",
      "       &         &          & roberta & -0.90 \\\\\n",
      "       & trained & isomers & bart &  0.03 \\\\\n",
      "       &         &          & roberta &  0.51 \\\\\n",
      "       &         & standard & bart & -0.06 \\\\\n",
      "       &         &          & roberta &  0.04 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{llllrrr}\n",
      "\\toprule\n",
      "       &         &          & {} & z\\_rectified\\_mean\\_squared\\_error\\_clearance & z\\_rectified\\_mean\\_squared\\_error\\_delaney & z\\_rectified\\_mean\\_squared\\_error\\_lipo \\\\\n",
      "       &         &          & task \\\\\n",
      "embedding & tokenizer & dataset & architecture &                                          &                                        &                                     \\\\\n",
      "\\midrule\n",
      "selfies & atom & isomers & bart &                                    -0.58 &                                   1.30 &                                0.11 \\\\\n",
      "       &         &          & roberta &                                     0.88 &                                  -0.68 &                                1.08 \\\\\n",
      "       &         & standard & bart &                                    -0.17 &                                   1.88 &                               -0.63 \\\\\n",
      "       &         &          & roberta &                                     0.14 &                                  -1.19 &                               -0.15 \\\\\n",
      "       & trained & isomers & bart &                                    -0.30 &                                  -0.13 &                               -0.87 \\\\\n",
      "       &         &          & roberta &                                     0.26 &                                  -0.02 &                                1.18 \\\\\n",
      "       &         & standard & bart &                                     0.92 &                                   0.30 &                                0.24 \\\\\n",
      "       &         &          & roberta &                                     0.90 &                                  -0.02 &                                1.73 \\\\\n",
      "smiles & atom & isomers & bart &                                    -0.84 &                                   0.14 &                               -0.83 \\\\\n",
      "       &         &          & roberta &                                     0.31 &                                  -1.12 &                               -0.80 \\\\\n",
      "       &         & standard & bart &                                    -0.18 &                                  -1.13 &                               -0.65 \\\\\n",
      "       &         &          & roberta &                                    -0.37 &                                  -0.90 &                               -1.43 \\\\\n",
      "       & trained & isomers & bart &                                    -0.35 &                                   0.73 &                               -0.28 \\\\\n",
      "       &         &          & roberta &                                     0.67 &                                   0.08 &                                0.79 \\\\\n",
      "       &         & standard & bart &                                    -1.04 &                                   0.85 &                                0.03 \\\\\n",
      "       &         &          & roberta &                                    -0.23 &                                  -0.11 &                                0.47 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3610201/1322612663.py:4: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  print(z_df.mean(axis=1).to_latex(float_format=\"%.2f\"))\n",
      "/tmp/ipykernel_3610201/1322612663.py:5: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  print(z_df.to_latex(float_format=\"%.2f\"))\n"
     ]
    }
   ],
   "source": [
    "interesting_rows = [('selfies','atom','standard','bart'), ('selfies', 'atom','standard','roberta'), ('smiles', 'atom','standard','roberta'),('smiles', 'atom','standard','bart'), ('smiles', 'trained','standard','roberta'), ('smiles', 'atom','isomers','roberta')]\n",
    "interesting_rows_regression = interesting_rows\n",
    "z_df_vals = z_df.mean(axis=1).values\n",
    "print(z_df.mean(axis=1).to_latex(float_format=\"%.2f\"))\n",
    "print(z_df.to_latex(float_format=\"%.2f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafc6383",
   "metadata": {},
   "source": [
    "## Most drilled down classification scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd320d9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"5\" halign=\"left\">ROC_AUC</th>\n",
       "      <th>z_ROC_AUC_bace_classification</th>\n",
       "      <th>z_ROC_AUC_bbbp</th>\n",
       "      <th>z_ROC_AUC_clintox</th>\n",
       "      <th>z_ROC_AUC_hiv</th>\n",
       "      <th>z_ROC_AUC_tox21</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>task</th>\n",
       "      <th>bace_classification</th>\n",
       "      <th>bbbp</th>\n",
       "      <th>clintox</th>\n",
       "      <th>hiv</th>\n",
       "      <th>tox21</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embedding</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>dataset</th>\n",
       "      <th>architecture</th>\n",
       "      <th>seed</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"40\" valign=\"top\">selfies</th>\n",
       "      <th rowspan=\"20\" valign=\"top\">atom</th>\n",
       "      <th rowspan=\"10\" valign=\"top\">isomers</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">bart</th>\n",
       "      <th>0</th>\n",
       "      <td>0.766486</td>\n",
       "      <td>0.682243</td>\n",
       "      <td>0.818841</td>\n",
       "      <td>0.743073</td>\n",
       "      <td>0.690075</td>\n",
       "      <td>0.200625</td>\n",
       "      <td>-1.001658</td>\n",
       "      <td>1.003315</td>\n",
       "      <td>-0.326548</td>\n",
       "      <td>0.381167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.776812</td>\n",
       "      <td>0.712111</td>\n",
       "      <td>0.856522</td>\n",
       "      <td>0.755044</td>\n",
       "      <td>0.676225</td>\n",
       "      <td>0.343906</td>\n",
       "      <td>0.334604</td>\n",
       "      <td>1.357927</td>\n",
       "      <td>0.235838</td>\n",
       "      <td>-0.016206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.754167</td>\n",
       "      <td>0.713749</td>\n",
       "      <td>0.650725</td>\n",
       "      <td>0.775503</td>\n",
       "      <td>0.675523</td>\n",
       "      <td>0.029693</td>\n",
       "      <td>0.407883</td>\n",
       "      <td>-0.578803</td>\n",
       "      <td>1.196967</td>\n",
       "      <td>-0.036355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.765399</td>\n",
       "      <td>0.657289</td>\n",
       "      <td>0.663768</td>\n",
       "      <td>0.721728</td>\n",
       "      <td>0.690485</td>\n",
       "      <td>0.185543</td>\n",
       "      <td>-2.118084</td>\n",
       "      <td>-0.456052</td>\n",
       "      <td>-1.329315</td>\n",
       "      <td>0.392920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.783514</td>\n",
       "      <td>0.739570</td>\n",
       "      <td>0.659420</td>\n",
       "      <td>0.770910</td>\n",
       "      <td>0.622211</td>\n",
       "      <td>0.436914</td>\n",
       "      <td>1.563104</td>\n",
       "      <td>-0.496969</td>\n",
       "      <td>0.981231</td>\n",
       "      <td>-1.565960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">roberta</th>\n",
       "      <th>0</th>\n",
       "      <td>0.807790</td>\n",
       "      <td>0.698430</td>\n",
       "      <td>0.918116</td>\n",
       "      <td>0.776729</td>\n",
       "      <td>0.637543</td>\n",
       "      <td>0.773750</td>\n",
       "      <td>-0.277490</td>\n",
       "      <td>1.937583</td>\n",
       "      <td>1.254560</td>\n",
       "      <td>-1.126052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.799094</td>\n",
       "      <td>0.694672</td>\n",
       "      <td>0.902174</td>\n",
       "      <td>0.765490</td>\n",
       "      <td>0.571629</td>\n",
       "      <td>0.653092</td>\n",
       "      <td>-0.445601</td>\n",
       "      <td>1.787554</td>\n",
       "      <td>0.726584</td>\n",
       "      <td>-3.017210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.801449</td>\n",
       "      <td>0.718759</td>\n",
       "      <td>0.865942</td>\n",
       "      <td>0.749682</td>\n",
       "      <td>0.693703</td>\n",
       "      <td>0.685770</td>\n",
       "      <td>0.632031</td>\n",
       "      <td>1.446581</td>\n",
       "      <td>-0.016081</td>\n",
       "      <td>0.485267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.780254</td>\n",
       "      <td>0.697851</td>\n",
       "      <td>0.841304</td>\n",
       "      <td>0.782022</td>\n",
       "      <td>0.648779</td>\n",
       "      <td>0.391667</td>\n",
       "      <td>-0.303353</td>\n",
       "      <td>1.214718</td>\n",
       "      <td>1.503251</td>\n",
       "      <td>-0.803676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.831884</td>\n",
       "      <td>0.695057</td>\n",
       "      <td>0.870290</td>\n",
       "      <td>0.755133</td>\n",
       "      <td>0.640898</td>\n",
       "      <td>1.108073</td>\n",
       "      <td>-0.428358</td>\n",
       "      <td>1.487497</td>\n",
       "      <td>0.240020</td>\n",
       "      <td>-1.029787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">standard</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">bart</th>\n",
       "      <th>0</th>\n",
       "      <td>0.778261</td>\n",
       "      <td>0.687446</td>\n",
       "      <td>0.658696</td>\n",
       "      <td>0.739870</td>\n",
       "      <td>0.670373</td>\n",
       "      <td>0.364016</td>\n",
       "      <td>-0.768890</td>\n",
       "      <td>-0.503789</td>\n",
       "      <td>-0.477009</td>\n",
       "      <td>-0.184110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.787862</td>\n",
       "      <td>0.695057</td>\n",
       "      <td>0.584783</td>\n",
       "      <td>0.760666</td>\n",
       "      <td>0.698365</td>\n",
       "      <td>0.497243</td>\n",
       "      <td>-0.428358</td>\n",
       "      <td>-1.199375</td>\n",
       "      <td>0.499939</td>\n",
       "      <td>0.619031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.810326</td>\n",
       "      <td>0.692552</td>\n",
       "      <td>0.613768</td>\n",
       "      <td>0.761949</td>\n",
       "      <td>0.698541</td>\n",
       "      <td>0.808942</td>\n",
       "      <td>-0.540432</td>\n",
       "      <td>-0.926596</td>\n",
       "      <td>0.560214</td>\n",
       "      <td>0.624068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.768478</td>\n",
       "      <td>0.653049</td>\n",
       "      <td>0.595652</td>\n",
       "      <td>0.748432</td>\n",
       "      <td>0.692065</td>\n",
       "      <td>0.228276</td>\n",
       "      <td>-2.307747</td>\n",
       "      <td>-1.097083</td>\n",
       "      <td>-0.074811</td>\n",
       "      <td>0.438254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.776812</td>\n",
       "      <td>0.697081</td>\n",
       "      <td>0.585507</td>\n",
       "      <td>0.760685</td>\n",
       "      <td>0.714849</td>\n",
       "      <td>0.343906</td>\n",
       "      <td>-0.337837</td>\n",
       "      <td>-1.192555</td>\n",
       "      <td>0.500848</td>\n",
       "      <td>1.091960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">roberta</th>\n",
       "      <th>0</th>\n",
       "      <td>0.811051</td>\n",
       "      <td>0.694287</td>\n",
       "      <td>0.874638</td>\n",
       "      <td>0.718717</td>\n",
       "      <td>0.579686</td>\n",
       "      <td>0.818997</td>\n",
       "      <td>-0.462843</td>\n",
       "      <td>1.528414</td>\n",
       "      <td>-1.470775</td>\n",
       "      <td>-2.786063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.612138</td>\n",
       "      <td>0.716639</td>\n",
       "      <td>0.876087</td>\n",
       "      <td>0.727172</td>\n",
       "      <td>0.632783</td>\n",
       "      <td>-1.941053</td>\n",
       "      <td>0.537199</td>\n",
       "      <td>1.542053</td>\n",
       "      <td>-1.073578</td>\n",
       "      <td>-1.262614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.777717</td>\n",
       "      <td>0.682821</td>\n",
       "      <td>0.889130</td>\n",
       "      <td>0.740174</td>\n",
       "      <td>0.626287</td>\n",
       "      <td>0.356475</td>\n",
       "      <td>-0.975795</td>\n",
       "      <td>1.664804</td>\n",
       "      <td>-0.462735</td>\n",
       "      <td>-1.448987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.838768</td>\n",
       "      <td>0.726371</td>\n",
       "      <td>0.871739</td>\n",
       "      <td>0.755975</td>\n",
       "      <td>0.696512</td>\n",
       "      <td>1.203594</td>\n",
       "      <td>0.972562</td>\n",
       "      <td>1.501136</td>\n",
       "      <td>0.279567</td>\n",
       "      <td>0.565861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.773913</td>\n",
       "      <td>0.707197</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.708252</td>\n",
       "      <td>0.656211</td>\n",
       "      <td>0.303687</td>\n",
       "      <td>0.114768</td>\n",
       "      <td>1.453400</td>\n",
       "      <td>-1.962431</td>\n",
       "      <td>-0.590438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"20\" valign=\"top\">trained</th>\n",
       "      <th rowspan=\"10\" valign=\"top\">isomers</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">bart</th>\n",
       "      <th>0</th>\n",
       "      <td>0.793478</td>\n",
       "      <td>0.711051</td>\n",
       "      <td>0.672464</td>\n",
       "      <td>0.727166</td>\n",
       "      <td>0.710947</td>\n",
       "      <td>0.575167</td>\n",
       "      <td>0.287189</td>\n",
       "      <td>-0.374219</td>\n",
       "      <td>-1.073850</td>\n",
       "      <td>0.980024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.786413</td>\n",
       "      <td>0.709510</td>\n",
       "      <td>0.644203</td>\n",
       "      <td>0.735412</td>\n",
       "      <td>0.739505</td>\n",
       "      <td>0.477133</td>\n",
       "      <td>0.218220</td>\n",
       "      <td>-0.640178</td>\n",
       "      <td>-0.686471</td>\n",
       "      <td>1.799396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.817210</td>\n",
       "      <td>0.682628</td>\n",
       "      <td>0.731159</td>\n",
       "      <td>0.723954</td>\n",
       "      <td>0.710225</td>\n",
       "      <td>0.904463</td>\n",
       "      <td>-0.984416</td>\n",
       "      <td>0.178159</td>\n",
       "      <td>-1.224765</td>\n",
       "      <td>0.959316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.798913</td>\n",
       "      <td>0.704018</td>\n",
       "      <td>0.582609</td>\n",
       "      <td>0.725277</td>\n",
       "      <td>0.725304</td>\n",
       "      <td>0.650579</td>\n",
       "      <td>-0.027480</td>\n",
       "      <td>-1.219833</td>\n",
       "      <td>-1.162581</td>\n",
       "      <td>1.391949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.769022</td>\n",
       "      <td>0.685230</td>\n",
       "      <td>0.655797</td>\n",
       "      <td>0.707855</td>\n",
       "      <td>0.709133</td>\n",
       "      <td>0.235817</td>\n",
       "      <td>-0.868032</td>\n",
       "      <td>-0.531066</td>\n",
       "      <td>-1.981068</td>\n",
       "      <td>0.927974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">roberta</th>\n",
       "      <th>0</th>\n",
       "      <td>0.803080</td>\n",
       "      <td>0.702573</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.744954</td>\n",
       "      <td>0.684515</td>\n",
       "      <td>0.708394</td>\n",
       "      <td>-0.092137</td>\n",
       "      <td>0.253173</td>\n",
       "      <td>-0.238181</td>\n",
       "      <td>0.221658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.835326</td>\n",
       "      <td>0.710184</td>\n",
       "      <td>0.772464</td>\n",
       "      <td>0.710614</td>\n",
       "      <td>0.724290</td>\n",
       "      <td>1.155834</td>\n",
       "      <td>0.248394</td>\n",
       "      <td>0.566869</td>\n",
       "      <td>-1.851427</td>\n",
       "      <td>1.362845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.777899</td>\n",
       "      <td>0.684941</td>\n",
       "      <td>0.776812</td>\n",
       "      <td>0.728385</td>\n",
       "      <td>0.702891</td>\n",
       "      <td>0.358989</td>\n",
       "      <td>-0.880964</td>\n",
       "      <td>0.607785</td>\n",
       "      <td>-1.016575</td>\n",
       "      <td>0.748876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.814855</td>\n",
       "      <td>0.707390</td>\n",
       "      <td>0.788406</td>\n",
       "      <td>0.742332</td>\n",
       "      <td>0.717853</td>\n",
       "      <td>0.871785</td>\n",
       "      <td>0.123389</td>\n",
       "      <td>0.716897</td>\n",
       "      <td>-0.361368</td>\n",
       "      <td>1.178151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.801087</td>\n",
       "      <td>0.705945</td>\n",
       "      <td>0.861594</td>\n",
       "      <td>0.736910</td>\n",
       "      <td>0.685042</td>\n",
       "      <td>0.680743</td>\n",
       "      <td>0.058731</td>\n",
       "      <td>1.405664</td>\n",
       "      <td>-0.616105</td>\n",
       "      <td>0.236769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">standard</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">bart</th>\n",
       "      <th>0</th>\n",
       "      <td>0.709239</td>\n",
       "      <td>0.716639</td>\n",
       "      <td>0.619565</td>\n",
       "      <td>0.740250</td>\n",
       "      <td>0.668851</td>\n",
       "      <td>-0.593706</td>\n",
       "      <td>0.537199</td>\n",
       "      <td>-0.872040</td>\n",
       "      <td>-0.459190</td>\n",
       "      <td>-0.227765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.683333</td>\n",
       "      <td>0.674246</td>\n",
       "      <td>0.607971</td>\n",
       "      <td>0.739803</td>\n",
       "      <td>0.645482</td>\n",
       "      <td>-0.953166</td>\n",
       "      <td>-1.359432</td>\n",
       "      <td>-0.981152</td>\n",
       "      <td>-0.480191</td>\n",
       "      <td>-0.898262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.656522</td>\n",
       "      <td>0.718759</td>\n",
       "      <td>0.621739</td>\n",
       "      <td>0.744269</td>\n",
       "      <td>0.693684</td>\n",
       "      <td>-1.325194</td>\n",
       "      <td>0.632031</td>\n",
       "      <td>-0.851582</td>\n",
       "      <td>-0.270364</td>\n",
       "      <td>0.484707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.670833</td>\n",
       "      <td>0.702380</td>\n",
       "      <td>0.646377</td>\n",
       "      <td>0.729777</td>\n",
       "      <td>0.679951</td>\n",
       "      <td>-1.126612</td>\n",
       "      <td>-0.100759</td>\n",
       "      <td>-0.619720</td>\n",
       "      <td>-0.951164</td>\n",
       "      <td>0.090693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.697645</td>\n",
       "      <td>0.710473</td>\n",
       "      <td>0.560145</td>\n",
       "      <td>0.745972</td>\n",
       "      <td>0.678293</td>\n",
       "      <td>-0.754583</td>\n",
       "      <td>0.261326</td>\n",
       "      <td>-1.431237</td>\n",
       "      <td>-0.190361</td>\n",
       "      <td>0.043120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">roberta</th>\n",
       "      <th>0</th>\n",
       "      <td>0.810870</td>\n",
       "      <td>0.692456</td>\n",
       "      <td>0.736957</td>\n",
       "      <td>0.718433</td>\n",
       "      <td>0.660815</td>\n",
       "      <td>0.816483</td>\n",
       "      <td>-0.544743</td>\n",
       "      <td>0.232714</td>\n",
       "      <td>-1.484139</td>\n",
       "      <td>-0.458353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.726812</td>\n",
       "      <td>0.690914</td>\n",
       "      <td>0.662319</td>\n",
       "      <td>0.737362</td>\n",
       "      <td>0.677103</td>\n",
       "      <td>-0.349876</td>\n",
       "      <td>-0.613711</td>\n",
       "      <td>-0.469691</td>\n",
       "      <td>-0.594831</td>\n",
       "      <td>0.008979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.647464</td>\n",
       "      <td>0.697177</td>\n",
       "      <td>0.697101</td>\n",
       "      <td>0.723083</td>\n",
       "      <td>0.675289</td>\n",
       "      <td>-1.450880</td>\n",
       "      <td>-0.333527</td>\n",
       "      <td>-0.142356</td>\n",
       "      <td>-1.265676</td>\n",
       "      <td>-0.043071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.635326</td>\n",
       "      <td>0.675017</td>\n",
       "      <td>0.665942</td>\n",
       "      <td>0.726080</td>\n",
       "      <td>0.677786</td>\n",
       "      <td>-1.619298</td>\n",
       "      <td>-1.324948</td>\n",
       "      <td>-0.435594</td>\n",
       "      <td>-1.124852</td>\n",
       "      <td>0.028568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.556703</td>\n",
       "      <td>0.698430</td>\n",
       "      <td>0.684783</td>\n",
       "      <td>0.676097</td>\n",
       "      <td>0.669827</td>\n",
       "      <td>-2.710247</td>\n",
       "      <td>-0.277490</td>\n",
       "      <td>-0.258288</td>\n",
       "      <td>-3.473036</td>\n",
       "      <td>-0.199781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"40\" valign=\"top\">smiles</th>\n",
       "      <th rowspan=\"20\" valign=\"top\">atom</th>\n",
       "      <th rowspan=\"10\" valign=\"top\">isomers</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">bart</th>\n",
       "      <th>0</th>\n",
       "      <td>0.861051</td>\n",
       "      <td>0.722228</td>\n",
       "      <td>0.792754</td>\n",
       "      <td>0.769415</td>\n",
       "      <td>0.697097</td>\n",
       "      <td>1.512780</td>\n",
       "      <td>0.787210</td>\n",
       "      <td>0.757814</td>\n",
       "      <td>0.910955</td>\n",
       "      <td>0.582651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.809420</td>\n",
       "      <td>0.672801</td>\n",
       "      <td>0.755072</td>\n",
       "      <td>0.780685</td>\n",
       "      <td>0.595272</td>\n",
       "      <td>0.796374</td>\n",
       "      <td>-1.424090</td>\n",
       "      <td>0.403201</td>\n",
       "      <td>1.440431</td>\n",
       "      <td>-2.338878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.826630</td>\n",
       "      <td>0.720590</td>\n",
       "      <td>0.755072</td>\n",
       "      <td>0.751164</td>\n",
       "      <td>0.672051</td>\n",
       "      <td>1.035176</td>\n",
       "      <td>0.713931</td>\n",
       "      <td>0.403201</td>\n",
       "      <td>0.053558</td>\n",
       "      <td>-0.135978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.831522</td>\n",
       "      <td>0.659408</td>\n",
       "      <td>0.676812</td>\n",
       "      <td>0.762490</td>\n",
       "      <td>0.673631</td>\n",
       "      <td>1.103046</td>\n",
       "      <td>-2.023253</td>\n",
       "      <td>-0.333302</td>\n",
       "      <td>0.585624</td>\n",
       "      <td>-0.090644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.699489</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.750910</td>\n",
       "      <td>0.657557</td>\n",
       "      <td>0.550030</td>\n",
       "      <td>-0.230074</td>\n",
       "      <td>0.198617</td>\n",
       "      <td>0.041648</td>\n",
       "      <td>-0.551820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">roberta</th>\n",
       "      <th>0</th>\n",
       "      <td>0.616848</td>\n",
       "      <td>0.748145</td>\n",
       "      <td>0.844203</td>\n",
       "      <td>0.728314</td>\n",
       "      <td>0.653929</td>\n",
       "      <td>-1.875696</td>\n",
       "      <td>1.946741</td>\n",
       "      <td>1.241996</td>\n",
       "      <td>-1.019894</td>\n",
       "      <td>-0.655920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.725362</td>\n",
       "      <td>0.754023</td>\n",
       "      <td>0.878261</td>\n",
       "      <td>0.767520</td>\n",
       "      <td>0.648291</td>\n",
       "      <td>-0.369986</td>\n",
       "      <td>2.209683</td>\n",
       "      <td>1.562512</td>\n",
       "      <td>0.821952</td>\n",
       "      <td>-0.817668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.754529</td>\n",
       "      <td>0.734078</td>\n",
       "      <td>0.839855</td>\n",
       "      <td>0.745279</td>\n",
       "      <td>0.611053</td>\n",
       "      <td>0.034721</td>\n",
       "      <td>1.317404</td>\n",
       "      <td>1.201080</td>\n",
       "      <td>-0.222908</td>\n",
       "      <td>-1.886097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.672645</td>\n",
       "      <td>0.760382</td>\n",
       "      <td>0.874638</td>\n",
       "      <td>0.766837</td>\n",
       "      <td>0.633427</td>\n",
       "      <td>-1.101475</td>\n",
       "      <td>2.494177</td>\n",
       "      <td>1.528414</td>\n",
       "      <td>0.789860</td>\n",
       "      <td>-1.244144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.735145</td>\n",
       "      <td>0.733115</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.757090</td>\n",
       "      <td>0.650983</td>\n",
       "      <td>-0.234246</td>\n",
       "      <td>1.274299</td>\n",
       "      <td>0.662341</td>\n",
       "      <td>0.331932</td>\n",
       "      <td>-0.740432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">standard</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">bart</th>\n",
       "      <th>0</th>\n",
       "      <td>0.785145</td>\n",
       "      <td>0.701609</td>\n",
       "      <td>0.579710</td>\n",
       "      <td>0.742491</td>\n",
       "      <td>0.707202</td>\n",
       "      <td>0.459537</td>\n",
       "      <td>-0.135243</td>\n",
       "      <td>-1.247111</td>\n",
       "      <td>-0.353913</td>\n",
       "      <td>0.872566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.822101</td>\n",
       "      <td>0.718759</td>\n",
       "      <td>0.797101</td>\n",
       "      <td>0.752015</td>\n",
       "      <td>0.697546</td>\n",
       "      <td>0.972333</td>\n",
       "      <td>0.632031</td>\n",
       "      <td>0.798731</td>\n",
       "      <td>0.093559</td>\n",
       "      <td>0.595524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.783152</td>\n",
       "      <td>0.689855</td>\n",
       "      <td>0.645652</td>\n",
       "      <td>0.773254</td>\n",
       "      <td>0.674772</td>\n",
       "      <td>0.431886</td>\n",
       "      <td>-0.661127</td>\n",
       "      <td>-0.626539</td>\n",
       "      <td>1.091326</td>\n",
       "      <td>-0.057902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.798188</td>\n",
       "      <td>0.664226</td>\n",
       "      <td>0.842029</td>\n",
       "      <td>0.766756</td>\n",
       "      <td>0.687754</td>\n",
       "      <td>0.640524</td>\n",
       "      <td>-1.807726</td>\n",
       "      <td>1.221538</td>\n",
       "      <td>0.786041</td>\n",
       "      <td>0.314565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.803442</td>\n",
       "      <td>0.685133</td>\n",
       "      <td>0.785507</td>\n",
       "      <td>0.749498</td>\n",
       "      <td>0.668188</td>\n",
       "      <td>0.713421</td>\n",
       "      <td>-0.872342</td>\n",
       "      <td>0.689619</td>\n",
       "      <td>-0.024718</td>\n",
       "      <td>-0.246794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">roberta</th>\n",
       "      <th>0</th>\n",
       "      <td>0.674094</td>\n",
       "      <td>0.743521</td>\n",
       "      <td>0.721739</td>\n",
       "      <td>0.748135</td>\n",
       "      <td>0.682545</td>\n",
       "      <td>-1.081365</td>\n",
       "      <td>1.739836</td>\n",
       "      <td>0.089506</td>\n",
       "      <td>-0.088721</td>\n",
       "      <td>0.165130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.715217</td>\n",
       "      <td>0.733404</td>\n",
       "      <td>0.846377</td>\n",
       "      <td>0.751166</td>\n",
       "      <td>0.655236</td>\n",
       "      <td>-0.510754</td>\n",
       "      <td>1.287230</td>\n",
       "      <td>1.262455</td>\n",
       "      <td>0.053649</td>\n",
       "      <td>-0.618422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.776087</td>\n",
       "      <td>0.716061</td>\n",
       "      <td>0.618841</td>\n",
       "      <td>0.756650</td>\n",
       "      <td>0.676674</td>\n",
       "      <td>0.333852</td>\n",
       "      <td>0.511336</td>\n",
       "      <td>-0.878859</td>\n",
       "      <td>0.311295</td>\n",
       "      <td>-0.003334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.767754</td>\n",
       "      <td>0.697273</td>\n",
       "      <td>0.812319</td>\n",
       "      <td>0.759042</td>\n",
       "      <td>0.684964</td>\n",
       "      <td>0.218221</td>\n",
       "      <td>-0.329216</td>\n",
       "      <td>0.941940</td>\n",
       "      <td>0.423663</td>\n",
       "      <td>0.234530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.748188</td>\n",
       "      <td>0.746122</td>\n",
       "      <td>0.629710</td>\n",
       "      <td>0.752726</td>\n",
       "      <td>0.670666</td>\n",
       "      <td>-0.053259</td>\n",
       "      <td>1.856220</td>\n",
       "      <td>-0.776567</td>\n",
       "      <td>0.126924</td>\n",
       "      <td>-0.175715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"20\" valign=\"top\">trained</th>\n",
       "      <th rowspan=\"10\" valign=\"top\">isomers</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">bart</th>\n",
       "      <th>0</th>\n",
       "      <td>0.793297</td>\n",
       "      <td>0.687831</td>\n",
       "      <td>0.602899</td>\n",
       "      <td>0.772753</td>\n",
       "      <td>0.712878</td>\n",
       "      <td>0.572654</td>\n",
       "      <td>-0.751648</td>\n",
       "      <td>-1.028888</td>\n",
       "      <td>1.067780</td>\n",
       "      <td>1.035432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.793841</td>\n",
       "      <td>0.692360</td>\n",
       "      <td>0.628261</td>\n",
       "      <td>0.765233</td>\n",
       "      <td>0.719257</td>\n",
       "      <td>0.580195</td>\n",
       "      <td>-0.549053</td>\n",
       "      <td>-0.790206</td>\n",
       "      <td>0.714493</td>\n",
       "      <td>1.218448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.740942</td>\n",
       "      <td>0.670296</td>\n",
       "      <td>0.577536</td>\n",
       "      <td>0.780259</td>\n",
       "      <td>0.709036</td>\n",
       "      <td>-0.153807</td>\n",
       "      <td>-1.536163</td>\n",
       "      <td>-1.267569</td>\n",
       "      <td>1.420430</td>\n",
       "      <td>0.925176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.795652</td>\n",
       "      <td>0.692938</td>\n",
       "      <td>0.578986</td>\n",
       "      <td>0.775545</td>\n",
       "      <td>0.634988</td>\n",
       "      <td>0.605332</td>\n",
       "      <td>-0.523190</td>\n",
       "      <td>-1.253930</td>\n",
       "      <td>1.198967</td>\n",
       "      <td>-1.199370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.817029</td>\n",
       "      <td>0.682917</td>\n",
       "      <td>0.668841</td>\n",
       "      <td>0.793749</td>\n",
       "      <td>0.712040</td>\n",
       "      <td>0.901949</td>\n",
       "      <td>-0.971485</td>\n",
       "      <td>-0.408316</td>\n",
       "      <td>2.054182</td>\n",
       "      <td>1.011366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">roberta</th>\n",
       "      <th>0</th>\n",
       "      <td>0.763406</td>\n",
       "      <td>0.725022</td>\n",
       "      <td>0.637681</td>\n",
       "      <td>0.769687</td>\n",
       "      <td>0.626326</td>\n",
       "      <td>0.157892</td>\n",
       "      <td>0.912215</td>\n",
       "      <td>-0.701553</td>\n",
       "      <td>0.923774</td>\n",
       "      <td>-1.447868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.773007</td>\n",
       "      <td>0.723673</td>\n",
       "      <td>0.665942</td>\n",
       "      <td>0.762278</td>\n",
       "      <td>0.726299</td>\n",
       "      <td>0.291119</td>\n",
       "      <td>0.851867</td>\n",
       "      <td>-0.435594</td>\n",
       "      <td>0.575669</td>\n",
       "      <td>1.420492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.774819</td>\n",
       "      <td>0.744869</td>\n",
       "      <td>0.647101</td>\n",
       "      <td>0.760828</td>\n",
       "      <td>0.698346</td>\n",
       "      <td>0.316256</td>\n",
       "      <td>1.800183</td>\n",
       "      <td>-0.612900</td>\n",
       "      <td>0.507576</td>\n",
       "      <td>0.618471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.784783</td>\n",
       "      <td>0.724347</td>\n",
       "      <td>0.618116</td>\n",
       "      <td>0.785457</td>\n",
       "      <td>0.588698</td>\n",
       "      <td>0.454510</td>\n",
       "      <td>0.882041</td>\n",
       "      <td>-0.885679</td>\n",
       "      <td>1.664621</td>\n",
       "      <td>-2.527490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.784058</td>\n",
       "      <td>0.728201</td>\n",
       "      <td>0.652899</td>\n",
       "      <td>0.777554</td>\n",
       "      <td>0.695225</td>\n",
       "      <td>0.444455</td>\n",
       "      <td>1.054462</td>\n",
       "      <td>-0.558344</td>\n",
       "      <td>1.293334</td>\n",
       "      <td>0.528922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">standard</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">bart</th>\n",
       "      <th>0</th>\n",
       "      <td>0.667391</td>\n",
       "      <td>0.717314</td>\n",
       "      <td>0.558696</td>\n",
       "      <td>0.770328</td>\n",
       "      <td>0.688514</td>\n",
       "      <td>-1.174372</td>\n",
       "      <td>0.567373</td>\n",
       "      <td>-1.444876</td>\n",
       "      <td>0.953866</td>\n",
       "      <td>0.336392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.834783</td>\n",
       "      <td>0.709606</td>\n",
       "      <td>0.692754</td>\n",
       "      <td>0.725088</td>\n",
       "      <td>0.700492</td>\n",
       "      <td>1.148292</td>\n",
       "      <td>0.222531</td>\n",
       "      <td>-0.183273</td>\n",
       "      <td>-1.171491</td>\n",
       "      <td>0.680036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.629167</td>\n",
       "      <td>0.719337</td>\n",
       "      <td>0.595652</td>\n",
       "      <td>0.782834</td>\n",
       "      <td>0.688904</td>\n",
       "      <td>-1.704764</td>\n",
       "      <td>0.657894</td>\n",
       "      <td>-1.097083</td>\n",
       "      <td>1.541389</td>\n",
       "      <td>0.347586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.644746</td>\n",
       "      <td>0.707872</td>\n",
       "      <td>0.617391</td>\n",
       "      <td>0.745362</td>\n",
       "      <td>0.678429</td>\n",
       "      <td>-1.488585</td>\n",
       "      <td>0.144941</td>\n",
       "      <td>-0.892498</td>\n",
       "      <td>-0.218998</td>\n",
       "      <td>0.047038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.811232</td>\n",
       "      <td>0.715772</td>\n",
       "      <td>0.502174</td>\n",
       "      <td>0.771946</td>\n",
       "      <td>0.723002</td>\n",
       "      <td>0.821511</td>\n",
       "      <td>0.498404</td>\n",
       "      <td>-1.976794</td>\n",
       "      <td>1.029869</td>\n",
       "      <td>1.325906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">roberta</th>\n",
       "      <th>0</th>\n",
       "      <td>0.535326</td>\n",
       "      <td>0.687928</td>\n",
       "      <td>0.652899</td>\n",
       "      <td>0.748555</td>\n",
       "      <td>0.681375</td>\n",
       "      <td>-3.006864</td>\n",
       "      <td>-0.747337</td>\n",
       "      <td>-0.558344</td>\n",
       "      <td>-0.068992</td>\n",
       "      <td>0.131549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.647645</td>\n",
       "      <td>0.704789</td>\n",
       "      <td>0.687681</td>\n",
       "      <td>0.750758</td>\n",
       "      <td>0.699263</td>\n",
       "      <td>-1.448366</td>\n",
       "      <td>0.007005</td>\n",
       "      <td>-0.231010</td>\n",
       "      <td>0.034466</td>\n",
       "      <td>0.644776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.626268</td>\n",
       "      <td>0.715676</td>\n",
       "      <td>0.659420</td>\n",
       "      <td>0.741391</td>\n",
       "      <td>0.704451</td>\n",
       "      <td>-1.744983</td>\n",
       "      <td>0.494094</td>\n",
       "      <td>-0.496969</td>\n",
       "      <td>-0.405551</td>\n",
       "      <td>0.793651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.609058</td>\n",
       "      <td>0.689565</td>\n",
       "      <td>0.727536</td>\n",
       "      <td>0.749436</td>\n",
       "      <td>0.700355</td>\n",
       "      <td>-1.983786</td>\n",
       "      <td>-0.674058</td>\n",
       "      <td>0.144061</td>\n",
       "      <td>-0.027627</td>\n",
       "      <td>0.676118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.643297</td>\n",
       "      <td>0.687831</td>\n",
       "      <td>0.602899</td>\n",
       "      <td>0.760251</td>\n",
       "      <td>0.706948</td>\n",
       "      <td>-1.508695</td>\n",
       "      <td>-0.751648</td>\n",
       "      <td>-1.028888</td>\n",
       "      <td>0.480438</td>\n",
       "      <td>0.865290</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                           ROC_AUC            \\\n",
       "task                                           bace_classification      bbbp   \n",
       "embedding tokenizer dataset  architecture seed                                 \n",
       "selfies   atom      isomers  bart         0               0.766486  0.682243   \n",
       "                                          1               0.776812  0.712111   \n",
       "                                          2               0.754167  0.713749   \n",
       "                                          3               0.765399  0.657289   \n",
       "                                          4               0.783514  0.739570   \n",
       "                             roberta      0               0.807790  0.698430   \n",
       "                                          1               0.799094  0.694672   \n",
       "                                          2               0.801449  0.718759   \n",
       "                                          3               0.780254  0.697851   \n",
       "                                          4               0.831884  0.695057   \n",
       "                    standard bart         0               0.778261  0.687446   \n",
       "                                          1               0.787862  0.695057   \n",
       "                                          2               0.810326  0.692552   \n",
       "                                          3               0.768478  0.653049   \n",
       "                                          4               0.776812  0.697081   \n",
       "                             roberta      0               0.811051  0.694287   \n",
       "                                          1               0.612138  0.716639   \n",
       "                                          2               0.777717  0.682821   \n",
       "                                          3               0.838768  0.726371   \n",
       "                                          4               0.773913  0.707197   \n",
       "          trained   isomers  bart         0               0.793478  0.711051   \n",
       "                                          1               0.786413  0.709510   \n",
       "                                          2               0.817210  0.682628   \n",
       "                                          3               0.798913  0.704018   \n",
       "                                          4               0.769022  0.685230   \n",
       "                             roberta      0               0.803080  0.702573   \n",
       "                                          1               0.835326  0.710184   \n",
       "                                          2               0.777899  0.684941   \n",
       "                                          3               0.814855  0.707390   \n",
       "                                          4               0.801087  0.705945   \n",
       "                    standard bart         0               0.709239  0.716639   \n",
       "                                          1               0.683333  0.674246   \n",
       "                                          2               0.656522  0.718759   \n",
       "                                          3               0.670833  0.702380   \n",
       "                                          4               0.697645  0.710473   \n",
       "                             roberta      0               0.810870  0.692456   \n",
       "                                          1               0.726812  0.690914   \n",
       "                                          2               0.647464  0.697177   \n",
       "                                          3               0.635326  0.675017   \n",
       "                                          4               0.556703  0.698430   \n",
       "smiles    atom      isomers  bart         0               0.861051  0.722228   \n",
       "                                          1               0.809420  0.672801   \n",
       "                                          2               0.826630  0.720590   \n",
       "                                          3               0.831522  0.659408   \n",
       "                                          4               0.791667  0.699489   \n",
       "                             roberta      0               0.616848  0.748145   \n",
       "                                          1               0.725362  0.754023   \n",
       "                                          2               0.754529  0.734078   \n",
       "                                          3               0.672645  0.760382   \n",
       "                                          4               0.735145  0.733115   \n",
       "                    standard bart         0               0.785145  0.701609   \n",
       "                                          1               0.822101  0.718759   \n",
       "                                          2               0.783152  0.689855   \n",
       "                                          3               0.798188  0.664226   \n",
       "                                          4               0.803442  0.685133   \n",
       "                             roberta      0               0.674094  0.743521   \n",
       "                                          1               0.715217  0.733404   \n",
       "                                          2               0.776087  0.716061   \n",
       "                                          3               0.767754  0.697273   \n",
       "                                          4               0.748188  0.746122   \n",
       "          trained   isomers  bart         0               0.793297  0.687831   \n",
       "                                          1               0.793841  0.692360   \n",
       "                                          2               0.740942  0.670296   \n",
       "                                          3               0.795652  0.692938   \n",
       "                                          4               0.817029  0.682917   \n",
       "                             roberta      0               0.763406  0.725022   \n",
       "                                          1               0.773007  0.723673   \n",
       "                                          2               0.774819  0.744869   \n",
       "                                          3               0.784783  0.724347   \n",
       "                                          4               0.784058  0.728201   \n",
       "                    standard bart         0               0.667391  0.717314   \n",
       "                                          1               0.834783  0.709606   \n",
       "                                          2               0.629167  0.719337   \n",
       "                                          3               0.644746  0.707872   \n",
       "                                          4               0.811232  0.715772   \n",
       "                             roberta      0               0.535326  0.687928   \n",
       "                                          1               0.647645  0.704789   \n",
       "                                          2               0.626268  0.715676   \n",
       "                                          3               0.609058  0.689565   \n",
       "                                          4               0.643297  0.687831   \n",
       "\n",
       "                                                                              \\\n",
       "task                                             clintox       hiv     tox21   \n",
       "embedding tokenizer dataset  architecture seed                                 \n",
       "selfies   atom      isomers  bart         0     0.818841  0.743073  0.690075   \n",
       "                                          1     0.856522  0.755044  0.676225   \n",
       "                                          2     0.650725  0.775503  0.675523   \n",
       "                                          3     0.663768  0.721728  0.690485   \n",
       "                                          4     0.659420  0.770910  0.622211   \n",
       "                             roberta      0     0.918116  0.776729  0.637543   \n",
       "                                          1     0.902174  0.765490  0.571629   \n",
       "                                          2     0.865942  0.749682  0.693703   \n",
       "                                          3     0.841304  0.782022  0.648779   \n",
       "                                          4     0.870290  0.755133  0.640898   \n",
       "                    standard bart         0     0.658696  0.739870  0.670373   \n",
       "                                          1     0.584783  0.760666  0.698365   \n",
       "                                          2     0.613768  0.761949  0.698541   \n",
       "                                          3     0.595652  0.748432  0.692065   \n",
       "                                          4     0.585507  0.760685  0.714849   \n",
       "                             roberta      0     0.874638  0.718717  0.579686   \n",
       "                                          1     0.876087  0.727172  0.632783   \n",
       "                                          2     0.889130  0.740174  0.626287   \n",
       "                                          3     0.871739  0.755975  0.696512   \n",
       "                                          4     0.866667  0.708252  0.656211   \n",
       "          trained   isomers  bart         0     0.672464  0.727166  0.710947   \n",
       "                                          1     0.644203  0.735412  0.739505   \n",
       "                                          2     0.731159  0.723954  0.710225   \n",
       "                                          3     0.582609  0.725277  0.725304   \n",
       "                                          4     0.655797  0.707855  0.709133   \n",
       "                             roberta      0     0.739130  0.744954  0.684515   \n",
       "                                          1     0.772464  0.710614  0.724290   \n",
       "                                          2     0.776812  0.728385  0.702891   \n",
       "                                          3     0.788406  0.742332  0.717853   \n",
       "                                          4     0.861594  0.736910  0.685042   \n",
       "                    standard bart         0     0.619565  0.740250  0.668851   \n",
       "                                          1     0.607971  0.739803  0.645482   \n",
       "                                          2     0.621739  0.744269  0.693684   \n",
       "                                          3     0.646377  0.729777  0.679951   \n",
       "                                          4     0.560145  0.745972  0.678293   \n",
       "                             roberta      0     0.736957  0.718433  0.660815   \n",
       "                                          1     0.662319  0.737362  0.677103   \n",
       "                                          2     0.697101  0.723083  0.675289   \n",
       "                                          3     0.665942  0.726080  0.677786   \n",
       "                                          4     0.684783  0.676097  0.669827   \n",
       "smiles    atom      isomers  bart         0     0.792754  0.769415  0.697097   \n",
       "                                          1     0.755072  0.780685  0.595272   \n",
       "                                          2     0.755072  0.751164  0.672051   \n",
       "                                          3     0.676812  0.762490  0.673631   \n",
       "                                          4     0.733333  0.750910  0.657557   \n",
       "                             roberta      0     0.844203  0.728314  0.653929   \n",
       "                                          1     0.878261  0.767520  0.648291   \n",
       "                                          2     0.839855  0.745279  0.611053   \n",
       "                                          3     0.874638  0.766837  0.633427   \n",
       "                                          4     0.782609  0.757090  0.650983   \n",
       "                    standard bart         0     0.579710  0.742491  0.707202   \n",
       "                                          1     0.797101  0.752015  0.697546   \n",
       "                                          2     0.645652  0.773254  0.674772   \n",
       "                                          3     0.842029  0.766756  0.687754   \n",
       "                                          4     0.785507  0.749498  0.668188   \n",
       "                             roberta      0     0.721739  0.748135  0.682545   \n",
       "                                          1     0.846377  0.751166  0.655236   \n",
       "                                          2     0.618841  0.756650  0.676674   \n",
       "                                          3     0.812319  0.759042  0.684964   \n",
       "                                          4     0.629710  0.752726  0.670666   \n",
       "          trained   isomers  bart         0     0.602899  0.772753  0.712878   \n",
       "                                          1     0.628261  0.765233  0.719257   \n",
       "                                          2     0.577536  0.780259  0.709036   \n",
       "                                          3     0.578986  0.775545  0.634988   \n",
       "                                          4     0.668841  0.793749  0.712040   \n",
       "                             roberta      0     0.637681  0.769687  0.626326   \n",
       "                                          1     0.665942  0.762278  0.726299   \n",
       "                                          2     0.647101  0.760828  0.698346   \n",
       "                                          3     0.618116  0.785457  0.588698   \n",
       "                                          4     0.652899  0.777554  0.695225   \n",
       "                    standard bart         0     0.558696  0.770328  0.688514   \n",
       "                                          1     0.692754  0.725088  0.700492   \n",
       "                                          2     0.595652  0.782834  0.688904   \n",
       "                                          3     0.617391  0.745362  0.678429   \n",
       "                                          4     0.502174  0.771946  0.723002   \n",
       "                             roberta      0     0.652899  0.748555  0.681375   \n",
       "                                          1     0.687681  0.750758  0.699263   \n",
       "                                          2     0.659420  0.741391  0.704451   \n",
       "                                          3     0.727536  0.749436  0.700355   \n",
       "                                          4     0.602899  0.760251  0.706948   \n",
       "\n",
       "                                               z_ROC_AUC_bace_classification  \\\n",
       "task                                                                           \n",
       "embedding tokenizer dataset  architecture seed                                 \n",
       "selfies   atom      isomers  bart         0                         0.200625   \n",
       "                                          1                         0.343906   \n",
       "                                          2                         0.029693   \n",
       "                                          3                         0.185543   \n",
       "                                          4                         0.436914   \n",
       "                             roberta      0                         0.773750   \n",
       "                                          1                         0.653092   \n",
       "                                          2                         0.685770   \n",
       "                                          3                         0.391667   \n",
       "                                          4                         1.108073   \n",
       "                    standard bart         0                         0.364016   \n",
       "                                          1                         0.497243   \n",
       "                                          2                         0.808942   \n",
       "                                          3                         0.228276   \n",
       "                                          4                         0.343906   \n",
       "                             roberta      0                         0.818997   \n",
       "                                          1                        -1.941053   \n",
       "                                          2                         0.356475   \n",
       "                                          3                         1.203594   \n",
       "                                          4                         0.303687   \n",
       "          trained   isomers  bart         0                         0.575167   \n",
       "                                          1                         0.477133   \n",
       "                                          2                         0.904463   \n",
       "                                          3                         0.650579   \n",
       "                                          4                         0.235817   \n",
       "                             roberta      0                         0.708394   \n",
       "                                          1                         1.155834   \n",
       "                                          2                         0.358989   \n",
       "                                          3                         0.871785   \n",
       "                                          4                         0.680743   \n",
       "                    standard bart         0                        -0.593706   \n",
       "                                          1                        -0.953166   \n",
       "                                          2                        -1.325194   \n",
       "                                          3                        -1.126612   \n",
       "                                          4                        -0.754583   \n",
       "                             roberta      0                         0.816483   \n",
       "                                          1                        -0.349876   \n",
       "                                          2                        -1.450880   \n",
       "                                          3                        -1.619298   \n",
       "                                          4                        -2.710247   \n",
       "smiles    atom      isomers  bart         0                         1.512780   \n",
       "                                          1                         0.796374   \n",
       "                                          2                         1.035176   \n",
       "                                          3                         1.103046   \n",
       "                                          4                         0.550030   \n",
       "                             roberta      0                        -1.875696   \n",
       "                                          1                        -0.369986   \n",
       "                                          2                         0.034721   \n",
       "                                          3                        -1.101475   \n",
       "                                          4                        -0.234246   \n",
       "                    standard bart         0                         0.459537   \n",
       "                                          1                         0.972333   \n",
       "                                          2                         0.431886   \n",
       "                                          3                         0.640524   \n",
       "                                          4                         0.713421   \n",
       "                             roberta      0                        -1.081365   \n",
       "                                          1                        -0.510754   \n",
       "                                          2                         0.333852   \n",
       "                                          3                         0.218221   \n",
       "                                          4                        -0.053259   \n",
       "          trained   isomers  bart         0                         0.572654   \n",
       "                                          1                         0.580195   \n",
       "                                          2                        -0.153807   \n",
       "                                          3                         0.605332   \n",
       "                                          4                         0.901949   \n",
       "                             roberta      0                         0.157892   \n",
       "                                          1                         0.291119   \n",
       "                                          2                         0.316256   \n",
       "                                          3                         0.454510   \n",
       "                                          4                         0.444455   \n",
       "                    standard bart         0                        -1.174372   \n",
       "                                          1                         1.148292   \n",
       "                                          2                        -1.704764   \n",
       "                                          3                        -1.488585   \n",
       "                                          4                         0.821511   \n",
       "                             roberta      0                        -3.006864   \n",
       "                                          1                        -1.448366   \n",
       "                                          2                        -1.744983   \n",
       "                                          3                        -1.983786   \n",
       "                                          4                        -1.508695   \n",
       "\n",
       "                                               z_ROC_AUC_bbbp  \\\n",
       "task                                                            \n",
       "embedding tokenizer dataset  architecture seed                  \n",
       "selfies   atom      isomers  bart         0         -1.001658   \n",
       "                                          1          0.334604   \n",
       "                                          2          0.407883   \n",
       "                                          3         -2.118084   \n",
       "                                          4          1.563104   \n",
       "                             roberta      0         -0.277490   \n",
       "                                          1         -0.445601   \n",
       "                                          2          0.632031   \n",
       "                                          3         -0.303353   \n",
       "                                          4         -0.428358   \n",
       "                    standard bart         0         -0.768890   \n",
       "                                          1         -0.428358   \n",
       "                                          2         -0.540432   \n",
       "                                          3         -2.307747   \n",
       "                                          4         -0.337837   \n",
       "                             roberta      0         -0.462843   \n",
       "                                          1          0.537199   \n",
       "                                          2         -0.975795   \n",
       "                                          3          0.972562   \n",
       "                                          4          0.114768   \n",
       "          trained   isomers  bart         0          0.287189   \n",
       "                                          1          0.218220   \n",
       "                                          2         -0.984416   \n",
       "                                          3         -0.027480   \n",
       "                                          4         -0.868032   \n",
       "                             roberta      0         -0.092137   \n",
       "                                          1          0.248394   \n",
       "                                          2         -0.880964   \n",
       "                                          3          0.123389   \n",
       "                                          4          0.058731   \n",
       "                    standard bart         0          0.537199   \n",
       "                                          1         -1.359432   \n",
       "                                          2          0.632031   \n",
       "                                          3         -0.100759   \n",
       "                                          4          0.261326   \n",
       "                             roberta      0         -0.544743   \n",
       "                                          1         -0.613711   \n",
       "                                          2         -0.333527   \n",
       "                                          3         -1.324948   \n",
       "                                          4         -0.277490   \n",
       "smiles    atom      isomers  bart         0          0.787210   \n",
       "                                          1         -1.424090   \n",
       "                                          2          0.713931   \n",
       "                                          3         -2.023253   \n",
       "                                          4         -0.230074   \n",
       "                             roberta      0          1.946741   \n",
       "                                          1          2.209683   \n",
       "                                          2          1.317404   \n",
       "                                          3          2.494177   \n",
       "                                          4          1.274299   \n",
       "                    standard bart         0         -0.135243   \n",
       "                                          1          0.632031   \n",
       "                                          2         -0.661127   \n",
       "                                          3         -1.807726   \n",
       "                                          4         -0.872342   \n",
       "                             roberta      0          1.739836   \n",
       "                                          1          1.287230   \n",
       "                                          2          0.511336   \n",
       "                                          3         -0.329216   \n",
       "                                          4          1.856220   \n",
       "          trained   isomers  bart         0         -0.751648   \n",
       "                                          1         -0.549053   \n",
       "                                          2         -1.536163   \n",
       "                                          3         -0.523190   \n",
       "                                          4         -0.971485   \n",
       "                             roberta      0          0.912215   \n",
       "                                          1          0.851867   \n",
       "                                          2          1.800183   \n",
       "                                          3          0.882041   \n",
       "                                          4          1.054462   \n",
       "                    standard bart         0          0.567373   \n",
       "                                          1          0.222531   \n",
       "                                          2          0.657894   \n",
       "                                          3          0.144941   \n",
       "                                          4          0.498404   \n",
       "                             roberta      0         -0.747337   \n",
       "                                          1          0.007005   \n",
       "                                          2          0.494094   \n",
       "                                          3         -0.674058   \n",
       "                                          4         -0.751648   \n",
       "\n",
       "                                               z_ROC_AUC_clintox  \\\n",
       "task                                                               \n",
       "embedding tokenizer dataset  architecture seed                     \n",
       "selfies   atom      isomers  bart         0             1.003315   \n",
       "                                          1             1.357927   \n",
       "                                          2            -0.578803   \n",
       "                                          3            -0.456052   \n",
       "                                          4            -0.496969   \n",
       "                             roberta      0             1.937583   \n",
       "                                          1             1.787554   \n",
       "                                          2             1.446581   \n",
       "                                          3             1.214718   \n",
       "                                          4             1.487497   \n",
       "                    standard bart         0            -0.503789   \n",
       "                                          1            -1.199375   \n",
       "                                          2            -0.926596   \n",
       "                                          3            -1.097083   \n",
       "                                          4            -1.192555   \n",
       "                             roberta      0             1.528414   \n",
       "                                          1             1.542053   \n",
       "                                          2             1.664804   \n",
       "                                          3             1.501136   \n",
       "                                          4             1.453400   \n",
       "          trained   isomers  bart         0            -0.374219   \n",
       "                                          1            -0.640178   \n",
       "                                          2             0.178159   \n",
       "                                          3            -1.219833   \n",
       "                                          4            -0.531066   \n",
       "                             roberta      0             0.253173   \n",
       "                                          1             0.566869   \n",
       "                                          2             0.607785   \n",
       "                                          3             0.716897   \n",
       "                                          4             1.405664   \n",
       "                    standard bart         0            -0.872040   \n",
       "                                          1            -0.981152   \n",
       "                                          2            -0.851582   \n",
       "                                          3            -0.619720   \n",
       "                                          4            -1.431237   \n",
       "                             roberta      0             0.232714   \n",
       "                                          1            -0.469691   \n",
       "                                          2            -0.142356   \n",
       "                                          3            -0.435594   \n",
       "                                          4            -0.258288   \n",
       "smiles    atom      isomers  bart         0             0.757814   \n",
       "                                          1             0.403201   \n",
       "                                          2             0.403201   \n",
       "                                          3            -0.333302   \n",
       "                                          4             0.198617   \n",
       "                             roberta      0             1.241996   \n",
       "                                          1             1.562512   \n",
       "                                          2             1.201080   \n",
       "                                          3             1.528414   \n",
       "                                          4             0.662341   \n",
       "                    standard bart         0            -1.247111   \n",
       "                                          1             0.798731   \n",
       "                                          2            -0.626539   \n",
       "                                          3             1.221538   \n",
       "                                          4             0.689619   \n",
       "                             roberta      0             0.089506   \n",
       "                                          1             1.262455   \n",
       "                                          2            -0.878859   \n",
       "                                          3             0.941940   \n",
       "                                          4            -0.776567   \n",
       "          trained   isomers  bart         0            -1.028888   \n",
       "                                          1            -0.790206   \n",
       "                                          2            -1.267569   \n",
       "                                          3            -1.253930   \n",
       "                                          4            -0.408316   \n",
       "                             roberta      0            -0.701553   \n",
       "                                          1            -0.435594   \n",
       "                                          2            -0.612900   \n",
       "                                          3            -0.885679   \n",
       "                                          4            -0.558344   \n",
       "                    standard bart         0            -1.444876   \n",
       "                                          1            -0.183273   \n",
       "                                          2            -1.097083   \n",
       "                                          3            -0.892498   \n",
       "                                          4            -1.976794   \n",
       "                             roberta      0            -0.558344   \n",
       "                                          1            -0.231010   \n",
       "                                          2            -0.496969   \n",
       "                                          3             0.144061   \n",
       "                                          4            -1.028888   \n",
       "\n",
       "                                               z_ROC_AUC_hiv z_ROC_AUC_tox21  \n",
       "task                                                                          \n",
       "embedding tokenizer dataset  architecture seed                                \n",
       "selfies   atom      isomers  bart         0        -0.326548        0.381167  \n",
       "                                          1         0.235838       -0.016206  \n",
       "                                          2         1.196967       -0.036355  \n",
       "                                          3        -1.329315        0.392920  \n",
       "                                          4         0.981231       -1.565960  \n",
       "                             roberta      0         1.254560       -1.126052  \n",
       "                                          1         0.726584       -3.017210  \n",
       "                                          2        -0.016081        0.485267  \n",
       "                                          3         1.503251       -0.803676  \n",
       "                                          4         0.240020       -1.029787  \n",
       "                    standard bart         0        -0.477009       -0.184110  \n",
       "                                          1         0.499939        0.619031  \n",
       "                                          2         0.560214        0.624068  \n",
       "                                          3        -0.074811        0.438254  \n",
       "                                          4         0.500848        1.091960  \n",
       "                             roberta      0        -1.470775       -2.786063  \n",
       "                                          1        -1.073578       -1.262614  \n",
       "                                          2        -0.462735       -1.448987  \n",
       "                                          3         0.279567        0.565861  \n",
       "                                          4        -1.962431       -0.590438  \n",
       "          trained   isomers  bart         0        -1.073850        0.980024  \n",
       "                                          1        -0.686471        1.799396  \n",
       "                                          2        -1.224765        0.959316  \n",
       "                                          3        -1.162581        1.391949  \n",
       "                                          4        -1.981068        0.927974  \n",
       "                             roberta      0        -0.238181        0.221658  \n",
       "                                          1        -1.851427        1.362845  \n",
       "                                          2        -1.016575        0.748876  \n",
       "                                          3        -0.361368        1.178151  \n",
       "                                          4        -0.616105        0.236769  \n",
       "                    standard bart         0        -0.459190       -0.227765  \n",
       "                                          1        -0.480191       -0.898262  \n",
       "                                          2        -0.270364        0.484707  \n",
       "                                          3        -0.951164        0.090693  \n",
       "                                          4        -0.190361        0.043120  \n",
       "                             roberta      0        -1.484139       -0.458353  \n",
       "                                          1        -0.594831        0.008979  \n",
       "                                          2        -1.265676       -0.043071  \n",
       "                                          3        -1.124852        0.028568  \n",
       "                                          4        -3.473036       -0.199781  \n",
       "smiles    atom      isomers  bart         0         0.910955        0.582651  \n",
       "                                          1         1.440431       -2.338878  \n",
       "                                          2         0.053558       -0.135978  \n",
       "                                          3         0.585624       -0.090644  \n",
       "                                          4         0.041648       -0.551820  \n",
       "                             roberta      0        -1.019894       -0.655920  \n",
       "                                          1         0.821952       -0.817668  \n",
       "                                          2        -0.222908       -1.886097  \n",
       "                                          3         0.789860       -1.244144  \n",
       "                                          4         0.331932       -0.740432  \n",
       "                    standard bart         0        -0.353913        0.872566  \n",
       "                                          1         0.093559        0.595524  \n",
       "                                          2         1.091326       -0.057902  \n",
       "                                          3         0.786041        0.314565  \n",
       "                                          4        -0.024718       -0.246794  \n",
       "                             roberta      0        -0.088721        0.165130  \n",
       "                                          1         0.053649       -0.618422  \n",
       "                                          2         0.311295       -0.003334  \n",
       "                                          3         0.423663        0.234530  \n",
       "                                          4         0.126924       -0.175715  \n",
       "          trained   isomers  bart         0         1.067780        1.035432  \n",
       "                                          1         0.714493        1.218448  \n",
       "                                          2         1.420430        0.925176  \n",
       "                                          3         1.198967       -1.199370  \n",
       "                                          4         2.054182        1.011366  \n",
       "                             roberta      0         0.923774       -1.447868  \n",
       "                                          1         0.575669        1.420492  \n",
       "                                          2         0.507576        0.618471  \n",
       "                                          3         1.664621       -2.527490  \n",
       "                                          4         1.293334        0.528922  \n",
       "                    standard bart         0         0.953866        0.336392  \n",
       "                                          1        -1.171491        0.680036  \n",
       "                                          2         1.541389        0.347586  \n",
       "                                          3        -0.218998        0.047038  \n",
       "                                          4         1.029869        1.325906  \n",
       "                             roberta      0        -0.068992        0.131549  \n",
       "                                          1         0.034466        0.644776  \n",
       "                                          2        -0.405551        0.793651  \n",
       "                                          3        -0.027627        0.676118  \n",
       "                                          4         0.480438        0.865290  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chemberta_best_roc = {\"BACE_classificiation\": 0.799, \"BBBP\": 0.742, \"ClinTox\":0.601, \"Tox21\":0.834}\n",
    "classification_metrics = [\"ROC_AUC\"]\n",
    "classification_pivot = classification_scores.pivot_table(values=classification_metrics, index=[\"embedding\",\"tokenizer\",\"dataset\",\"architecture\", \"seed\"],columns=[\"task\"])\n",
    "for metric in classification_metrics:\n",
    "    for task in classification_scores.task.unique():\n",
    "        classification_pivot[f\"z_{metric}_{task}\"] = compute_zscore(classification_pivot[metric][task])\n",
    "classification_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e707f0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>z_ROC_AUC_bace_classification</th>\n",
       "      <th>z_ROC_AUC_bbbp</th>\n",
       "      <th>z_ROC_AUC_clintox</th>\n",
       "      <th>z_ROC_AUC_hiv</th>\n",
       "      <th>z_ROC_AUC_tox21</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>task</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embedding</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>dataset</th>\n",
       "      <th>architecture</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">selfies</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">atom</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">isomers</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.239336</td>\n",
       "      <td>-0.162830</td>\n",
       "      <td>0.165884</td>\n",
       "      <td>0.151634</td>\n",
       "      <td>-0.168887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.722471</td>\n",
       "      <td>-0.164554</td>\n",
       "      <td>1.574787</td>\n",
       "      <td>0.741667</td>\n",
       "      <td>-1.098292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">standard</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.448477</td>\n",
       "      <td>-0.876653</td>\n",
       "      <td>-0.983879</td>\n",
       "      <td>0.201836</td>\n",
       "      <td>0.517840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.148340</td>\n",
       "      <td>0.037178</td>\n",
       "      <td>1.537961</td>\n",
       "      <td>-0.937990</td>\n",
       "      <td>-1.104448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">trained</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">isomers</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.568632</td>\n",
       "      <td>-0.274904</td>\n",
       "      <td>-0.517427</td>\n",
       "      <td>-1.225747</td>\n",
       "      <td>1.211732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.755149</td>\n",
       "      <td>-0.108517</td>\n",
       "      <td>0.710078</td>\n",
       "      <td>-0.816731</td>\n",
       "      <td>0.749660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">standard</th>\n",
       "      <th>bart</th>\n",
       "      <td>-0.950652</td>\n",
       "      <td>-0.005927</td>\n",
       "      <td>-0.951146</td>\n",
       "      <td>-0.470254</td>\n",
       "      <td>-0.101501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>-1.062764</td>\n",
       "      <td>-0.618884</td>\n",
       "      <td>-0.214643</td>\n",
       "      <td>-1.588507</td>\n",
       "      <td>-0.132732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">smiles</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">atom</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">isomers</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.999481</td>\n",
       "      <td>-0.435255</td>\n",
       "      <td>0.285906</td>\n",
       "      <td>0.606443</td>\n",
       "      <td>-0.506934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>-0.709336</td>\n",
       "      <td>1.848461</td>\n",
       "      <td>1.239269</td>\n",
       "      <td>0.140188</td>\n",
       "      <td>-1.068852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">standard</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.643540</td>\n",
       "      <td>-0.568882</td>\n",
       "      <td>0.167248</td>\n",
       "      <td>0.318459</td>\n",
       "      <td>0.295591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>-0.218661</td>\n",
       "      <td>1.013081</td>\n",
       "      <td>0.127695</td>\n",
       "      <td>0.165362</td>\n",
       "      <td>-0.079562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">trained</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">isomers</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.501264</td>\n",
       "      <td>-0.866308</td>\n",
       "      <td>-0.949782</td>\n",
       "      <td>1.291170</td>\n",
       "      <td>0.598210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.332846</td>\n",
       "      <td>1.100154</td>\n",
       "      <td>-0.638814</td>\n",
       "      <td>0.992995</td>\n",
       "      <td>-0.281495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">standard</th>\n",
       "      <th>bart</th>\n",
       "      <td>-0.479584</td>\n",
       "      <td>0.418229</td>\n",
       "      <td>-1.118905</td>\n",
       "      <td>0.426927</td>\n",
       "      <td>0.547392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>-1.938539</td>\n",
       "      <td>-0.334389</td>\n",
       "      <td>-0.434230</td>\n",
       "      <td>0.002547</td>\n",
       "      <td>0.622277</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          z_ROC_AUC_bace_classification  \\\n",
       "task                                                                      \n",
       "embedding tokenizer dataset  architecture                                 \n",
       "selfies   atom      isomers  bart                              0.239336   \n",
       "                             roberta                           0.722471   \n",
       "                    standard bart                              0.448477   \n",
       "                             roberta                           0.148340   \n",
       "          trained   isomers  bart                              0.568632   \n",
       "                             roberta                           0.755149   \n",
       "                    standard bart                             -0.950652   \n",
       "                             roberta                          -1.062764   \n",
       "smiles    atom      isomers  bart                              0.999481   \n",
       "                             roberta                          -0.709336   \n",
       "                    standard bart                              0.643540   \n",
       "                             roberta                          -0.218661   \n",
       "          trained   isomers  bart                              0.501264   \n",
       "                             roberta                           0.332846   \n",
       "                    standard bart                             -0.479584   \n",
       "                             roberta                          -1.938539   \n",
       "\n",
       "                                          z_ROC_AUC_bbbp z_ROC_AUC_clintox  \\\n",
       "task                                                                         \n",
       "embedding tokenizer dataset  architecture                                    \n",
       "selfies   atom      isomers  bart              -0.162830          0.165884   \n",
       "                             roberta           -0.164554          1.574787   \n",
       "                    standard bart              -0.876653         -0.983879   \n",
       "                             roberta            0.037178          1.537961   \n",
       "          trained   isomers  bart              -0.274904         -0.517427   \n",
       "                             roberta           -0.108517          0.710078   \n",
       "                    standard bart              -0.005927         -0.951146   \n",
       "                             roberta           -0.618884         -0.214643   \n",
       "smiles    atom      isomers  bart              -0.435255          0.285906   \n",
       "                             roberta            1.848461          1.239269   \n",
       "                    standard bart              -0.568882          0.167248   \n",
       "                             roberta            1.013081          0.127695   \n",
       "          trained   isomers  bart              -0.866308         -0.949782   \n",
       "                             roberta            1.100154         -0.638814   \n",
       "                    standard bart               0.418229         -1.118905   \n",
       "                             roberta           -0.334389         -0.434230   \n",
       "\n",
       "                                          z_ROC_AUC_hiv z_ROC_AUC_tox21  \n",
       "task                                                                     \n",
       "embedding tokenizer dataset  architecture                                \n",
       "selfies   atom      isomers  bart              0.151634       -0.168887  \n",
       "                             roberta           0.741667       -1.098292  \n",
       "                    standard bart              0.201836        0.517840  \n",
       "                             roberta          -0.937990       -1.104448  \n",
       "          trained   isomers  bart             -1.225747        1.211732  \n",
       "                             roberta          -0.816731        0.749660  \n",
       "                    standard bart             -0.470254       -0.101501  \n",
       "                             roberta          -1.588507       -0.132732  \n",
       "smiles    atom      isomers  bart              0.606443       -0.506934  \n",
       "                             roberta           0.140188       -1.068852  \n",
       "                    standard bart              0.318459        0.295591  \n",
       "                             roberta           0.165362       -0.079562  \n",
       "          trained   isomers  bart              1.291170        0.598210  \n",
       "                             roberta           0.992995       -0.281495  \n",
       "                    standard bart              0.426927        0.547392  \n",
       "                             roberta           0.002547        0.622277  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_cols = [\"embedding\",\"tokenizer\",\"dataset\",\"architecture\", \"z_ROC_AUC_bace_classification\",\"z_ROC_AUC_bbbp\",\"z_ROC_AUC_clintox\",\"z_ROC_AUC_hiv\",\"z_ROC_AUC_tox21\"]\n",
    "z_df_class = classification_pivot.groupby(classification_cols[:4]).agg(\"mean\")[classification_cols[4:]]\n",
    "chemberta_best_roc = {\"BACE_classification\": 0.799, \"BBBP\": 0.742, \"ClinTox\":0.601, \"Tox21\":0.834}\n",
    "z_df_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9943136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llllrrrrr}\n",
      "\\toprule\n",
      "       &         &          & {} & z\\_ROC\\_AUC\\_bace\\_classification & z\\_ROC\\_AUC\\_bbbp & z\\_ROC\\_AUC\\_clintox & z\\_ROC\\_AUC\\_hiv & z\\_ROC\\_AUC\\_tox21 \\\\\n",
      "       &         &          & task \\\\\n",
      "embedding & tokenizer & dataset & architecture &                               &                &                   &               &                 \\\\\n",
      "\\midrule\n",
      "selfies & atom & isomers & bart &                          0.24 &          -0.16 &              0.17 &          0.15 &           -0.17 \\\\\n",
      "       &         &          & roberta &                          0.72 &          -0.16 &              1.57 &          0.74 &           -1.10 \\\\\n",
      "       &         & standard & bart &                          0.45 &          -0.88 &             -0.98 &          0.20 &            0.52 \\\\\n",
      "       &         &          & roberta &                          0.15 &           0.04 &              1.54 &         -0.94 &           -1.10 \\\\\n",
      "       & trained & isomers & bart &                          0.57 &          -0.27 &             -0.52 &         -1.23 &            1.21 \\\\\n",
      "       &         &          & roberta &                          0.76 &          -0.11 &              0.71 &         -0.82 &            0.75 \\\\\n",
      "       &         & standard & bart &                         -0.95 &          -0.01 &             -0.95 &         -0.47 &           -0.10 \\\\\n",
      "       &         &          & roberta &                         -1.06 &          -0.62 &             -0.21 &         -1.59 &           -0.13 \\\\\n",
      "smiles & atom & isomers & bart &                          1.00 &          -0.44 &              0.29 &          0.61 &           -0.51 \\\\\n",
      "       &         &          & roberta &                         -0.71 &           1.85 &              1.24 &          0.14 &           -1.07 \\\\\n",
      "       &         & standard & bart &                          0.64 &          -0.57 &              0.17 &          0.32 &            0.30 \\\\\n",
      "       &         &          & roberta &                         -0.22 &           1.01 &              0.13 &          0.17 &           -0.08 \\\\\n",
      "       & trained & isomers & bart &                          0.50 &          -0.87 &             -0.95 &          1.29 &            0.60 \\\\\n",
      "       &         &          & roberta &                          0.33 &           1.10 &             -0.64 &          0.99 &           -0.28 \\\\\n",
      "       &         & standard & bart &                         -0.48 &           0.42 &             -1.12 &          0.43 &            0.55 \\\\\n",
      "       &         &          & roberta &                         -1.94 &          -0.33 &             -0.43 &          0.00 &            0.62 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3610201/3305505681.py:5: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  print(z_df_class.to_latex(float_format=\"%.2f\"))\n"
     ]
    }
   ],
   "source": [
    "interesting_rows_classification = interesting_rows\n",
    "vals = []\n",
    "for column in z_df_class:\n",
    "    vals = vals + list(z_df_class[column].values)\n",
    "print(z_df_class.to_latex(float_format=\"%.2f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5575f722",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>(z_ROC_AUC_bace_classification, )</th>\n",
       "      <th>(z_ROC_AUC_bbbp, )</th>\n",
       "      <th>(z_ROC_AUC_clintox, )</th>\n",
       "      <th>(z_ROC_AUC_hiv, )</th>\n",
       "      <th>(z_ROC_AUC_tox21, )</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embedding</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>dataset</th>\n",
       "      <th>architecture</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">selfies</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">atom</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">isomers</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.239336</td>\n",
       "      <td>-0.162830</td>\n",
       "      <td>0.165884</td>\n",
       "      <td>0.151634</td>\n",
       "      <td>-0.168887</td>\n",
       "      <td>0.275982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.722471</td>\n",
       "      <td>-0.164554</td>\n",
       "      <td>1.574787</td>\n",
       "      <td>0.741667</td>\n",
       "      <td>-1.098292</td>\n",
       "      <td>0.429858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">standard</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.448477</td>\n",
       "      <td>-0.876653</td>\n",
       "      <td>-0.983879</td>\n",
       "      <td>0.201836</td>\n",
       "      <td>0.517840</td>\n",
       "      <td>0.361232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.148340</td>\n",
       "      <td>0.037178</td>\n",
       "      <td>1.537961</td>\n",
       "      <td>-0.937990</td>\n",
       "      <td>-1.104448</td>\n",
       "      <td>-0.399788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">trained</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">isomers</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.568632</td>\n",
       "      <td>-0.274904</td>\n",
       "      <td>-0.517427</td>\n",
       "      <td>-1.225747</td>\n",
       "      <td>1.211732</td>\n",
       "      <td>-0.434092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.755149</td>\n",
       "      <td>-0.108517</td>\n",
       "      <td>0.710078</td>\n",
       "      <td>-0.816731</td>\n",
       "      <td>0.749660</td>\n",
       "      <td>0.475519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">standard</th>\n",
       "      <th>bart</th>\n",
       "      <td>-0.950652</td>\n",
       "      <td>-0.005927</td>\n",
       "      <td>-0.951146</td>\n",
       "      <td>-0.470254</td>\n",
       "      <td>-0.101501</td>\n",
       "      <td>0.488163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>-1.062764</td>\n",
       "      <td>-0.618884</td>\n",
       "      <td>-0.214643</td>\n",
       "      <td>-1.588507</td>\n",
       "      <td>-0.132732</td>\n",
       "      <td>0.869828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">smiles</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">atom</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">isomers</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.999481</td>\n",
       "      <td>-0.435255</td>\n",
       "      <td>0.285906</td>\n",
       "      <td>0.606443</td>\n",
       "      <td>-0.506934</td>\n",
       "      <td>-0.509698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>-0.709336</td>\n",
       "      <td>1.848461</td>\n",
       "      <td>1.239269</td>\n",
       "      <td>0.140188</td>\n",
       "      <td>-1.068852</td>\n",
       "      <td>-0.537195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">standard</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.643540</td>\n",
       "      <td>-0.568882</td>\n",
       "      <td>0.167248</td>\n",
       "      <td>0.318459</td>\n",
       "      <td>0.295591</td>\n",
       "      <td>-0.651638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>-0.218661</td>\n",
       "      <td>1.013081</td>\n",
       "      <td>0.127695</td>\n",
       "      <td>0.165362</td>\n",
       "      <td>-0.079562</td>\n",
       "      <td>-0.902012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">trained</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">isomers</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.501264</td>\n",
       "      <td>-0.866308</td>\n",
       "      <td>-0.949782</td>\n",
       "      <td>1.291170</td>\n",
       "      <td>0.598210</td>\n",
       "      <td>0.034371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.332846</td>\n",
       "      <td>1.100154</td>\n",
       "      <td>-0.638814</td>\n",
       "      <td>0.992995</td>\n",
       "      <td>-0.281495</td>\n",
       "      <td>0.514768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">standard</th>\n",
       "      <th>bart</th>\n",
       "      <td>-0.479584</td>\n",
       "      <td>0.418229</td>\n",
       "      <td>-1.118905</td>\n",
       "      <td>0.426927</td>\n",
       "      <td>0.547392</td>\n",
       "      <td>-0.057208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>-1.938539</td>\n",
       "      <td>-0.334389</td>\n",
       "      <td>-0.434230</td>\n",
       "      <td>0.002547</td>\n",
       "      <td>0.622277</td>\n",
       "      <td>0.041909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           (z_ROC_AUC_bace_classification, )  \\\n",
       "embedding tokenizer dataset  architecture                                      \n",
       "selfies   atom      isomers  bart                                   0.239336   \n",
       "                             roberta                                0.722471   \n",
       "                    standard bart                                   0.448477   \n",
       "                             roberta                                0.148340   \n",
       "          trained   isomers  bart                                   0.568632   \n",
       "                             roberta                                0.755149   \n",
       "                    standard bart                                  -0.950652   \n",
       "                             roberta                               -1.062764   \n",
       "smiles    atom      isomers  bart                                   0.999481   \n",
       "                             roberta                               -0.709336   \n",
       "                    standard bart                                   0.643540   \n",
       "                             roberta                               -0.218661   \n",
       "          trained   isomers  bart                                   0.501264   \n",
       "                             roberta                                0.332846   \n",
       "                    standard bart                                  -0.479584   \n",
       "                             roberta                               -1.938539   \n",
       "\n",
       "                                           (z_ROC_AUC_bbbp, )  \\\n",
       "embedding tokenizer dataset  architecture                       \n",
       "selfies   atom      isomers  bart                   -0.162830   \n",
       "                             roberta                -0.164554   \n",
       "                    standard bart                   -0.876653   \n",
       "                             roberta                 0.037178   \n",
       "          trained   isomers  bart                   -0.274904   \n",
       "                             roberta                -0.108517   \n",
       "                    standard bart                   -0.005927   \n",
       "                             roberta                -0.618884   \n",
       "smiles    atom      isomers  bart                   -0.435255   \n",
       "                             roberta                 1.848461   \n",
       "                    standard bart                   -0.568882   \n",
       "                             roberta                 1.013081   \n",
       "          trained   isomers  bart                   -0.866308   \n",
       "                             roberta                 1.100154   \n",
       "                    standard bart                    0.418229   \n",
       "                             roberta                -0.334389   \n",
       "\n",
       "                                           (z_ROC_AUC_clintox, )  \\\n",
       "embedding tokenizer dataset  architecture                          \n",
       "selfies   atom      isomers  bart                       0.165884   \n",
       "                             roberta                    1.574787   \n",
       "                    standard bart                      -0.983879   \n",
       "                             roberta                    1.537961   \n",
       "          trained   isomers  bart                      -0.517427   \n",
       "                             roberta                    0.710078   \n",
       "                    standard bart                      -0.951146   \n",
       "                             roberta                   -0.214643   \n",
       "smiles    atom      isomers  bart                       0.285906   \n",
       "                             roberta                    1.239269   \n",
       "                    standard bart                       0.167248   \n",
       "                             roberta                    0.127695   \n",
       "          trained   isomers  bart                      -0.949782   \n",
       "                             roberta                   -0.638814   \n",
       "                    standard bart                      -1.118905   \n",
       "                             roberta                   -0.434230   \n",
       "\n",
       "                                           (z_ROC_AUC_hiv, )  \\\n",
       "embedding tokenizer dataset  architecture                      \n",
       "selfies   atom      isomers  bart                   0.151634   \n",
       "                             roberta                0.741667   \n",
       "                    standard bart                   0.201836   \n",
       "                             roberta               -0.937990   \n",
       "          trained   isomers  bart                  -1.225747   \n",
       "                             roberta               -0.816731   \n",
       "                    standard bart                  -0.470254   \n",
       "                             roberta               -1.588507   \n",
       "smiles    atom      isomers  bart                   0.606443   \n",
       "                             roberta                0.140188   \n",
       "                    standard bart                   0.318459   \n",
       "                             roberta                0.165362   \n",
       "          trained   isomers  bart                   1.291170   \n",
       "                             roberta                0.992995   \n",
       "                    standard bart                   0.426927   \n",
       "                             roberta                0.002547   \n",
       "\n",
       "                                           (z_ROC_AUC_tox21, )         0  \n",
       "embedding tokenizer dataset  architecture                                 \n",
       "selfies   atom      isomers  bart                    -0.168887  0.275982  \n",
       "                             roberta                 -1.098292  0.429858  \n",
       "                    standard bart                     0.517840  0.361232  \n",
       "                             roberta                 -1.104448 -0.399788  \n",
       "          trained   isomers  bart                     1.211732 -0.434092  \n",
       "                             roberta                  0.749660  0.475519  \n",
       "                    standard bart                    -0.101501  0.488163  \n",
       "                             roberta                 -0.132732  0.869828  \n",
       "smiles    atom      isomers  bart                    -0.506934 -0.509698  \n",
       "                             roberta                 -1.068852 -0.537195  \n",
       "                    standard bart                     0.295591 -0.651638  \n",
       "                             roberta                 -0.079562 -0.902012  \n",
       "          trained   isomers  bart                     0.598210  0.034371  \n",
       "                             roberta                 -0.281495  0.514768  \n",
       "                    standard bart                     0.547392 -0.057208  \n",
       "                             roberta                  0.622277  0.041909  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "together_df = pd.concat([z_df_class,z_df.mean(axis=1)], axis=1)\n",
    "together_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234a0394",
   "metadata": {},
   "source": [
    "## Statistical tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8a4fd84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>z_ROC_AUC_bbbp</th>\n",
       "      <th>z_ROC_AUC_clintox</th>\n",
       "      <th>z_ROC_AUC_hiv</th>\n",
       "      <th>z_ROC_AUC_tox21</th>\n",
       "      <th>z_rectified_mean_squared_error_clearance</th>\n",
       "      <th>z_rectified_mean_squared_error_delaney</th>\n",
       "      <th>z_rectified_mean_squared_error_lipo</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>task</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embedding</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>dataset</th>\n",
       "      <th>architecture</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">selfies</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">atom</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">isomers</th>\n",
       "      <th>bart</th>\n",
       "      <td>-0.162830</td>\n",
       "      <td>0.165884</td>\n",
       "      <td>0.151634</td>\n",
       "      <td>-0.168887</td>\n",
       "      <td>0.584487</td>\n",
       "      <td>-1.297594</td>\n",
       "      <td>-0.114841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>-0.164554</td>\n",
       "      <td>1.574787</td>\n",
       "      <td>0.741667</td>\n",
       "      <td>-1.098292</td>\n",
       "      <td>-0.880342</td>\n",
       "      <td>0.675092</td>\n",
       "      <td>-1.084326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">standard</th>\n",
       "      <th>bart</th>\n",
       "      <td>-0.876653</td>\n",
       "      <td>-0.983879</td>\n",
       "      <td>0.201836</td>\n",
       "      <td>0.517840</td>\n",
       "      <td>0.170258</td>\n",
       "      <td>-1.882725</td>\n",
       "      <td>0.628770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.037178</td>\n",
       "      <td>1.537961</td>\n",
       "      <td>-0.937990</td>\n",
       "      <td>-1.104448</td>\n",
       "      <td>-0.139583</td>\n",
       "      <td>1.186816</td>\n",
       "      <td>0.152132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">trained</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">isomers</th>\n",
       "      <th>bart</th>\n",
       "      <td>-0.274904</td>\n",
       "      <td>-0.517427</td>\n",
       "      <td>-1.225747</td>\n",
       "      <td>1.211732</td>\n",
       "      <td>0.301958</td>\n",
       "      <td>0.129224</td>\n",
       "      <td>0.871093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>-0.108517</td>\n",
       "      <td>0.710078</td>\n",
       "      <td>-0.816731</td>\n",
       "      <td>0.749660</td>\n",
       "      <td>-0.260561</td>\n",
       "      <td>0.017686</td>\n",
       "      <td>-1.183682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">standard</th>\n",
       "      <th>bart</th>\n",
       "      <td>-0.005927</td>\n",
       "      <td>-0.951146</td>\n",
       "      <td>-0.470254</td>\n",
       "      <td>-0.101501</td>\n",
       "      <td>-0.921672</td>\n",
       "      <td>-0.304876</td>\n",
       "      <td>-0.237943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>-0.618884</td>\n",
       "      <td>-0.214643</td>\n",
       "      <td>-1.588507</td>\n",
       "      <td>-0.132732</td>\n",
       "      <td>-0.895104</td>\n",
       "      <td>0.018145</td>\n",
       "      <td>-1.732526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">smiles</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">atom</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">isomers</th>\n",
       "      <th>bart</th>\n",
       "      <td>-0.435255</td>\n",
       "      <td>0.285906</td>\n",
       "      <td>0.606443</td>\n",
       "      <td>-0.506934</td>\n",
       "      <td>0.841603</td>\n",
       "      <td>-0.138867</td>\n",
       "      <td>0.826357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>1.848461</td>\n",
       "      <td>1.239269</td>\n",
       "      <td>0.140188</td>\n",
       "      <td>-1.068852</td>\n",
       "      <td>-0.308057</td>\n",
       "      <td>1.119985</td>\n",
       "      <td>0.799658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">standard</th>\n",
       "      <th>bart</th>\n",
       "      <td>-0.568882</td>\n",
       "      <td>0.167248</td>\n",
       "      <td>0.318459</td>\n",
       "      <td>0.295591</td>\n",
       "      <td>0.177510</td>\n",
       "      <td>1.125470</td>\n",
       "      <td>0.651933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>1.013081</td>\n",
       "      <td>0.127695</td>\n",
       "      <td>0.165362</td>\n",
       "      <td>-0.079562</td>\n",
       "      <td>0.369582</td>\n",
       "      <td>0.903376</td>\n",
       "      <td>1.433078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">trained</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">isomers</th>\n",
       "      <th>bart</th>\n",
       "      <td>-0.866308</td>\n",
       "      <td>-0.949782</td>\n",
       "      <td>1.291170</td>\n",
       "      <td>0.598210</td>\n",
       "      <td>0.353675</td>\n",
       "      <td>-0.734860</td>\n",
       "      <td>0.278072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>1.100154</td>\n",
       "      <td>-0.638814</td>\n",
       "      <td>0.992995</td>\n",
       "      <td>-0.281495</td>\n",
       "      <td>-0.671705</td>\n",
       "      <td>-0.081790</td>\n",
       "      <td>-0.790808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">standard</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.418229</td>\n",
       "      <td>-1.118905</td>\n",
       "      <td>0.426927</td>\n",
       "      <td>0.547392</td>\n",
       "      <td>1.043210</td>\n",
       "      <td>-0.845550</td>\n",
       "      <td>-0.026036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>-0.334389</td>\n",
       "      <td>-0.434230</td>\n",
       "      <td>0.002547</td>\n",
       "      <td>0.622277</td>\n",
       "      <td>0.234739</td>\n",
       "      <td>0.110466</td>\n",
       "      <td>-0.470933</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          z_ROC_AUC_bbbp z_ROC_AUC_clintox  \\\n",
       "task                                                                         \n",
       "embedding tokenizer dataset  architecture                                    \n",
       "selfies   atom      isomers  bart              -0.162830          0.165884   \n",
       "                             roberta           -0.164554          1.574787   \n",
       "                    standard bart              -0.876653         -0.983879   \n",
       "                             roberta            0.037178          1.537961   \n",
       "          trained   isomers  bart              -0.274904         -0.517427   \n",
       "                             roberta           -0.108517          0.710078   \n",
       "                    standard bart              -0.005927         -0.951146   \n",
       "                             roberta           -0.618884         -0.214643   \n",
       "smiles    atom      isomers  bart              -0.435255          0.285906   \n",
       "                             roberta            1.848461          1.239269   \n",
       "                    standard bart              -0.568882          0.167248   \n",
       "                             roberta            1.013081          0.127695   \n",
       "          trained   isomers  bart              -0.866308         -0.949782   \n",
       "                             roberta            1.100154         -0.638814   \n",
       "                    standard bart               0.418229         -1.118905   \n",
       "                             roberta           -0.334389         -0.434230   \n",
       "\n",
       "                                          z_ROC_AUC_hiv z_ROC_AUC_tox21  \\\n",
       "task                                                                      \n",
       "embedding tokenizer dataset  architecture                                 \n",
       "selfies   atom      isomers  bart              0.151634       -0.168887   \n",
       "                             roberta           0.741667       -1.098292   \n",
       "                    standard bart              0.201836        0.517840   \n",
       "                             roberta          -0.937990       -1.104448   \n",
       "          trained   isomers  bart             -1.225747        1.211732   \n",
       "                             roberta          -0.816731        0.749660   \n",
       "                    standard bart             -0.470254       -0.101501   \n",
       "                             roberta          -1.588507       -0.132732   \n",
       "smiles    atom      isomers  bart              0.606443       -0.506934   \n",
       "                             roberta           0.140188       -1.068852   \n",
       "                    standard bart              0.318459        0.295591   \n",
       "                             roberta           0.165362       -0.079562   \n",
       "          trained   isomers  bart              1.291170        0.598210   \n",
       "                             roberta           0.992995       -0.281495   \n",
       "                    standard bart              0.426927        0.547392   \n",
       "                             roberta           0.002547        0.622277   \n",
       "\n",
       "                                          z_rectified_mean_squared_error_clearance  \\\n",
       "task                                                                                 \n",
       "embedding tokenizer dataset  architecture                                            \n",
       "selfies   atom      isomers  bart                                         0.584487   \n",
       "                             roberta                                     -0.880342   \n",
       "                    standard bart                                         0.170258   \n",
       "                             roberta                                     -0.139583   \n",
       "          trained   isomers  bart                                         0.301958   \n",
       "                             roberta                                     -0.260561   \n",
       "                    standard bart                                        -0.921672   \n",
       "                             roberta                                     -0.895104   \n",
       "smiles    atom      isomers  bart                                         0.841603   \n",
       "                             roberta                                     -0.308057   \n",
       "                    standard bart                                         0.177510   \n",
       "                             roberta                                      0.369582   \n",
       "          trained   isomers  bart                                         0.353675   \n",
       "                             roberta                                     -0.671705   \n",
       "                    standard bart                                         1.043210   \n",
       "                             roberta                                      0.234739   \n",
       "\n",
       "                                          z_rectified_mean_squared_error_delaney  \\\n",
       "task                                                                               \n",
       "embedding tokenizer dataset  architecture                                          \n",
       "selfies   atom      isomers  bart                                      -1.297594   \n",
       "                             roberta                                    0.675092   \n",
       "                    standard bart                                      -1.882725   \n",
       "                             roberta                                    1.186816   \n",
       "          trained   isomers  bart                                       0.129224   \n",
       "                             roberta                                    0.017686   \n",
       "                    standard bart                                      -0.304876   \n",
       "                             roberta                                    0.018145   \n",
       "smiles    atom      isomers  bart                                      -0.138867   \n",
       "                             roberta                                    1.119985   \n",
       "                    standard bart                                       1.125470   \n",
       "                             roberta                                    0.903376   \n",
       "          trained   isomers  bart                                      -0.734860   \n",
       "                             roberta                                   -0.081790   \n",
       "                    standard bart                                      -0.845550   \n",
       "                             roberta                                    0.110466   \n",
       "\n",
       "                                          z_rectified_mean_squared_error_lipo  \n",
       "task                                                                           \n",
       "embedding tokenizer dataset  architecture                                      \n",
       "selfies   atom      isomers  bart                                   -0.114841  \n",
       "                             roberta                                -1.084326  \n",
       "                    standard bart                                    0.628770  \n",
       "                             roberta                                 0.152132  \n",
       "          trained   isomers  bart                                    0.871093  \n",
       "                             roberta                                -1.183682  \n",
       "                    standard bart                                   -0.237943  \n",
       "                             roberta                                -1.732526  \n",
       "smiles    atom      isomers  bart                                    0.826357  \n",
       "                             roberta                                 0.799658  \n",
       "                    standard bart                                    0.651933  \n",
       "                             roberta                                 1.433078  \n",
       "          trained   isomers  bart                                    0.278072  \n",
       "                             roberta                                -0.790808  \n",
       "                    standard bart                                   -0.026036  \n",
       "                             roberta                                -0.470933  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "together_df = pd.concat([z_df_class,z_df], axis=1).drop(\"z_ROC_AUC_bace_classification\",axis=\"columns\")\n",
    "together_df[\"z_rectified_mean_squared_error_clearance\"] = -together_df[\"z_rectified_mean_squared_error_clearance\"]\n",
    "together_df[\"z_rectified_mean_squared_error_delaney\"] = -together_df[\"z_rectified_mean_squared_error_delaney\"]\n",
    "together_df[\"z_rectified_mean_squared_error_lipo\"] = -together_df[\"z_rectified_mean_squared_error_lipo\"]\n",
    "together_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4df0246a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len: 56\n",
      "sum atomwise-sentencepiece : 17.558495333130413\n",
      "sum atomwise>sentencepiece : 37\n",
      "The wilcox stats for atomwise vs. sentencepiece, two-sided: statistic 546, p-value 0.040\n",
      "The wilcox stats for atomwise vs. sentencepiece, less: statistic 1050, p-value 0.980\n",
      "The wilcox stats for atomwise vs. sentencepiece, greater: statistic 1050, p-value 0.020\n"
     ]
    }
   ],
   "source": [
    "atom = together_df.loc[:,\"atom\",:,:].values.flatten()\n",
    "trained = together_df.loc[:,\"trained\",:,:].values.flatten()\n",
    "print(f\"len: {len(atom)}\")\n",
    "print(f\"sum atomwise-sentencepiece : {sum(atom-trained)}\")\n",
    "print(f\"sum atomwise>sentencepiece : {sum(atom>trained)}\")\n",
    "two_sided = scipy.stats.wilcoxon(atom, trained, zero_method='wilcox', correction=False, alternative='two-sided', method='auto', axis=0, nan_policy='propagate', keepdims=False)\n",
    "print(f\"The wilcox stats for atomwise vs. sentencepiece, two-sided: statistic {int(two_sided.statistic)}, p-value {two_sided.pvalue:.3f}\")\n",
    "less = scipy.stats.wilcoxon(atom, trained, zero_method='wilcox', correction=False, alternative='less', method='auto', axis=0, nan_policy='propagate', keepdims=False)\n",
    "print(f\"The wilcox stats for atomwise vs. sentencepiece, less: statistic {int(less.statistic)}, p-value {less.pvalue:.3f}\")\n",
    "greater = scipy.stats.wilcoxon(atom, trained, zero_method='wilcox', correction=False, alternative='greater', method='auto', axis=0, nan_policy='propagate', keepdims=False)\n",
    "print(f\"The wilcox stats for atomwise vs. sentencepiece, greater: statistic {int(greater.statistic)}, p-value {greater.pvalue:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "34106399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len: 56\n",
      "sum standard-isomers : -6.7487352761360135\n",
      "sum standard>isomers : 23\n",
      "The wilcox stats for standard vs. isomers, two-sided: statistic 656, p-value 0.247\n",
      "The wilcox stats for standard vs. isomers, less: statistic 656, p-value 0.123\n",
      "The wilcox stats for standard vs. isomers, greater: statistic 656, p-value 0.877\n"
     ]
    }
   ],
   "source": [
    "atom = together_df.loc[:,:,\"standard\",:].values.flatten()\n",
    "trained = together_df.loc[:,:,\"isomers\",:].values.flatten()\n",
    "print(f\"len: {len(atom)}\")\n",
    "print(f\"sum standard-isomers : {sum(atom-trained)}\")\n",
    "print(f\"sum standard>isomers : {sum(atom>trained)}\")\n",
    "two_sided = scipy.stats.wilcoxon(atom, trained, zero_method='wilcox', correction=False, alternative='two-sided', method='auto', axis=0, nan_policy='propagate', keepdims=False)\n",
    "print(f\"The wilcox stats for standard vs. isomers, two-sided: statistic {int(two_sided.statistic)}, p-value {two_sided.pvalue:.3f}\")\n",
    "less = scipy.stats.wilcoxon(atom, trained, zero_method='wilcox', correction=False, alternative='less', method='auto', axis=0, nan_policy='propagate', keepdims=False)\n",
    "print(f\"The wilcox stats for standard vs. isomers, less: statistic {int(less.statistic)}, p-value {less.pvalue:.3f}\")\n",
    "greater = scipy.stats.wilcoxon(atom, trained, zero_method='wilcox', correction=False, alternative='greater', method='auto', axis=0, nan_policy='propagate', keepdims=False)\n",
    "print(f\"The wilcox stats for standard vs. isomers, greater: statistic {int(greater.statistic)}, p-value {greater.pvalue:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "89fc0f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len: 56\n",
      "sum smiles-selfies : 22.24861288483121\n",
      "sum smiles>selfies : 37\n",
      "The wilcox stats for smiles vs. selfies, two-sided: statistic 469, p-value 0.007\n",
      "The wilcox stats for smiles vs. selfies, less: statistic 1127, p-value 0.996\n",
      "The wilcox stats for smiles vs. selfies, greater: statistic 1127, p-value 0.004\n"
     ]
    }
   ],
   "source": [
    "atom = together_df.loc[\"smiles\",:,:,:].values.flatten()\n",
    "trained = together_df.loc[\"selfies\",:,:,:].values.flatten()\n",
    "print(f\"len: {len(atom)}\")\n",
    "print(f\"sum smiles-selfies : {sum(atom-trained)}\")\n",
    "print(f\"sum smiles>selfies : {sum(atom>trained)}\")\n",
    "two_sided = scipy.stats.wilcoxon(atom, trained, zero_method='wilcox', correction=False, alternative='two-sided', method='auto', axis=0, nan_policy='propagate', keepdims=False)\n",
    "print(f\"The wilcox stats for smiles vs. selfies, two-sided: statistic {int(two_sided.statistic)}, p-value {two_sided.pvalue:.3f}\")\n",
    "less = scipy.stats.wilcoxon(atom, trained, zero_method='wilcox', correction=False, alternative='less', method='auto', axis=0, nan_policy='propagate', keepdims=False)\n",
    "print(f\"The wilcox stats for smiles vs. selfies, less: statistic {int(less.statistic)}, p-value {less.pvalue:.3f}\")\n",
    "greater = scipy.stats.wilcoxon(atom, trained, zero_method='wilcox', correction=False, alternative='greater', method='auto', axis=0, nan_policy='propagate', keepdims=False)\n",
    "print(f\"The wilcox stats for smiles vs. selfies, greater: statistic {int(greater.statistic)}, p-value {greater.pvalue:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5c439241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len: 56\n",
      "sum roberta-bart : 3.004119606991411\n",
      "sum roberta>bart : 24\n",
      "The wilcox stats for roberta vs. bart, two-sided: statistic 772, p-value 0.832\n",
      "The wilcox stats for roberta vs. bart, less: statistic 772, p-value 0.416\n",
      "The wilcox stats for roberta vs. bart, greater: statistic 772, p-value 0.584\n"
     ]
    }
   ],
   "source": [
    "atom = together_df.loc[:,:,:,\"roberta\"].values.flatten()\n",
    "trained = together_df.loc[:,:,:,\"bart\"].values.flatten()\n",
    "print(f\"len: {len(atom)}\")\n",
    "print(f\"sum roberta-bart : {sum(atom-trained)}\")\n",
    "print(f\"sum roberta>bart : {sum(atom>trained)}\")\n",
    "two_sided = scipy.stats.wilcoxon(atom, trained, zero_method='wilcox', correction=False, alternative='two-sided', method='auto', axis=0, nan_policy='propagate', keepdims=False)\n",
    "print(f\"The wilcox stats for roberta vs. bart, two-sided: statistic {int(two_sided.statistic)}, p-value {two_sided.pvalue:.3f}\")\n",
    "less = scipy.stats.wilcoxon(atom, trained, zero_method='wilcox', correction=False, alternative='less', method='auto', axis=0, nan_policy='propagate', keepdims=False)\n",
    "print(f\"The wilcox stats for roberta vs. bart, less: statistic {int(less.statistic)}, p-value {less.pvalue:.3f}\")\n",
    "greater = scipy.stats.wilcoxon(atom, trained, zero_method='wilcox', correction=False, alternative='greater', method='auto', axis=0, nan_policy='propagate', keepdims=False)\n",
    "print(f\"The wilcox stats for roberta vs. bart, greater: statistic {int(greater.statistic)}, p-value {greater.pvalue:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
