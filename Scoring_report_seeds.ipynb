{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "889c6e79",
   "metadata": {},
   "source": [
    "# Scoring report\n",
    "## Gather scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f622a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jgut/miniconda3/envs/SMILES_OR_SELFIES/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Skipped loading some Tensorflow models, missing a dependency. No module named 'tensorflow'\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'torch_geometric'\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. cannot import name 'DMPNN' from 'deepchem.models.torch_models' (/home/jgut/miniconda3/envs/SMILES_OR_SELFIES/lib/python3.10/site-packages/deepchem/models/torch_models/__init__.py)\n",
      "Skipped loading modules with pytorch-lightning dependency, missing a dependency. No module named 'pytorch_lightning'\n",
      "Skipped loading some Jax models, missing a dependency. No module named 'jax'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data2/jgut/SoS_models/bace_classification/smiles_atom_isomers_bart\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_atom_isomers_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_atom_isomers_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_atom_isomers_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_atom_isomers_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_atom_isomers_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_atom_isomers_roberta\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_atom_isomers_roberta/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_atom_isomers_roberta/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_atom_isomers_roberta/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_atom_isomers_roberta/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_atom_isomers_roberta/5e-06_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_atom_isomers_bart\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_atom_isomers_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_atom_isomers_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_atom_isomers_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_atom_isomers_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_atom_isomers_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_atom_isomers_roberta\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_atom_isomers_roberta/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_atom_isomers_roberta/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_atom_isomers_roberta/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_atom_isomers_roberta/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_atom_isomers_roberta/5e-06_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_atom_isomers_bart\n",
      "/data2/jgut/SoS_models/bbbp/smiles_atom_isomers_bart/1e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_atom_isomers_bart/1e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_atom_isomers_bart/1e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_atom_isomers_bart/1e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_atom_isomers_bart/1e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_atom_isomers_roberta\n",
      "/data2/jgut/SoS_models/bbbp/smiles_atom_isomers_roberta/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_atom_isomers_roberta/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_atom_isomers_roberta/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_atom_isomers_roberta/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_atom_isomers_roberta/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_atom_isomers_bart\n",
      "/data2/jgut/SoS_models/clearance/smiles_atom_isomers_bart/1e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_atom_isomers_bart/1e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_atom_isomers_bart/1e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_atom_isomers_bart/1e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_atom_isomers_bart/1e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_atom_isomers_roberta\n",
      "/data2/jgut/SoS_models/clearance/smiles_atom_isomers_roberta/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_atom_isomers_roberta/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_atom_isomers_roberta/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_atom_isomers_roberta/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_atom_isomers_roberta/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_atom_isomers_bart\n",
      "/data2/jgut/SoS_models/clintox/smiles_atom_isomers_bart/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_atom_isomers_bart/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_atom_isomers_bart/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_atom_isomers_bart/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_atom_isomers_bart/5e-06_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_atom_isomers_roberta\n",
      "/data2/jgut/SoS_models/clintox/smiles_atom_isomers_roberta/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_atom_isomers_roberta/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_atom_isomers_roberta/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_atom_isomers_roberta/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_atom_isomers_roberta/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_atom_isomers_bart\n",
      "/data2/jgut/SoS_models/delaney/smiles_atom_isomers_bart/1e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_atom_isomers_bart/1e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_atom_isomers_bart/1e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_atom_isomers_bart/1e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_atom_isomers_bart/1e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_atom_isomers_roberta\n",
      "/data2/jgut/SoS_models/delaney/smiles_atom_isomers_roberta/1e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_atom_isomers_roberta/1e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_atom_isomers_roberta/1e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_atom_isomers_roberta/1e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_atom_isomers_roberta/1e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_atom_isomers_bart\n",
      "/data2/jgut/SoS_models/hiv/smiles_atom_isomers_bart/1e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_atom_isomers_bart/1e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_atom_isomers_bart/1e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_atom_isomers_bart/1e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_atom_isomers_bart/1e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_atom_isomers_roberta\n",
      "/data2/jgut/SoS_models/hiv/smiles_atom_isomers_roberta/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_atom_isomers_roberta/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_atom_isomers_roberta/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_atom_isomers_roberta/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_atom_isomers_roberta/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_atom_isomers_bart\n",
      "/data2/jgut/SoS_models/lipo/smiles_atom_isomers_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_atom_isomers_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_atom_isomers_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_atom_isomers_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_atom_isomers_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_atom_isomers_roberta\n",
      "/data2/jgut/SoS_models/lipo/smiles_atom_isomers_roberta/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_atom_isomers_roberta/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_atom_isomers_roberta/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_atom_isomers_roberta/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_atom_isomers_roberta/5e-06_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_atom_isomers_bart\n",
      "/data2/jgut/SoS_models/tox21/smiles_atom_isomers_bart/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_atom_isomers_bart/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_atom_isomers_bart/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_atom_isomers_bart/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_atom_isomers_bart/5e-06_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_atom_isomers_roberta\n",
      "/data2/jgut/SoS_models/tox21/smiles_atom_isomers_roberta/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_atom_isomers_roberta/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_atom_isomers_roberta/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_atom_isomers_roberta/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_atom_isomers_roberta/5e-06_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_atom_standard_bart\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_atom_standard_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_atom_standard_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_atom_standard_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_atom_standard_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_atom_standard_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_atom_standard_roberta\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_atom_standard_roberta/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_atom_standard_roberta/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_atom_standard_roberta/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_atom_standard_roberta/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_atom_standard_roberta/5e-06_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_atom_standard_bart\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_atom_standard_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_atom_standard_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_atom_standard_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_atom_standard_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_atom_standard_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_atom_standard_roberta\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_atom_standard_roberta/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_atom_standard_roberta/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_atom_standard_roberta/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_atom_standard_roberta/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_atom_standard_roberta/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_atom_standard_bart\n",
      "/data2/jgut/SoS_models/bbbp/smiles_atom_standard_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_atom_standard_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_atom_standard_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_atom_standard_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_atom_standard_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_atom_standard_roberta\n",
      "/data2/jgut/SoS_models/bbbp/smiles_atom_standard_roberta/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_atom_standard_roberta/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_atom_standard_roberta/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_atom_standard_roberta/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_atom_standard_roberta/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_atom_standard_bart\n",
      "/data2/jgut/SoS_models/clearance/smiles_atom_standard_bart/1e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_atom_standard_bart/1e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_atom_standard_bart/1e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_atom_standard_bart/1e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_atom_standard_bart/1e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_atom_standard_roberta\n",
      "/data2/jgut/SoS_models/clearance/smiles_atom_standard_roberta/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_atom_standard_roberta/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_atom_standard_roberta/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_atom_standard_roberta/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_atom_standard_roberta/5e-06_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_atom_standard_bart\n",
      "/data2/jgut/SoS_models/clintox/smiles_atom_standard_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_atom_standard_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_atom_standard_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_atom_standard_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_atom_standard_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_atom_standard_roberta\n",
      "/data2/jgut/SoS_models/clintox/smiles_atom_standard_roberta/1e-05_0.2_seed_0/scores.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data2/jgut/SoS_models/clintox/smiles_atom_standard_roberta/1e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_atom_standard_roberta/1e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_atom_standard_roberta/1e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_atom_standard_roberta/1e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_atom_standard_bart\n",
      "/data2/jgut/SoS_models/delaney/smiles_atom_standard_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_atom_standard_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_atom_standard_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_atom_standard_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_atom_standard_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_atom_standard_roberta\n",
      "/data2/jgut/SoS_models/delaney/smiles_atom_standard_roberta/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_atom_standard_roberta/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_atom_standard_roberta/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_atom_standard_roberta/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_atom_standard_roberta/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_atom_standard_bart\n",
      "/data2/jgut/SoS_models/hiv/smiles_atom_standard_bart/1e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_atom_standard_bart/1e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_atom_standard_bart/1e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_atom_standard_bart/1e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_atom_standard_bart/1e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_atom_standard_roberta\n",
      "/data2/jgut/SoS_models/hiv/smiles_atom_standard_roberta/1e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_atom_standard_roberta/1e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_atom_standard_roberta/1e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_atom_standard_roberta/1e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_atom_standard_roberta/1e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_atom_standard_bart\n",
      "/data2/jgut/SoS_models/lipo/smiles_atom_standard_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_atom_standard_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_atom_standard_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_atom_standard_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_atom_standard_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_atom_standard_roberta\n",
      "/data2/jgut/SoS_models/lipo/smiles_atom_standard_roberta/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_atom_standard_roberta/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_atom_standard_roberta/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_atom_standard_roberta/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_atom_standard_roberta/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_atom_standard_bart\n",
      "/data2/jgut/SoS_models/tox21/smiles_atom_standard_bart/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_atom_standard_bart/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_atom_standard_bart/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_atom_standard_bart/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_atom_standard_bart/5e-06_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_atom_standard_roberta\n",
      "/data2/jgut/SoS_models/tox21/smiles_atom_standard_roberta/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_atom_standard_roberta/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_atom_standard_roberta/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_atom_standard_roberta/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_atom_standard_roberta/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_trained_isomers_bart\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_trained_isomers_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_trained_isomers_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_trained_isomers_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_trained_isomers_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_trained_isomers_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_trained_isomers_roberta\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_trained_isomers_roberta/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_trained_isomers_roberta/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_trained_isomers_roberta/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_trained_isomers_roberta/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_trained_isomers_roberta/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_trained_isomers_bart\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_trained_isomers_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_trained_isomers_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_trained_isomers_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_trained_isomers_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_trained_isomers_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_trained_isomers_roberta\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_trained_isomers_roberta/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_trained_isomers_roberta/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_trained_isomers_roberta/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_trained_isomers_roberta/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_trained_isomers_roberta/5e-06_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_trained_isomers_bart\n",
      "/data2/jgut/SoS_models/bbbp/smiles_trained_isomers_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_trained_isomers_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_trained_isomers_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_trained_isomers_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_trained_isomers_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_trained_isomers_roberta\n",
      "/data2/jgut/SoS_models/bbbp/smiles_trained_isomers_roberta/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_trained_isomers_roberta/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_trained_isomers_roberta/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_trained_isomers_roberta/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_trained_isomers_roberta/5e-06_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_trained_isomers_bart\n",
      "/data2/jgut/SoS_models/clearance/smiles_trained_isomers_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_trained_isomers_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_trained_isomers_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_trained_isomers_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_trained_isomers_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_trained_isomers_roberta\n",
      "/data2/jgut/SoS_models/clearance/smiles_trained_isomers_roberta/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_trained_isomers_roberta/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_trained_isomers_roberta/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_trained_isomers_roberta/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_trained_isomers_roberta/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_trained_isomers_bart\n",
      "/data2/jgut/SoS_models/clintox/smiles_trained_isomers_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_trained_isomers_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_trained_isomers_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_trained_isomers_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_trained_isomers_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_trained_isomers_roberta\n",
      "/data2/jgut/SoS_models/clintox/smiles_trained_isomers_roberta/1e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_trained_isomers_roberta/1e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_trained_isomers_roberta/1e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_trained_isomers_roberta/1e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_trained_isomers_roberta/1e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_trained_isomers_bart\n",
      "/data2/jgut/SoS_models/delaney/smiles_trained_isomers_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_trained_isomers_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_trained_isomers_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_trained_isomers_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_trained_isomers_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_trained_isomers_roberta\n",
      "/data2/jgut/SoS_models/delaney/smiles_trained_isomers_roberta/1e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_trained_isomers_roberta/1e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_trained_isomers_roberta/1e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_trained_isomers_roberta/1e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_trained_isomers_roberta/1e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_trained_isomers_bart\n",
      "/data2/jgut/SoS_models/hiv/smiles_trained_isomers_bart/1e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_trained_isomers_bart/1e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_trained_isomers_bart/1e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_trained_isomers_bart/1e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_trained_isomers_bart/1e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_trained_isomers_roberta\n",
      "/data2/jgut/SoS_models/hiv/smiles_trained_isomers_roberta/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_trained_isomers_roberta/5e-05_0.2_seed_1/scores.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data2/jgut/SoS_models/hiv/smiles_trained_isomers_roberta/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_trained_isomers_roberta/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_trained_isomers_roberta/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_trained_isomers_bart\n",
      "/data2/jgut/SoS_models/lipo/smiles_trained_isomers_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_trained_isomers_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_trained_isomers_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_trained_isomers_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_trained_isomers_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_trained_isomers_roberta\n",
      "/data2/jgut/SoS_models/lipo/smiles_trained_isomers_roberta/1e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_trained_isomers_roberta/1e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_trained_isomers_roberta/1e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_trained_isomers_roberta/1e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_trained_isomers_roberta/1e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_trained_isomers_bart\n",
      "/data2/jgut/SoS_models/tox21/smiles_trained_isomers_bart/1e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_trained_isomers_bart/1e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_trained_isomers_bart/1e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_trained_isomers_bart/1e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_trained_isomers_bart/1e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_trained_isomers_roberta\n",
      "/data2/jgut/SoS_models/tox21/smiles_trained_isomers_roberta/1e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_trained_isomers_roberta/1e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_trained_isomers_roberta/1e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_trained_isomers_roberta/1e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_trained_isomers_roberta/1e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_trained_standard_bart\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_trained_standard_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_trained_standard_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_trained_standard_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_trained_standard_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_trained_standard_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_trained_standard_roberta\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_trained_standard_roberta/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_trained_standard_roberta/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_trained_standard_roberta/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_trained_standard_roberta/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/smiles_trained_standard_roberta/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_trained_standard_bart\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_trained_standard_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_trained_standard_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_trained_standard_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_trained_standard_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_trained_standard_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_trained_standard_roberta\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_trained_standard_roberta/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_trained_standard_roberta/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_trained_standard_roberta/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_trained_standard_roberta/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/smiles_trained_standard_roberta/5e-06_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_trained_standard_bart\n",
      "/data2/jgut/SoS_models/bbbp/smiles_trained_standard_bart/1e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_trained_standard_bart/1e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_trained_standard_bart/1e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_trained_standard_bart/1e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_trained_standard_bart/1e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_trained_standard_roberta\n",
      "/data2/jgut/SoS_models/bbbp/smiles_trained_standard_roberta/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_trained_standard_roberta/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_trained_standard_roberta/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_trained_standard_roberta/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/smiles_trained_standard_roberta/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_trained_standard_bart\n",
      "/data2/jgut/SoS_models/clearance/smiles_trained_standard_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_trained_standard_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_trained_standard_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_trained_standard_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_trained_standard_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_trained_standard_roberta\n",
      "/data2/jgut/SoS_models/clearance/smiles_trained_standard_roberta/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_trained_standard_roberta/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_trained_standard_roberta/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_trained_standard_roberta/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/smiles_trained_standard_roberta/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_trained_standard_bart\n",
      "/data2/jgut/SoS_models/clintox/smiles_trained_standard_bart/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_trained_standard_bart/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_trained_standard_bart/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_trained_standard_bart/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_trained_standard_bart/5e-06_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_trained_standard_roberta\n",
      "/data2/jgut/SoS_models/clintox/smiles_trained_standard_roberta/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_trained_standard_roberta/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_trained_standard_roberta/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_trained_standard_roberta/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/smiles_trained_standard_roberta/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_trained_standard_bart\n",
      "/data2/jgut/SoS_models/delaney/smiles_trained_standard_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_trained_standard_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_trained_standard_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_trained_standard_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_trained_standard_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_trained_standard_roberta\n",
      "/data2/jgut/SoS_models/delaney/smiles_trained_standard_roberta/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_trained_standard_roberta/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_trained_standard_roberta/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_trained_standard_roberta/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/smiles_trained_standard_roberta/5e-06_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_trained_standard_bart\n",
      "/data2/jgut/SoS_models/hiv/smiles_trained_standard_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_trained_standard_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_trained_standard_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_trained_standard_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_trained_standard_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_trained_standard_roberta\n",
      "/data2/jgut/SoS_models/hiv/smiles_trained_standard_roberta/1e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_trained_standard_roberta/1e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_trained_standard_roberta/1e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_trained_standard_roberta/1e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/smiles_trained_standard_roberta/1e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_trained_standard_bart\n",
      "/data2/jgut/SoS_models/lipo/smiles_trained_standard_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_trained_standard_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_trained_standard_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_trained_standard_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_trained_standard_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_trained_standard_roberta\n",
      "/data2/jgut/SoS_models/lipo/smiles_trained_standard_roberta/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_trained_standard_roberta/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_trained_standard_roberta/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_trained_standard_roberta/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/smiles_trained_standard_roberta/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_trained_standard_bart\n",
      "/data2/jgut/SoS_models/tox21/smiles_trained_standard_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_trained_standard_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_trained_standard_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_trained_standard_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_trained_standard_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_trained_standard_roberta\n",
      "/data2/jgut/SoS_models/tox21/smiles_trained_standard_roberta/1e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_trained_standard_roberta/1e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_trained_standard_roberta/1e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_trained_standard_roberta/1e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/smiles_trained_standard_roberta/1e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_atom_isomers_bart\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_atom_isomers_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_atom_isomers_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_atom_isomers_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_atom_isomers_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_atom_isomers_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_atom_isomers_roberta\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_atom_isomers_roberta/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_atom_isomers_roberta/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_atom_isomers_roberta/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_atom_isomers_roberta/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_atom_isomers_roberta/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_atom_isomers_bart\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_atom_isomers_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_atom_isomers_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_atom_isomers_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_atom_isomers_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_atom_isomers_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_atom_isomers_roberta\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_atom_isomers_roberta/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_atom_isomers_roberta/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_atom_isomers_roberta/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_atom_isomers_roberta/5e-06_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_atom_isomers_roberta/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_atom_isomers_bart\n",
      "/data2/jgut/SoS_models/bbbp/selfies_atom_isomers_bart/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_atom_isomers_bart/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_atom_isomers_bart/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_atom_isomers_bart/5e-06_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_atom_isomers_bart/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_atom_isomers_roberta\n",
      "/data2/jgut/SoS_models/bbbp/selfies_atom_isomers_roberta/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_atom_isomers_roberta/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_atom_isomers_roberta/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_atom_isomers_roberta/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_atom_isomers_roberta/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_atom_isomers_bart\n",
      "/data2/jgut/SoS_models/clearance/selfies_atom_isomers_bart/1e-05_0.2_seed_0/scores.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data2/jgut/SoS_models/clearance/selfies_atom_isomers_bart/1e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_atom_isomers_bart/1e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_atom_isomers_bart/1e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_atom_isomers_bart/1e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_atom_isomers_roberta\n",
      "/data2/jgut/SoS_models/clearance/selfies_atom_isomers_roberta/1e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_atom_isomers_roberta/1e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_atom_isomers_roberta/1e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_atom_isomers_roberta/1e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_atom_isomers_roberta/1e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_atom_isomers_bart\n",
      "/data2/jgut/SoS_models/clintox/selfies_atom_isomers_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_atom_isomers_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_atom_isomers_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_atom_isomers_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_atom_isomers_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_atom_isomers_roberta\n",
      "/data2/jgut/SoS_models/clintox/selfies_atom_isomers_roberta/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_atom_isomers_roberta/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_atom_isomers_roberta/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_atom_isomers_roberta/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_atom_isomers_roberta/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_atom_isomers_bart\n",
      "/data2/jgut/SoS_models/delaney/selfies_atom_isomers_bart/1e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_atom_isomers_bart/1e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_atom_isomers_bart/1e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_atom_isomers_bart/1e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_atom_isomers_bart/1e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_atom_isomers_roberta\n",
      "/data2/jgut/SoS_models/delaney/selfies_atom_isomers_roberta/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_atom_isomers_roberta/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_atom_isomers_roberta/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_atom_isomers_roberta/5e-06_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_atom_isomers_roberta/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_atom_isomers_bart\n",
      "/data2/jgut/SoS_models/hiv/selfies_atom_isomers_bart/1e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_atom_isomers_bart/1e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_atom_isomers_bart/1e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_atom_isomers_bart/1e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_atom_isomers_bart/1e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_atom_isomers_roberta\n",
      "/data2/jgut/SoS_models/hiv/selfies_atom_isomers_roberta/1e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_atom_isomers_roberta/1e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_atom_isomers_roberta/1e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_atom_isomers_roberta/1e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_atom_isomers_roberta/1e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_atom_isomers_bart\n",
      "/data2/jgut/SoS_models/lipo/selfies_atom_isomers_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_atom_isomers_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_atom_isomers_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_atom_isomers_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_atom_isomers_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_atom_isomers_roberta\n",
      "/data2/jgut/SoS_models/lipo/selfies_atom_isomers_roberta/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_atom_isomers_roberta/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_atom_isomers_roberta/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_atom_isomers_roberta/5e-06_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_atom_isomers_roberta/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_atom_isomers_bart\n",
      "/data2/jgut/SoS_models/tox21/selfies_atom_isomers_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_atom_isomers_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_atom_isomers_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_atom_isomers_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_atom_isomers_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_atom_isomers_roberta\n",
      "/data2/jgut/SoS_models/tox21/selfies_atom_isomers_roberta/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_atom_isomers_roberta/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_atom_isomers_roberta/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_atom_isomers_roberta/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_atom_isomers_roberta/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_atom_standard_bart\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_atom_standard_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_atom_standard_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_atom_standard_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_atom_standard_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_atom_standard_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_atom_standard_roberta\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_atom_standard_roberta/1e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_atom_standard_roberta/1e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_atom_standard_roberta/1e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_atom_standard_roberta/1e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_atom_standard_roberta/1e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_atom_standard_bart\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_atom_standard_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_atom_standard_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_atom_standard_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_atom_standard_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_atom_standard_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_atom_standard_roberta\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_atom_standard_roberta/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_atom_standard_roberta/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_atom_standard_roberta/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_atom_standard_roberta/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_atom_standard_roberta/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_atom_standard_bart\n",
      "/data2/jgut/SoS_models/bbbp/selfies_atom_standard_bart/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_atom_standard_bart/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_atom_standard_bart/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_atom_standard_bart/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_atom_standard_bart/5e-06_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_atom_standard_roberta\n",
      "/data2/jgut/SoS_models/bbbp/selfies_atom_standard_roberta/1e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_atom_standard_roberta/1e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_atom_standard_roberta/1e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_atom_standard_roberta/1e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_atom_standard_roberta/1e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_atom_standard_bart\n",
      "/data2/jgut/SoS_models/clearance/selfies_atom_standard_bart/1e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_atom_standard_bart/1e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_atom_standard_bart/1e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_atom_standard_bart/1e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_atom_standard_bart/1e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_atom_standard_roberta\n",
      "/data2/jgut/SoS_models/clearance/selfies_atom_standard_roberta/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_atom_standard_roberta/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_atom_standard_roberta/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_atom_standard_roberta/5e-06_0.2_seed_3/scores.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data2/jgut/SoS_models/clearance/selfies_atom_standard_roberta/5e-06_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_atom_standard_bart\n",
      "/data2/jgut/SoS_models/clintox/selfies_atom_standard_bart/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_atom_standard_bart/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_atom_standard_bart/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_atom_standard_bart/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_atom_standard_bart/5e-06_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_atom_standard_roberta\n",
      "/data2/jgut/SoS_models/clintox/selfies_atom_standard_roberta/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_atom_standard_roberta/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_atom_standard_roberta/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_atom_standard_roberta/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_atom_standard_roberta/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_atom_standard_bart\n",
      "/data2/jgut/SoS_models/delaney/selfies_atom_standard_bart/1e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_atom_standard_bart/1e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_atom_standard_bart/1e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_atom_standard_bart/1e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_atom_standard_bart/1e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_atom_standard_roberta\n",
      "/data2/jgut/SoS_models/delaney/selfies_atom_standard_roberta/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_atom_standard_roberta/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_atom_standard_roberta/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_atom_standard_roberta/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_atom_standard_roberta/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_atom_standard_bart\n",
      "/data2/jgut/SoS_models/hiv/selfies_atom_standard_bart/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_atom_standard_bart/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_atom_standard_bart/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_atom_standard_bart/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_atom_standard_bart/5e-06_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_atom_standard_roberta\n",
      "/data2/jgut/SoS_models/hiv/selfies_atom_standard_roberta/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_atom_standard_roberta/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_atom_standard_roberta/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_atom_standard_roberta/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_atom_standard_roberta/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_atom_standard_bart\n",
      "/data2/jgut/SoS_models/lipo/selfies_atom_standard_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_atom_standard_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_atom_standard_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_atom_standard_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_atom_standard_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_atom_standard_roberta\n",
      "/data2/jgut/SoS_models/lipo/selfies_atom_standard_roberta/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_atom_standard_roberta/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_atom_standard_roberta/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_atom_standard_roberta/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_atom_standard_roberta/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_atom_standard_bart\n",
      "/data2/jgut/SoS_models/tox21/selfies_atom_standard_bart/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_atom_standard_bart/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_atom_standard_bart/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_atom_standard_bart/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_atom_standard_bart/5e-06_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_atom_standard_roberta\n",
      "/data2/jgut/SoS_models/tox21/selfies_atom_standard_roberta/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_atom_standard_roberta/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_atom_standard_roberta/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_atom_standard_roberta/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_atom_standard_roberta/5e-06_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_trained_isomers_bart\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_trained_isomers_bart/1e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_trained_isomers_bart/1e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_trained_isomers_bart/1e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_trained_isomers_bart/1e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_trained_isomers_bart/1e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_trained_isomers_roberta\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_trained_isomers_roberta/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_trained_isomers_roberta/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_trained_isomers_roberta/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_trained_isomers_roberta/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_trained_isomers_roberta/5e-06_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_trained_isomers_bart\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_trained_isomers_bart/1e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_trained_isomers_bart/1e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_trained_isomers_bart/1e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_trained_isomers_bart/1e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_trained_isomers_bart/1e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_trained_isomers_roberta\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_trained_isomers_roberta/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_trained_isomers_roberta/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_trained_isomers_roberta/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_trained_isomers_roberta/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_trained_isomers_roberta/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_trained_isomers_bart\n",
      "/data2/jgut/SoS_models/bbbp/selfies_trained_isomers_bart/1e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_trained_isomers_bart/1e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_trained_isomers_bart/1e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_trained_isomers_bart/1e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_trained_isomers_bart/1e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_trained_isomers_roberta\n",
      "/data2/jgut/SoS_models/bbbp/selfies_trained_isomers_roberta/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_trained_isomers_roberta/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_trained_isomers_roberta/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_trained_isomers_roberta/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_trained_isomers_roberta/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_trained_isomers_bart\n",
      "/data2/jgut/SoS_models/clearance/selfies_trained_isomers_bart/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_trained_isomers_bart/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_trained_isomers_bart/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_trained_isomers_bart/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_trained_isomers_bart/5e-06_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_trained_isomers_roberta\n",
      "/data2/jgut/SoS_models/clearance/selfies_trained_isomers_roberta/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_trained_isomers_roberta/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_trained_isomers_roberta/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_trained_isomers_roberta/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_trained_isomers_roberta/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_trained_isomers_bart\n",
      "/data2/jgut/SoS_models/clintox/selfies_trained_isomers_bart/1e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_trained_isomers_bart/1e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_trained_isomers_bart/1e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_trained_isomers_bart/1e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_trained_isomers_bart/1e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_trained_isomers_roberta\n",
      "/data2/jgut/SoS_models/clintox/selfies_trained_isomers_roberta/1e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_trained_isomers_roberta/1e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_trained_isomers_roberta/1e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_trained_isomers_roberta/1e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_trained_isomers_roberta/1e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_trained_isomers_bart\n",
      "/data2/jgut/SoS_models/delaney/selfies_trained_isomers_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_trained_isomers_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_trained_isomers_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_trained_isomers_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_trained_isomers_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_trained_isomers_roberta\n",
      "/data2/jgut/SoS_models/delaney/selfies_trained_isomers_roberta/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_trained_isomers_roberta/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_trained_isomers_roberta/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_trained_isomers_roberta/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_trained_isomers_roberta/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_trained_isomers_bart\n",
      "/data2/jgut/SoS_models/hiv/selfies_trained_isomers_bart/1e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_trained_isomers_bart/1e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_trained_isomers_bart/1e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_trained_isomers_bart/1e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_trained_isomers_bart/1e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_trained_isomers_roberta\n",
      "/data2/jgut/SoS_models/hiv/selfies_trained_isomers_roberta/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_trained_isomers_roberta/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_trained_isomers_roberta/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_trained_isomers_roberta/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_trained_isomers_roberta/5e-06_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_trained_isomers_bart\n",
      "/data2/jgut/SoS_models/lipo/selfies_trained_isomers_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_trained_isomers_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_trained_isomers_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_trained_isomers_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_trained_isomers_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_trained_isomers_roberta\n",
      "/data2/jgut/SoS_models/lipo/selfies_trained_isomers_roberta/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_trained_isomers_roberta/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_trained_isomers_roberta/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_trained_isomers_roberta/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_trained_isomers_roberta/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_trained_isomers_bart\n",
      "/data2/jgut/SoS_models/tox21/selfies_trained_isomers_bart/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_trained_isomers_bart/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_trained_isomers_bart/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_trained_isomers_bart/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_trained_isomers_bart/5e-06_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_trained_isomers_roberta\n",
      "/data2/jgut/SoS_models/tox21/selfies_trained_isomers_roberta/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_trained_isomers_roberta/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_trained_isomers_roberta/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_trained_isomers_roberta/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_trained_isomers_roberta/5e-06_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_trained_standard_bart\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_trained_standard_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_trained_standard_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_trained_standard_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_trained_standard_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_trained_standard_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_trained_standard_roberta\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_trained_standard_roberta/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_trained_standard_roberta/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_trained_standard_roberta/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_trained_standard_roberta/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bace_classification/selfies_trained_standard_roberta/5e-05_0.2_seed_4/scores.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data2/jgut/SoS_models/bace_regression/selfies_trained_standard_bart\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_trained_standard_bart/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_trained_standard_bart/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_trained_standard_bart/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_trained_standard_bart/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_trained_standard_bart/5e-06_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_trained_standard_roberta\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_trained_standard_roberta/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_trained_standard_roberta/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_trained_standard_roberta/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_trained_standard_roberta/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bace_regression/selfies_trained_standard_roberta/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_trained_standard_bart\n",
      "/data2/jgut/SoS_models/bbbp/selfies_trained_standard_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_trained_standard_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_trained_standard_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_trained_standard_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_trained_standard_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_trained_standard_roberta\n",
      "/data2/jgut/SoS_models/bbbp/selfies_trained_standard_roberta/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_trained_standard_roberta/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_trained_standard_roberta/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_trained_standard_roberta/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/bbbp/selfies_trained_standard_roberta/5e-06_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_trained_standard_bart\n",
      "/data2/jgut/SoS_models/clearance/selfies_trained_standard_bart/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_trained_standard_bart/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_trained_standard_bart/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_trained_standard_bart/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_trained_standard_bart/5e-06_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_trained_standard_roberta\n",
      "/data2/jgut/SoS_models/clearance/selfies_trained_standard_roberta/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_trained_standard_roberta/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_trained_standard_roberta/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_trained_standard_roberta/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/clearance/selfies_trained_standard_roberta/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_trained_standard_bart\n",
      "/data2/jgut/SoS_models/clintox/selfies_trained_standard_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_trained_standard_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_trained_standard_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_trained_standard_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_trained_standard_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_trained_standard_roberta\n",
      "/data2/jgut/SoS_models/clintox/selfies_trained_standard_roberta/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_trained_standard_roberta/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_trained_standard_roberta/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_trained_standard_roberta/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/clintox/selfies_trained_standard_roberta/5e-06_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_trained_standard_bart\n",
      "/data2/jgut/SoS_models/delaney/selfies_trained_standard_bart/1e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_trained_standard_bart/1e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_trained_standard_bart/1e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_trained_standard_bart/1e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_trained_standard_bart/1e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_trained_standard_roberta\n",
      "/data2/jgut/SoS_models/delaney/selfies_trained_standard_roberta/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_trained_standard_roberta/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_trained_standard_roberta/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_trained_standard_roberta/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/delaney/selfies_trained_standard_roberta/5e-06_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_trained_standard_bart\n",
      "/data2/jgut/SoS_models/hiv/selfies_trained_standard_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_trained_standard_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_trained_standard_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_trained_standard_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_trained_standard_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_trained_standard_roberta\n",
      "/data2/jgut/SoS_models/hiv/selfies_trained_standard_roberta/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_trained_standard_roberta/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_trained_standard_roberta/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_trained_standard_roberta/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/hiv/selfies_trained_standard_roberta/5e-06_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_trained_standard_bart\n",
      "/data2/jgut/SoS_models/lipo/selfies_trained_standard_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_trained_standard_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_trained_standard_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_trained_standard_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_trained_standard_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_trained_standard_roberta\n",
      "/data2/jgut/SoS_models/lipo/selfies_trained_standard_roberta/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_trained_standard_roberta/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_trained_standard_roberta/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_trained_standard_roberta/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/lipo/selfies_trained_standard_roberta/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_trained_standard_bart\n",
      "/data2/jgut/SoS_models/tox21/selfies_trained_standard_bart/5e-05_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_trained_standard_bart/5e-05_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_trained_standard_bart/5e-05_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_trained_standard_bart/5e-05_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_trained_standard_bart/5e-05_0.2_seed_4/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_trained_standard_roberta\n",
      "/data2/jgut/SoS_models/tox21/selfies_trained_standard_roberta/5e-06_0.2_seed_0/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_trained_standard_roberta/5e-06_0.2_seed_1/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_trained_standard_roberta/5e-06_0.2_seed_2/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_trained_standard_roberta/5e-06_0.2_seed_3/scores.csv\n",
      "/data2/jgut/SoS_models/tox21/selfies_trained_standard_roberta/5e-06_0.2_seed_4/scores.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from constants import TASK_MODEL_PATH, TOKENIZER_SUFFIXES, MOLNET_DIRECTORY, PROJECT_PATH, DESCRIPTORS\n",
    "\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "classification_scores = []\n",
    "regression_scores = []\n",
    "for tokenizer_suffix in TOKENIZER_SUFFIXES:\n",
    "    for task in MOLNET_DIRECTORY.keys():\n",
    "        for model in [\"bart\", \"roberta\"]:\n",
    "            task_tokenizer_path = TASK_MODEL_PATH/task/(tokenizer_suffix+\"_\"+model)\n",
    "            print(task_tokenizer_path)\n",
    "            if task_tokenizer_path.exists():\n",
    "                for hyperparameter_path in glob(str(task_tokenizer_path) + \"/*\", recursive=True):\n",
    "                    scores_path = hyperparameter_path+\"/scores.csv\"\n",
    "                    if not Path(scores_path).is_file() or not \"seed\" in str(scores_path):\n",
    "                        continue\n",
    "                    print(scores_path)\n",
    "                    new_score_df = pd.read_csv(scores_path)\n",
    "                    if list(new_score_df.task_type)[0] == \"classification\":\n",
    "                        classification_scores.append(new_score_df)\n",
    "                    else:\n",
    "                        regression_scores.append(new_score_df)\n",
    "\n",
    "regression_scores = pd.concat(regression_scores, axis = 0, sort = False)\n",
    "classification_scores = pd.concat(classification_scores, axis = 0, sort = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cce8ea1",
   "metadata": {},
   "source": [
    "## Most drilled down regression scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a936e48c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">mean_squared_error</th>\n",
       "      <th colspan=\"2\" halign=\"left\">rectified_mean_squared_error</th>\n",
       "      <th colspan=\"2\" halign=\"left\">mean_absolute_error</th>\n",
       "      <th colspan=\"2\" halign=\"left\">max_error</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>task</th>\n",
       "      <th>embedding</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>dataset</th>\n",
       "      <th>architecture</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"16\" valign=\"top\">bace_regression</th>\n",
       "      <th rowspan=\"8\" valign=\"top\">selfies</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">atom</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">isomers</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.786971</td>\n",
       "      <td>0.276483</td>\n",
       "      <td>0.877657</td>\n",
       "      <td>0.144436</td>\n",
       "      <td>0.689199</td>\n",
       "      <td>0.125782</td>\n",
       "      <td>2.818429</td>\n",
       "      <td>0.254402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>1.359734</td>\n",
       "      <td>0.313976</td>\n",
       "      <td>1.158809</td>\n",
       "      <td>0.145323</td>\n",
       "      <td>0.971504</td>\n",
       "      <td>0.127466</td>\n",
       "      <td>2.967101</td>\n",
       "      <td>0.142712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">standard</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.753125</td>\n",
       "      <td>0.346385</td>\n",
       "      <td>0.852835</td>\n",
       "      <td>0.179575</td>\n",
       "      <td>0.681601</td>\n",
       "      <td>0.176997</td>\n",
       "      <td>2.622514</td>\n",
       "      <td>0.445435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>1.170083</td>\n",
       "      <td>0.475655</td>\n",
       "      <td>1.061266</td>\n",
       "      <td>0.233981</td>\n",
       "      <td>0.867127</td>\n",
       "      <td>0.215934</td>\n",
       "      <td>2.758183</td>\n",
       "      <td>0.261091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">trained</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">isomers</th>\n",
       "      <th>bart</th>\n",
       "      <td>1.355943</td>\n",
       "      <td>0.244263</td>\n",
       "      <td>1.160913</td>\n",
       "      <td>0.101384</td>\n",
       "      <td>0.991640</td>\n",
       "      <td>0.096710</td>\n",
       "      <td>2.845659</td>\n",
       "      <td>0.304203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.838104</td>\n",
       "      <td>0.350459</td>\n",
       "      <td>0.901957</td>\n",
       "      <td>0.175276</td>\n",
       "      <td>0.712529</td>\n",
       "      <td>0.161259</td>\n",
       "      <td>2.569600</td>\n",
       "      <td>0.364987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">standard</th>\n",
       "      <th>bart</th>\n",
       "      <td>1.103250</td>\n",
       "      <td>0.366749</td>\n",
       "      <td>1.039303</td>\n",
       "      <td>0.169927</td>\n",
       "      <td>0.841488</td>\n",
       "      <td>0.172039</td>\n",
       "      <td>2.928223</td>\n",
       "      <td>0.232136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.797037</td>\n",
       "      <td>0.091786</td>\n",
       "      <td>0.891561</td>\n",
       "      <td>0.051923</td>\n",
       "      <td>0.711152</td>\n",
       "      <td>0.035569</td>\n",
       "      <td>2.549835</td>\n",
       "      <td>0.085324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">smiles</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">atom</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">isomers</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.712369</td>\n",
       "      <td>0.126763</td>\n",
       "      <td>0.841561</td>\n",
       "      <td>0.071970</td>\n",
       "      <td>0.669453</td>\n",
       "      <td>0.072434</td>\n",
       "      <td>2.990703</td>\n",
       "      <td>0.246801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>1.198558</td>\n",
       "      <td>0.327136</td>\n",
       "      <td>1.085468</td>\n",
       "      <td>0.159367</td>\n",
       "      <td>0.915948</td>\n",
       "      <td>0.138081</td>\n",
       "      <td>3.070028</td>\n",
       "      <td>0.179106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">standard</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.755532</td>\n",
       "      <td>0.347961</td>\n",
       "      <td>0.853553</td>\n",
       "      <td>0.183641</td>\n",
       "      <td>0.686055</td>\n",
       "      <td>0.169126</td>\n",
       "      <td>2.555069</td>\n",
       "      <td>0.435537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>1.002510</td>\n",
       "      <td>0.257041</td>\n",
       "      <td>0.994824</td>\n",
       "      <td>0.126662</td>\n",
       "      <td>0.807333</td>\n",
       "      <td>0.142281</td>\n",
       "      <td>3.343738</td>\n",
       "      <td>0.261494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">trained</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">isomers</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.819215</td>\n",
       "      <td>0.370427</td>\n",
       "      <td>0.888413</td>\n",
       "      <td>0.193448</td>\n",
       "      <td>0.729093</td>\n",
       "      <td>0.167652</td>\n",
       "      <td>2.465468</td>\n",
       "      <td>0.341288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.996360</td>\n",
       "      <td>0.193166</td>\n",
       "      <td>0.994809</td>\n",
       "      <td>0.091613</td>\n",
       "      <td>0.841788</td>\n",
       "      <td>0.081442</td>\n",
       "      <td>2.925401</td>\n",
       "      <td>0.208056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">standard</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.977231</td>\n",
       "      <td>0.341822</td>\n",
       "      <td>0.977586</td>\n",
       "      <td>0.164153</td>\n",
       "      <td>0.787169</td>\n",
       "      <td>0.172164</td>\n",
       "      <td>2.867970</td>\n",
       "      <td>0.389165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>1.048844</td>\n",
       "      <td>0.213824</td>\n",
       "      <td>1.020273</td>\n",
       "      <td>0.099286</td>\n",
       "      <td>0.851687</td>\n",
       "      <td>0.093186</td>\n",
       "      <td>2.604831</td>\n",
       "      <td>0.139867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"16\" valign=\"top\">clearance</th>\n",
       "      <th rowspan=\"8\" valign=\"top\">selfies</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">atom</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">isomers</th>\n",
       "      <th>bart</th>\n",
       "      <td>1.344785</td>\n",
       "      <td>0.076164</td>\n",
       "      <td>1.159278</td>\n",
       "      <td>0.032782</td>\n",
       "      <td>0.965775</td>\n",
       "      <td>0.044863</td>\n",
       "      <td>2.647645</td>\n",
       "      <td>0.147256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>1.548508</td>\n",
       "      <td>0.258403</td>\n",
       "      <td>1.241049</td>\n",
       "      <td>0.101897</td>\n",
       "      <td>0.974175</td>\n",
       "      <td>0.085363</td>\n",
       "      <td>3.284540</td>\n",
       "      <td>0.305033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">standard</th>\n",
       "      <th>bart</th>\n",
       "      <td>1.398488</td>\n",
       "      <td>0.053404</td>\n",
       "      <td>1.182401</td>\n",
       "      <td>0.022768</td>\n",
       "      <td>0.941316</td>\n",
       "      <td>0.038341</td>\n",
       "      <td>2.898177</td>\n",
       "      <td>0.139639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>1.439550</td>\n",
       "      <td>0.044387</td>\n",
       "      <td>1.199697</td>\n",
       "      <td>0.018571</td>\n",
       "      <td>0.941772</td>\n",
       "      <td>0.025064</td>\n",
       "      <td>3.046670</td>\n",
       "      <td>0.189894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">trained</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">isomers</th>\n",
       "      <th>bart</th>\n",
       "      <td>1.381264</td>\n",
       "      <td>0.059949</td>\n",
       "      <td>1.175050</td>\n",
       "      <td>0.025553</td>\n",
       "      <td>1.010956</td>\n",
       "      <td>0.033630</td>\n",
       "      <td>2.670256</td>\n",
       "      <td>0.193537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>1.458298</td>\n",
       "      <td>0.146625</td>\n",
       "      <td>1.206451</td>\n",
       "      <td>0.058895</td>\n",
       "      <td>0.975965</td>\n",
       "      <td>0.053204</td>\n",
       "      <td>2.851547</td>\n",
       "      <td>0.355995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">standard</th>\n",
       "      <th>bart</th>\n",
       "      <td>1.547177</td>\n",
       "      <td>0.099052</td>\n",
       "      <td>1.243356</td>\n",
       "      <td>0.039430</td>\n",
       "      <td>1.084230</td>\n",
       "      <td>0.038796</td>\n",
       "      <td>2.456965</td>\n",
       "      <td>0.155522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>1.544536</td>\n",
       "      <td>0.134513</td>\n",
       "      <td>1.241873</td>\n",
       "      <td>0.053488</td>\n",
       "      <td>0.978434</td>\n",
       "      <td>0.025373</td>\n",
       "      <td>3.214173</td>\n",
       "      <td>0.185720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">smiles</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">atom</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">isomers</th>\n",
       "      <th>bart</th>\n",
       "      <td>1.311405</td>\n",
       "      <td>0.059747</td>\n",
       "      <td>1.144925</td>\n",
       "      <td>0.026257</td>\n",
       "      <td>0.951305</td>\n",
       "      <td>0.025382</td>\n",
       "      <td>2.742291</td>\n",
       "      <td>0.157856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>1.467083</td>\n",
       "      <td>0.191580</td>\n",
       "      <td>1.209102</td>\n",
       "      <td>0.080274</td>\n",
       "      <td>0.955737</td>\n",
       "      <td>0.045181</td>\n",
       "      <td>3.273012</td>\n",
       "      <td>0.400013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">standard</th>\n",
       "      <th>bart</th>\n",
       "      <td>1.397299</td>\n",
       "      <td>0.035763</td>\n",
       "      <td>1.181997</td>\n",
       "      <td>0.015147</td>\n",
       "      <td>0.961960</td>\n",
       "      <td>0.027806</td>\n",
       "      <td>2.916118</td>\n",
       "      <td>0.156096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>1.372596</td>\n",
       "      <td>0.069919</td>\n",
       "      <td>1.171275</td>\n",
       "      <td>0.029827</td>\n",
       "      <td>0.900661</td>\n",
       "      <td>0.014858</td>\n",
       "      <td>3.152436</td>\n",
       "      <td>0.118784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">trained</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">isomers</th>\n",
       "      <th>bart</th>\n",
       "      <td>1.374806</td>\n",
       "      <td>0.076457</td>\n",
       "      <td>1.172163</td>\n",
       "      <td>0.032415</td>\n",
       "      <td>0.989748</td>\n",
       "      <td>0.024064</td>\n",
       "      <td>2.660344</td>\n",
       "      <td>0.155312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>1.512769</td>\n",
       "      <td>0.100633</td>\n",
       "      <td>1.229402</td>\n",
       "      <td>0.040930</td>\n",
       "      <td>1.035953</td>\n",
       "      <td>0.041584</td>\n",
       "      <td>2.821044</td>\n",
       "      <td>0.654429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">standard</th>\n",
       "      <th>bart</th>\n",
       "      <td>1.286692</td>\n",
       "      <td>0.097940</td>\n",
       "      <td>1.133671</td>\n",
       "      <td>0.043050</td>\n",
       "      <td>0.916472</td>\n",
       "      <td>0.042414</td>\n",
       "      <td>2.796895</td>\n",
       "      <td>0.239002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>1.394202</td>\n",
       "      <td>0.183811</td>\n",
       "      <td>1.178802</td>\n",
       "      <td>0.076057</td>\n",
       "      <td>0.978314</td>\n",
       "      <td>0.084018</td>\n",
       "      <td>2.664665</td>\n",
       "      <td>0.247127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"16\" valign=\"top\">delaney</th>\n",
       "      <th rowspan=\"8\" valign=\"top\">selfies</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">atom</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">isomers</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.334829</td>\n",
       "      <td>0.019311</td>\n",
       "      <td>0.578449</td>\n",
       "      <td>0.016774</td>\n",
       "      <td>0.476101</td>\n",
       "      <td>0.023650</td>\n",
       "      <td>1.615695</td>\n",
       "      <td>0.314359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.230902</td>\n",
       "      <td>0.029525</td>\n",
       "      <td>0.479770</td>\n",
       "      <td>0.030047</td>\n",
       "      <td>0.384001</td>\n",
       "      <td>0.026380</td>\n",
       "      <td>1.278504</td>\n",
       "      <td>0.051528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">standard</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.370367</td>\n",
       "      <td>0.044360</td>\n",
       "      <td>0.607719</td>\n",
       "      <td>0.036137</td>\n",
       "      <td>0.464355</td>\n",
       "      <td>0.024401</td>\n",
       "      <td>2.318580</td>\n",
       "      <td>0.063577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.206493</td>\n",
       "      <td>0.014987</td>\n",
       "      <td>0.454173</td>\n",
       "      <td>0.016584</td>\n",
       "      <td>0.359215</td>\n",
       "      <td>0.016859</td>\n",
       "      <td>1.334085</td>\n",
       "      <td>0.069381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">trained</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">isomers</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.257681</td>\n",
       "      <td>0.026944</td>\n",
       "      <td>0.507076</td>\n",
       "      <td>0.026346</td>\n",
       "      <td>0.399338</td>\n",
       "      <td>0.022187</td>\n",
       "      <td>1.369326</td>\n",
       "      <td>0.046431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.262989</td>\n",
       "      <td>0.015021</td>\n",
       "      <td>0.512655</td>\n",
       "      <td>0.014705</td>\n",
       "      <td>0.401011</td>\n",
       "      <td>0.007564</td>\n",
       "      <td>1.436678</td>\n",
       "      <td>0.132512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">standard</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.279993</td>\n",
       "      <td>0.023264</td>\n",
       "      <td>0.528791</td>\n",
       "      <td>0.021613</td>\n",
       "      <td>0.407134</td>\n",
       "      <td>0.019584</td>\n",
       "      <td>1.584147</td>\n",
       "      <td>0.116719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.263296</td>\n",
       "      <td>0.026220</td>\n",
       "      <td>0.512633</td>\n",
       "      <td>0.025103</td>\n",
       "      <td>0.411455</td>\n",
       "      <td>0.021416</td>\n",
       "      <td>1.319822</td>\n",
       "      <td>0.093416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">smiles</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">atom</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">isomers</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.273190</td>\n",
       "      <td>0.057182</td>\n",
       "      <td>0.520487</td>\n",
       "      <td>0.053426</td>\n",
       "      <td>0.420962</td>\n",
       "      <td>0.062022</td>\n",
       "      <td>1.617774</td>\n",
       "      <td>0.287974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.209632</td>\n",
       "      <td>0.017614</td>\n",
       "      <td>0.457516</td>\n",
       "      <td>0.019725</td>\n",
       "      <td>0.367689</td>\n",
       "      <td>0.020191</td>\n",
       "      <td>1.371778</td>\n",
       "      <td>0.092567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">standard</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.209411</td>\n",
       "      <td>0.018480</td>\n",
       "      <td>0.457241</td>\n",
       "      <td>0.020652</td>\n",
       "      <td>0.348388</td>\n",
       "      <td>0.017113</td>\n",
       "      <td>1.348211</td>\n",
       "      <td>0.112810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.219756</td>\n",
       "      <td>0.021303</td>\n",
       "      <td>0.468351</td>\n",
       "      <td>0.022451</td>\n",
       "      <td>0.364284</td>\n",
       "      <td>0.019365</td>\n",
       "      <td>1.417366</td>\n",
       "      <td>0.104908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">trained</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">isomers</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.303501</td>\n",
       "      <td>0.032714</td>\n",
       "      <td>0.550300</td>\n",
       "      <td>0.028958</td>\n",
       "      <td>0.443701</td>\n",
       "      <td>0.025588</td>\n",
       "      <td>1.618038</td>\n",
       "      <td>0.117965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.267977</td>\n",
       "      <td>0.006737</td>\n",
       "      <td>0.517632</td>\n",
       "      <td>0.006541</td>\n",
       "      <td>0.392173</td>\n",
       "      <td>0.010071</td>\n",
       "      <td>1.687406</td>\n",
       "      <td>0.137057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">standard</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.310719</td>\n",
       "      <td>0.054945</td>\n",
       "      <td>0.555837</td>\n",
       "      <td>0.046961</td>\n",
       "      <td>0.437595</td>\n",
       "      <td>0.037137</td>\n",
       "      <td>1.681966</td>\n",
       "      <td>0.164298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.258378</td>\n",
       "      <td>0.019635</td>\n",
       "      <td>0.508014</td>\n",
       "      <td>0.019338</td>\n",
       "      <td>0.398779</td>\n",
       "      <td>0.018774</td>\n",
       "      <td>1.351382</td>\n",
       "      <td>0.076835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"16\" valign=\"top\">lipo</th>\n",
       "      <th rowspan=\"8\" valign=\"top\">selfies</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">atom</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">isomers</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.492019</td>\n",
       "      <td>0.026963</td>\n",
       "      <td>0.701234</td>\n",
       "      <td>0.019004</td>\n",
       "      <td>0.547974</td>\n",
       "      <td>0.020319</td>\n",
       "      <td>2.211290</td>\n",
       "      <td>0.078074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.535272</td>\n",
       "      <td>0.032559</td>\n",
       "      <td>0.731349</td>\n",
       "      <td>0.022355</td>\n",
       "      <td>0.584473</td>\n",
       "      <td>0.021421</td>\n",
       "      <td>2.383615</td>\n",
       "      <td>0.197322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">standard</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.460063</td>\n",
       "      <td>0.021132</td>\n",
       "      <td>0.678136</td>\n",
       "      <td>0.015635</td>\n",
       "      <td>0.529645</td>\n",
       "      <td>0.016235</td>\n",
       "      <td>2.214204</td>\n",
       "      <td>0.021996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.480426</td>\n",
       "      <td>0.024968</td>\n",
       "      <td>0.692941</td>\n",
       "      <td>0.017976</td>\n",
       "      <td>0.541707</td>\n",
       "      <td>0.013526</td>\n",
       "      <td>2.417035</td>\n",
       "      <td>0.176842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">trained</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">isomers</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.449830</td>\n",
       "      <td>0.015953</td>\n",
       "      <td>0.670608</td>\n",
       "      <td>0.011964</td>\n",
       "      <td>0.534497</td>\n",
       "      <td>0.010023</td>\n",
       "      <td>2.141823</td>\n",
       "      <td>0.033489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.539478</td>\n",
       "      <td>0.014895</td>\n",
       "      <td>0.734436</td>\n",
       "      <td>0.010148</td>\n",
       "      <td>0.583565</td>\n",
       "      <td>0.009324</td>\n",
       "      <td>2.425444</td>\n",
       "      <td>0.160697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">standard</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.497150</td>\n",
       "      <td>0.010376</td>\n",
       "      <td>0.705058</td>\n",
       "      <td>0.007326</td>\n",
       "      <td>0.551315</td>\n",
       "      <td>0.008718</td>\n",
       "      <td>2.480120</td>\n",
       "      <td>0.106002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.566011</td>\n",
       "      <td>0.061824</td>\n",
       "      <td>0.751485</td>\n",
       "      <td>0.040038</td>\n",
       "      <td>0.591622</td>\n",
       "      <td>0.038624</td>\n",
       "      <td>2.485460</td>\n",
       "      <td>0.224466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">smiles</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">atom</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">isomers</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.451691</td>\n",
       "      <td>0.015798</td>\n",
       "      <td>0.671998</td>\n",
       "      <td>0.011690</td>\n",
       "      <td>0.515958</td>\n",
       "      <td>0.010700</td>\n",
       "      <td>2.514431</td>\n",
       "      <td>0.068333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.452740</td>\n",
       "      <td>0.009964</td>\n",
       "      <td>0.672827</td>\n",
       "      <td>0.007407</td>\n",
       "      <td>0.524973</td>\n",
       "      <td>0.005963</td>\n",
       "      <td>2.695077</td>\n",
       "      <td>0.319990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">standard</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.459242</td>\n",
       "      <td>0.028014</td>\n",
       "      <td>0.677416</td>\n",
       "      <td>0.020915</td>\n",
       "      <td>0.525496</td>\n",
       "      <td>0.011776</td>\n",
       "      <td>2.578259</td>\n",
       "      <td>0.277289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.426782</td>\n",
       "      <td>0.019454</td>\n",
       "      <td>0.653151</td>\n",
       "      <td>0.014822</td>\n",
       "      <td>0.508333</td>\n",
       "      <td>0.010764</td>\n",
       "      <td>2.381369</td>\n",
       "      <td>0.189437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">trained</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">isomers</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.474853</td>\n",
       "      <td>0.014784</td>\n",
       "      <td>0.689029</td>\n",
       "      <td>0.010700</td>\n",
       "      <td>0.536575</td>\n",
       "      <td>0.010692</td>\n",
       "      <td>2.410555</td>\n",
       "      <td>0.183092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.521792</td>\n",
       "      <td>0.021151</td>\n",
       "      <td>0.722232</td>\n",
       "      <td>0.014722</td>\n",
       "      <td>0.566769</td>\n",
       "      <td>0.008060</td>\n",
       "      <td>2.301848</td>\n",
       "      <td>0.116237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">standard</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.487984</td>\n",
       "      <td>0.016729</td>\n",
       "      <td>0.698476</td>\n",
       "      <td>0.012019</td>\n",
       "      <td>0.546165</td>\n",
       "      <td>0.010160</td>\n",
       "      <td>2.311626</td>\n",
       "      <td>0.094765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.507740</td>\n",
       "      <td>0.031113</td>\n",
       "      <td>0.712296</td>\n",
       "      <td>0.021640</td>\n",
       "      <td>0.563517</td>\n",
       "      <td>0.017364</td>\n",
       "      <td>2.351879</td>\n",
       "      <td>0.166583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                          mean_squared_error  \\\n",
       "                                                                        mean   \n",
       "task            embedding tokenizer dataset  architecture                      \n",
       "bace_regression selfies   atom      isomers  bart                   0.786971   \n",
       "                                             roberta                1.359734   \n",
       "                                    standard bart                   0.753125   \n",
       "                                             roberta                1.170083   \n",
       "                          trained   isomers  bart                   1.355943   \n",
       "                                             roberta                0.838104   \n",
       "                                    standard bart                   1.103250   \n",
       "                                             roberta                0.797037   \n",
       "                smiles    atom      isomers  bart                   0.712369   \n",
       "                                             roberta                1.198558   \n",
       "                                    standard bart                   0.755532   \n",
       "                                             roberta                1.002510   \n",
       "                          trained   isomers  bart                   0.819215   \n",
       "                                             roberta                0.996360   \n",
       "                                    standard bart                   0.977231   \n",
       "                                             roberta                1.048844   \n",
       "clearance       selfies   atom      isomers  bart                   1.344785   \n",
       "                                             roberta                1.548508   \n",
       "                                    standard bart                   1.398488   \n",
       "                                             roberta                1.439550   \n",
       "                          trained   isomers  bart                   1.381264   \n",
       "                                             roberta                1.458298   \n",
       "                                    standard bart                   1.547177   \n",
       "                                             roberta                1.544536   \n",
       "                smiles    atom      isomers  bart                   1.311405   \n",
       "                                             roberta                1.467083   \n",
       "                                    standard bart                   1.397299   \n",
       "                                             roberta                1.372596   \n",
       "                          trained   isomers  bart                   1.374806   \n",
       "                                             roberta                1.512769   \n",
       "                                    standard bart                   1.286692   \n",
       "                                             roberta                1.394202   \n",
       "delaney         selfies   atom      isomers  bart                   0.334829   \n",
       "                                             roberta                0.230902   \n",
       "                                    standard bart                   0.370367   \n",
       "                                             roberta                0.206493   \n",
       "                          trained   isomers  bart                   0.257681   \n",
       "                                             roberta                0.262989   \n",
       "                                    standard bart                   0.279993   \n",
       "                                             roberta                0.263296   \n",
       "                smiles    atom      isomers  bart                   0.273190   \n",
       "                                             roberta                0.209632   \n",
       "                                    standard bart                   0.209411   \n",
       "                                             roberta                0.219756   \n",
       "                          trained   isomers  bart                   0.303501   \n",
       "                                             roberta                0.267977   \n",
       "                                    standard bart                   0.310719   \n",
       "                                             roberta                0.258378   \n",
       "lipo            selfies   atom      isomers  bart                   0.492019   \n",
       "                                             roberta                0.535272   \n",
       "                                    standard bart                   0.460063   \n",
       "                                             roberta                0.480426   \n",
       "                          trained   isomers  bart                   0.449830   \n",
       "                                             roberta                0.539478   \n",
       "                                    standard bart                   0.497150   \n",
       "                                             roberta                0.566011   \n",
       "                smiles    atom      isomers  bart                   0.451691   \n",
       "                                             roberta                0.452740   \n",
       "                                    standard bart                   0.459242   \n",
       "                                             roberta                0.426782   \n",
       "                          trained   isomers  bart                   0.474853   \n",
       "                                             roberta                0.521792   \n",
       "                                    standard bart                   0.487984   \n",
       "                                             roberta                0.507740   \n",
       "\n",
       "                                                                     \\\n",
       "                                                                std   \n",
       "task            embedding tokenizer dataset  architecture             \n",
       "bace_regression selfies   atom      isomers  bart          0.276483   \n",
       "                                             roberta       0.313976   \n",
       "                                    standard bart          0.346385   \n",
       "                                             roberta       0.475655   \n",
       "                          trained   isomers  bart          0.244263   \n",
       "                                             roberta       0.350459   \n",
       "                                    standard bart          0.366749   \n",
       "                                             roberta       0.091786   \n",
       "                smiles    atom      isomers  bart          0.126763   \n",
       "                                             roberta       0.327136   \n",
       "                                    standard bart          0.347961   \n",
       "                                             roberta       0.257041   \n",
       "                          trained   isomers  bart          0.370427   \n",
       "                                             roberta       0.193166   \n",
       "                                    standard bart          0.341822   \n",
       "                                             roberta       0.213824   \n",
       "clearance       selfies   atom      isomers  bart          0.076164   \n",
       "                                             roberta       0.258403   \n",
       "                                    standard bart          0.053404   \n",
       "                                             roberta       0.044387   \n",
       "                          trained   isomers  bart          0.059949   \n",
       "                                             roberta       0.146625   \n",
       "                                    standard bart          0.099052   \n",
       "                                             roberta       0.134513   \n",
       "                smiles    atom      isomers  bart          0.059747   \n",
       "                                             roberta       0.191580   \n",
       "                                    standard bart          0.035763   \n",
       "                                             roberta       0.069919   \n",
       "                          trained   isomers  bart          0.076457   \n",
       "                                             roberta       0.100633   \n",
       "                                    standard bart          0.097940   \n",
       "                                             roberta       0.183811   \n",
       "delaney         selfies   atom      isomers  bart          0.019311   \n",
       "                                             roberta       0.029525   \n",
       "                                    standard bart          0.044360   \n",
       "                                             roberta       0.014987   \n",
       "                          trained   isomers  bart          0.026944   \n",
       "                                             roberta       0.015021   \n",
       "                                    standard bart          0.023264   \n",
       "                                             roberta       0.026220   \n",
       "                smiles    atom      isomers  bart          0.057182   \n",
       "                                             roberta       0.017614   \n",
       "                                    standard bart          0.018480   \n",
       "                                             roberta       0.021303   \n",
       "                          trained   isomers  bart          0.032714   \n",
       "                                             roberta       0.006737   \n",
       "                                    standard bart          0.054945   \n",
       "                                             roberta       0.019635   \n",
       "lipo            selfies   atom      isomers  bart          0.026963   \n",
       "                                             roberta       0.032559   \n",
       "                                    standard bart          0.021132   \n",
       "                                             roberta       0.024968   \n",
       "                          trained   isomers  bart          0.015953   \n",
       "                                             roberta       0.014895   \n",
       "                                    standard bart          0.010376   \n",
       "                                             roberta       0.061824   \n",
       "                smiles    atom      isomers  bart          0.015798   \n",
       "                                             roberta       0.009964   \n",
       "                                    standard bart          0.028014   \n",
       "                                             roberta       0.019454   \n",
       "                          trained   isomers  bart          0.014784   \n",
       "                                             roberta       0.021151   \n",
       "                                    standard bart          0.016729   \n",
       "                                             roberta       0.031113   \n",
       "\n",
       "                                                          rectified_mean_squared_error  \\\n",
       "                                                                                  mean   \n",
       "task            embedding tokenizer dataset  architecture                                \n",
       "bace_regression selfies   atom      isomers  bart                             0.877657   \n",
       "                                             roberta                          1.158809   \n",
       "                                    standard bart                             0.852835   \n",
       "                                             roberta                          1.061266   \n",
       "                          trained   isomers  bart                             1.160913   \n",
       "                                             roberta                          0.901957   \n",
       "                                    standard bart                             1.039303   \n",
       "                                             roberta                          0.891561   \n",
       "                smiles    atom      isomers  bart                             0.841561   \n",
       "                                             roberta                          1.085468   \n",
       "                                    standard bart                             0.853553   \n",
       "                                             roberta                          0.994824   \n",
       "                          trained   isomers  bart                             0.888413   \n",
       "                                             roberta                          0.994809   \n",
       "                                    standard bart                             0.977586   \n",
       "                                             roberta                          1.020273   \n",
       "clearance       selfies   atom      isomers  bart                             1.159278   \n",
       "                                             roberta                          1.241049   \n",
       "                                    standard bart                             1.182401   \n",
       "                                             roberta                          1.199697   \n",
       "                          trained   isomers  bart                             1.175050   \n",
       "                                             roberta                          1.206451   \n",
       "                                    standard bart                             1.243356   \n",
       "                                             roberta                          1.241873   \n",
       "                smiles    atom      isomers  bart                             1.144925   \n",
       "                                             roberta                          1.209102   \n",
       "                                    standard bart                             1.181997   \n",
       "                                             roberta                          1.171275   \n",
       "                          trained   isomers  bart                             1.172163   \n",
       "                                             roberta                          1.229402   \n",
       "                                    standard bart                             1.133671   \n",
       "                                             roberta                          1.178802   \n",
       "delaney         selfies   atom      isomers  bart                             0.578449   \n",
       "                                             roberta                          0.479770   \n",
       "                                    standard bart                             0.607719   \n",
       "                                             roberta                          0.454173   \n",
       "                          trained   isomers  bart                             0.507076   \n",
       "                                             roberta                          0.512655   \n",
       "                                    standard bart                             0.528791   \n",
       "                                             roberta                          0.512633   \n",
       "                smiles    atom      isomers  bart                             0.520487   \n",
       "                                             roberta                          0.457516   \n",
       "                                    standard bart                             0.457241   \n",
       "                                             roberta                          0.468351   \n",
       "                          trained   isomers  bart                             0.550300   \n",
       "                                             roberta                          0.517632   \n",
       "                                    standard bart                             0.555837   \n",
       "                                             roberta                          0.508014   \n",
       "lipo            selfies   atom      isomers  bart                             0.701234   \n",
       "                                             roberta                          0.731349   \n",
       "                                    standard bart                             0.678136   \n",
       "                                             roberta                          0.692941   \n",
       "                          trained   isomers  bart                             0.670608   \n",
       "                                             roberta                          0.734436   \n",
       "                                    standard bart                             0.705058   \n",
       "                                             roberta                          0.751485   \n",
       "                smiles    atom      isomers  bart                             0.671998   \n",
       "                                             roberta                          0.672827   \n",
       "                                    standard bart                             0.677416   \n",
       "                                             roberta                          0.653151   \n",
       "                          trained   isomers  bart                             0.689029   \n",
       "                                             roberta                          0.722232   \n",
       "                                    standard bart                             0.698476   \n",
       "                                             roberta                          0.712296   \n",
       "\n",
       "                                                                     \\\n",
       "                                                                std   \n",
       "task            embedding tokenizer dataset  architecture             \n",
       "bace_regression selfies   atom      isomers  bart          0.144436   \n",
       "                                             roberta       0.145323   \n",
       "                                    standard bart          0.179575   \n",
       "                                             roberta       0.233981   \n",
       "                          trained   isomers  bart          0.101384   \n",
       "                                             roberta       0.175276   \n",
       "                                    standard bart          0.169927   \n",
       "                                             roberta       0.051923   \n",
       "                smiles    atom      isomers  bart          0.071970   \n",
       "                                             roberta       0.159367   \n",
       "                                    standard bart          0.183641   \n",
       "                                             roberta       0.126662   \n",
       "                          trained   isomers  bart          0.193448   \n",
       "                                             roberta       0.091613   \n",
       "                                    standard bart          0.164153   \n",
       "                                             roberta       0.099286   \n",
       "clearance       selfies   atom      isomers  bart          0.032782   \n",
       "                                             roberta       0.101897   \n",
       "                                    standard bart          0.022768   \n",
       "                                             roberta       0.018571   \n",
       "                          trained   isomers  bart          0.025553   \n",
       "                                             roberta       0.058895   \n",
       "                                    standard bart          0.039430   \n",
       "                                             roberta       0.053488   \n",
       "                smiles    atom      isomers  bart          0.026257   \n",
       "                                             roberta       0.080274   \n",
       "                                    standard bart          0.015147   \n",
       "                                             roberta       0.029827   \n",
       "                          trained   isomers  bart          0.032415   \n",
       "                                             roberta       0.040930   \n",
       "                                    standard bart          0.043050   \n",
       "                                             roberta       0.076057   \n",
       "delaney         selfies   atom      isomers  bart          0.016774   \n",
       "                                             roberta       0.030047   \n",
       "                                    standard bart          0.036137   \n",
       "                                             roberta       0.016584   \n",
       "                          trained   isomers  bart          0.026346   \n",
       "                                             roberta       0.014705   \n",
       "                                    standard bart          0.021613   \n",
       "                                             roberta       0.025103   \n",
       "                smiles    atom      isomers  bart          0.053426   \n",
       "                                             roberta       0.019725   \n",
       "                                    standard bart          0.020652   \n",
       "                                             roberta       0.022451   \n",
       "                          trained   isomers  bart          0.028958   \n",
       "                                             roberta       0.006541   \n",
       "                                    standard bart          0.046961   \n",
       "                                             roberta       0.019338   \n",
       "lipo            selfies   atom      isomers  bart          0.019004   \n",
       "                                             roberta       0.022355   \n",
       "                                    standard bart          0.015635   \n",
       "                                             roberta       0.017976   \n",
       "                          trained   isomers  bart          0.011964   \n",
       "                                             roberta       0.010148   \n",
       "                                    standard bart          0.007326   \n",
       "                                             roberta       0.040038   \n",
       "                smiles    atom      isomers  bart          0.011690   \n",
       "                                             roberta       0.007407   \n",
       "                                    standard bart          0.020915   \n",
       "                                             roberta       0.014822   \n",
       "                          trained   isomers  bart          0.010700   \n",
       "                                             roberta       0.014722   \n",
       "                                    standard bart          0.012019   \n",
       "                                             roberta       0.021640   \n",
       "\n",
       "                                                          mean_absolute_error  \\\n",
       "                                                                         mean   \n",
       "task            embedding tokenizer dataset  architecture                       \n",
       "bace_regression selfies   atom      isomers  bart                    0.689199   \n",
       "                                             roberta                 0.971504   \n",
       "                                    standard bart                    0.681601   \n",
       "                                             roberta                 0.867127   \n",
       "                          trained   isomers  bart                    0.991640   \n",
       "                                             roberta                 0.712529   \n",
       "                                    standard bart                    0.841488   \n",
       "                                             roberta                 0.711152   \n",
       "                smiles    atom      isomers  bart                    0.669453   \n",
       "                                             roberta                 0.915948   \n",
       "                                    standard bart                    0.686055   \n",
       "                                             roberta                 0.807333   \n",
       "                          trained   isomers  bart                    0.729093   \n",
       "                                             roberta                 0.841788   \n",
       "                                    standard bart                    0.787169   \n",
       "                                             roberta                 0.851687   \n",
       "clearance       selfies   atom      isomers  bart                    0.965775   \n",
       "                                             roberta                 0.974175   \n",
       "                                    standard bart                    0.941316   \n",
       "                                             roberta                 0.941772   \n",
       "                          trained   isomers  bart                    1.010956   \n",
       "                                             roberta                 0.975965   \n",
       "                                    standard bart                    1.084230   \n",
       "                                             roberta                 0.978434   \n",
       "                smiles    atom      isomers  bart                    0.951305   \n",
       "                                             roberta                 0.955737   \n",
       "                                    standard bart                    0.961960   \n",
       "                                             roberta                 0.900661   \n",
       "                          trained   isomers  bart                    0.989748   \n",
       "                                             roberta                 1.035953   \n",
       "                                    standard bart                    0.916472   \n",
       "                                             roberta                 0.978314   \n",
       "delaney         selfies   atom      isomers  bart                    0.476101   \n",
       "                                             roberta                 0.384001   \n",
       "                                    standard bart                    0.464355   \n",
       "                                             roberta                 0.359215   \n",
       "                          trained   isomers  bart                    0.399338   \n",
       "                                             roberta                 0.401011   \n",
       "                                    standard bart                    0.407134   \n",
       "                                             roberta                 0.411455   \n",
       "                smiles    atom      isomers  bart                    0.420962   \n",
       "                                             roberta                 0.367689   \n",
       "                                    standard bart                    0.348388   \n",
       "                                             roberta                 0.364284   \n",
       "                          trained   isomers  bart                    0.443701   \n",
       "                                             roberta                 0.392173   \n",
       "                                    standard bart                    0.437595   \n",
       "                                             roberta                 0.398779   \n",
       "lipo            selfies   atom      isomers  bart                    0.547974   \n",
       "                                             roberta                 0.584473   \n",
       "                                    standard bart                    0.529645   \n",
       "                                             roberta                 0.541707   \n",
       "                          trained   isomers  bart                    0.534497   \n",
       "                                             roberta                 0.583565   \n",
       "                                    standard bart                    0.551315   \n",
       "                                             roberta                 0.591622   \n",
       "                smiles    atom      isomers  bart                    0.515958   \n",
       "                                             roberta                 0.524973   \n",
       "                                    standard bart                    0.525496   \n",
       "                                             roberta                 0.508333   \n",
       "                          trained   isomers  bart                    0.536575   \n",
       "                                             roberta                 0.566769   \n",
       "                                    standard bart                    0.546165   \n",
       "                                             roberta                 0.563517   \n",
       "\n",
       "                                                                    max_error  \\\n",
       "                                                                std      mean   \n",
       "task            embedding tokenizer dataset  architecture                       \n",
       "bace_regression selfies   atom      isomers  bart          0.125782  2.818429   \n",
       "                                             roberta       0.127466  2.967101   \n",
       "                                    standard bart          0.176997  2.622514   \n",
       "                                             roberta       0.215934  2.758183   \n",
       "                          trained   isomers  bart          0.096710  2.845659   \n",
       "                                             roberta       0.161259  2.569600   \n",
       "                                    standard bart          0.172039  2.928223   \n",
       "                                             roberta       0.035569  2.549835   \n",
       "                smiles    atom      isomers  bart          0.072434  2.990703   \n",
       "                                             roberta       0.138081  3.070028   \n",
       "                                    standard bart          0.169126  2.555069   \n",
       "                                             roberta       0.142281  3.343738   \n",
       "                          trained   isomers  bart          0.167652  2.465468   \n",
       "                                             roberta       0.081442  2.925401   \n",
       "                                    standard bart          0.172164  2.867970   \n",
       "                                             roberta       0.093186  2.604831   \n",
       "clearance       selfies   atom      isomers  bart          0.044863  2.647645   \n",
       "                                             roberta       0.085363  3.284540   \n",
       "                                    standard bart          0.038341  2.898177   \n",
       "                                             roberta       0.025064  3.046670   \n",
       "                          trained   isomers  bart          0.033630  2.670256   \n",
       "                                             roberta       0.053204  2.851547   \n",
       "                                    standard bart          0.038796  2.456965   \n",
       "                                             roberta       0.025373  3.214173   \n",
       "                smiles    atom      isomers  bart          0.025382  2.742291   \n",
       "                                             roberta       0.045181  3.273012   \n",
       "                                    standard bart          0.027806  2.916118   \n",
       "                                             roberta       0.014858  3.152436   \n",
       "                          trained   isomers  bart          0.024064  2.660344   \n",
       "                                             roberta       0.041584  2.821044   \n",
       "                                    standard bart          0.042414  2.796895   \n",
       "                                             roberta       0.084018  2.664665   \n",
       "delaney         selfies   atom      isomers  bart          0.023650  1.615695   \n",
       "                                             roberta       0.026380  1.278504   \n",
       "                                    standard bart          0.024401  2.318580   \n",
       "                                             roberta       0.016859  1.334085   \n",
       "                          trained   isomers  bart          0.022187  1.369326   \n",
       "                                             roberta       0.007564  1.436678   \n",
       "                                    standard bart          0.019584  1.584147   \n",
       "                                             roberta       0.021416  1.319822   \n",
       "                smiles    atom      isomers  bart          0.062022  1.617774   \n",
       "                                             roberta       0.020191  1.371778   \n",
       "                                    standard bart          0.017113  1.348211   \n",
       "                                             roberta       0.019365  1.417366   \n",
       "                          trained   isomers  bart          0.025588  1.618038   \n",
       "                                             roberta       0.010071  1.687406   \n",
       "                                    standard bart          0.037137  1.681966   \n",
       "                                             roberta       0.018774  1.351382   \n",
       "lipo            selfies   atom      isomers  bart          0.020319  2.211290   \n",
       "                                             roberta       0.021421  2.383615   \n",
       "                                    standard bart          0.016235  2.214204   \n",
       "                                             roberta       0.013526  2.417035   \n",
       "                          trained   isomers  bart          0.010023  2.141823   \n",
       "                                             roberta       0.009324  2.425444   \n",
       "                                    standard bart          0.008718  2.480120   \n",
       "                                             roberta       0.038624  2.485460   \n",
       "                smiles    atom      isomers  bart          0.010700  2.514431   \n",
       "                                             roberta       0.005963  2.695077   \n",
       "                                    standard bart          0.011776  2.578259   \n",
       "                                             roberta       0.010764  2.381369   \n",
       "                          trained   isomers  bart          0.010692  2.410555   \n",
       "                                             roberta       0.008060  2.301848   \n",
       "                                    standard bart          0.010160  2.311626   \n",
       "                                             roberta       0.017364  2.351879   \n",
       "\n",
       "                                                                     \n",
       "                                                                std  \n",
       "task            embedding tokenizer dataset  architecture            \n",
       "bace_regression selfies   atom      isomers  bart          0.254402  \n",
       "                                             roberta       0.142712  \n",
       "                                    standard bart          0.445435  \n",
       "                                             roberta       0.261091  \n",
       "                          trained   isomers  bart          0.304203  \n",
       "                                             roberta       0.364987  \n",
       "                                    standard bart          0.232136  \n",
       "                                             roberta       0.085324  \n",
       "                smiles    atom      isomers  bart          0.246801  \n",
       "                                             roberta       0.179106  \n",
       "                                    standard bart          0.435537  \n",
       "                                             roberta       0.261494  \n",
       "                          trained   isomers  bart          0.341288  \n",
       "                                             roberta       0.208056  \n",
       "                                    standard bart          0.389165  \n",
       "                                             roberta       0.139867  \n",
       "clearance       selfies   atom      isomers  bart          0.147256  \n",
       "                                             roberta       0.305033  \n",
       "                                    standard bart          0.139639  \n",
       "                                             roberta       0.189894  \n",
       "                          trained   isomers  bart          0.193537  \n",
       "                                             roberta       0.355995  \n",
       "                                    standard bart          0.155522  \n",
       "                                             roberta       0.185720  \n",
       "                smiles    atom      isomers  bart          0.157856  \n",
       "                                             roberta       0.400013  \n",
       "                                    standard bart          0.156096  \n",
       "                                             roberta       0.118784  \n",
       "                          trained   isomers  bart          0.155312  \n",
       "                                             roberta       0.654429  \n",
       "                                    standard bart          0.239002  \n",
       "                                             roberta       0.247127  \n",
       "delaney         selfies   atom      isomers  bart          0.314359  \n",
       "                                             roberta       0.051528  \n",
       "                                    standard bart          0.063577  \n",
       "                                             roberta       0.069381  \n",
       "                          trained   isomers  bart          0.046431  \n",
       "                                             roberta       0.132512  \n",
       "                                    standard bart          0.116719  \n",
       "                                             roberta       0.093416  \n",
       "                smiles    atom      isomers  bart          0.287974  \n",
       "                                             roberta       0.092567  \n",
       "                                    standard bart          0.112810  \n",
       "                                             roberta       0.104908  \n",
       "                          trained   isomers  bart          0.117965  \n",
       "                                             roberta       0.137057  \n",
       "                                    standard bart          0.164298  \n",
       "                                             roberta       0.076835  \n",
       "lipo            selfies   atom      isomers  bart          0.078074  \n",
       "                                             roberta       0.197322  \n",
       "                                    standard bart          0.021996  \n",
       "                                             roberta       0.176842  \n",
       "                          trained   isomers  bart          0.033489  \n",
       "                                             roberta       0.160697  \n",
       "                                    standard bart          0.106002  \n",
       "                                             roberta       0.224466  \n",
       "                smiles    atom      isomers  bart          0.068333  \n",
       "                                             roberta       0.319990  \n",
       "                                    standard bart          0.277289  \n",
       "                                             roberta       0.189437  \n",
       "                          trained   isomers  bart          0.183092  \n",
       "                                             roberta       0.116237  \n",
       "                                    standard bart          0.094765  \n",
       "                                             roberta       0.166583  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression_columns = list(regression_scores.columns[-2:])\n",
    "regression_columns.extend(regression_scores.columns[:-2])\n",
    "regression_scores[regression_columns].drop([\"Unnamed: 0\", \"task_type\", \"learning_rate\", \"dropout\", \"seed\"], axis=\"columns\").groupby([\"task\", \"embedding\", \"tokenizer\", \"dataset\", \"architecture\"]).agg([\"mean\", \"std\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308a205b",
   "metadata": {},
   "source": [
    "## Compare embeddings in regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da0a89dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_751193/3677303742.py:1: FutureWarning: ['tokenizer', 'dataset', 'architecture'] did not aggregate successfully. If any error is raised this will raise in a future version of pandas. Drop these columns/ops to avoid this warning.\n",
      "  regression_scores[regression_columns].drop([\"Unnamed: 0\", \"task_type\", \"learning_rate\", \"dropout\", \"seed\"], axis=\"columns\").groupby([\"task\", \"embedding\"]).agg([\"mean\", \"std\"])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">mean_squared_error</th>\n",
       "      <th colspan=\"2\" halign=\"left\">rectified_mean_squared_error</th>\n",
       "      <th colspan=\"2\" halign=\"left\">mean_absolute_error</th>\n",
       "      <th colspan=\"2\" halign=\"left\">max_error</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>task</th>\n",
       "      <th>embedding</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">bace_regression</th>\n",
       "      <th>selfies</th>\n",
       "      <td>1.020531</td>\n",
       "      <td>0.382869</td>\n",
       "      <td>0.993038</td>\n",
       "      <td>0.187855</td>\n",
       "      <td>0.808280</td>\n",
       "      <td>0.180687</td>\n",
       "      <td>2.757443</td>\n",
       "      <td>0.297875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smiles</th>\n",
       "      <td>0.938827</td>\n",
       "      <td>0.300737</td>\n",
       "      <td>0.957061</td>\n",
       "      <td>0.153127</td>\n",
       "      <td>0.786066</td>\n",
       "      <td>0.147004</td>\n",
       "      <td>2.852901</td>\n",
       "      <td>0.385366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">clearance</th>\n",
       "      <th>selfies</th>\n",
       "      <td>1.457826</td>\n",
       "      <td>0.138922</td>\n",
       "      <td>1.206144</td>\n",
       "      <td>0.055856</td>\n",
       "      <td>0.984078</td>\n",
       "      <td>0.060879</td>\n",
       "      <td>2.883747</td>\n",
       "      <td>0.338851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smiles</th>\n",
       "      <td>1.389607</td>\n",
       "      <td>0.125627</td>\n",
       "      <td>1.177667</td>\n",
       "      <td>0.052693</td>\n",
       "      <td>0.961269</td>\n",
       "      <td>0.055892</td>\n",
       "      <td>2.878351</td>\n",
       "      <td>0.356105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">delaney</th>\n",
       "      <th>selfies</th>\n",
       "      <td>0.275819</td>\n",
       "      <td>0.055938</td>\n",
       "      <td>0.522658</td>\n",
       "      <td>0.052105</td>\n",
       "      <td>0.412826</td>\n",
       "      <td>0.041666</td>\n",
       "      <td>1.532105</td>\n",
       "      <td>0.346105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smiles</th>\n",
       "      <td>0.256570</td>\n",
       "      <td>0.048626</td>\n",
       "      <td>0.504422</td>\n",
       "      <td>0.046724</td>\n",
       "      <td>0.396696</td>\n",
       "      <td>0.043337</td>\n",
       "      <td>1.511740</td>\n",
       "      <td>0.198711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">lipo</th>\n",
       "      <th>selfies</th>\n",
       "      <td>0.502531</td>\n",
       "      <td>0.047348</td>\n",
       "      <td>0.708156</td>\n",
       "      <td>0.032760</td>\n",
       "      <td>0.558100</td>\n",
       "      <td>0.029331</td>\n",
       "      <td>2.344874</td>\n",
       "      <td>0.182465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smiles</th>\n",
       "      <td>0.472853</td>\n",
       "      <td>0.035348</td>\n",
       "      <td>0.687178</td>\n",
       "      <td>0.025606</td>\n",
       "      <td>0.535973</td>\n",
       "      <td>0.022639</td>\n",
       "      <td>2.443131</td>\n",
       "      <td>0.220767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          mean_squared_error            \\\n",
       "                                        mean       std   \n",
       "task            embedding                                \n",
       "bace_regression selfies             1.020531  0.382869   \n",
       "                smiles              0.938827  0.300737   \n",
       "clearance       selfies             1.457826  0.138922   \n",
       "                smiles              1.389607  0.125627   \n",
       "delaney         selfies             0.275819  0.055938   \n",
       "                smiles              0.256570  0.048626   \n",
       "lipo            selfies             0.502531  0.047348   \n",
       "                smiles              0.472853  0.035348   \n",
       "\n",
       "                          rectified_mean_squared_error            \\\n",
       "                                                  mean       std   \n",
       "task            embedding                                          \n",
       "bace_regression selfies                       0.993038  0.187855   \n",
       "                smiles                        0.957061  0.153127   \n",
       "clearance       selfies                       1.206144  0.055856   \n",
       "                smiles                        1.177667  0.052693   \n",
       "delaney         selfies                       0.522658  0.052105   \n",
       "                smiles                        0.504422  0.046724   \n",
       "lipo            selfies                       0.708156  0.032760   \n",
       "                smiles                        0.687178  0.025606   \n",
       "\n",
       "                          mean_absolute_error           max_error            \n",
       "                                         mean       std      mean       std  \n",
       "task            embedding                                                    \n",
       "bace_regression selfies              0.808280  0.180687  2.757443  0.297875  \n",
       "                smiles               0.786066  0.147004  2.852901  0.385366  \n",
       "clearance       selfies              0.984078  0.060879  2.883747  0.338851  \n",
       "                smiles               0.961269  0.055892  2.878351  0.356105  \n",
       "delaney         selfies              0.412826  0.041666  1.532105  0.346105  \n",
       "                smiles               0.396696  0.043337  1.511740  0.198711  \n",
       "lipo            selfies              0.558100  0.029331  2.344874  0.182465  \n",
       "                smiles               0.535973  0.022639  2.443131  0.220767  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression_scores[regression_columns].drop([\"Unnamed: 0\", \"task_type\", \"learning_rate\", \"dropout\", \"seed\"], axis=\"columns\").groupby([\"task\", \"embedding\"]).agg([\"mean\", \"std\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdf375a",
   "metadata": {},
   "source": [
    "## Compare tokenizers in regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb5c8034",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_751193/757221434.py:1: FutureWarning: ['embedding', 'dataset', 'architecture'] did not aggregate successfully. If any error is raised this will raise in a future version of pandas. Drop these columns/ops to avoid this warning.\n",
      "  regression_scores[regression_columns].drop([\"Unnamed: 0\", \"task_type\", \"learning_rate\", \"dropout\", \"seed\"], axis=\"columns\").groupby([\"task\", \"tokenizer\"]).agg([\"mean\", \"std\"])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">mean_squared_error</th>\n",
       "      <th colspan=\"2\" halign=\"left\">rectified_mean_squared_error</th>\n",
       "      <th colspan=\"2\" halign=\"left\">mean_absolute_error</th>\n",
       "      <th colspan=\"2\" halign=\"left\">max_error</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>task</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">bace_regression</th>\n",
       "      <th>atom</th>\n",
       "      <td>0.967360</td>\n",
       "      <td>0.376090</td>\n",
       "      <td>0.965747</td>\n",
       "      <td>0.188635</td>\n",
       "      <td>0.786028</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>2.890720</td>\n",
       "      <td>0.362113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trained</th>\n",
       "      <td>0.991998</td>\n",
       "      <td>0.314165</td>\n",
       "      <td>0.984352</td>\n",
       "      <td>0.153754</td>\n",
       "      <td>0.808318</td>\n",
       "      <td>0.149562</td>\n",
       "      <td>2.719623</td>\n",
       "      <td>0.309469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">clearance</th>\n",
       "      <th>atom</th>\n",
       "      <td>1.409964</td>\n",
       "      <td>0.133037</td>\n",
       "      <td>1.186215</td>\n",
       "      <td>0.054133</td>\n",
       "      <td>0.949088</td>\n",
       "      <td>0.044784</td>\n",
       "      <td>2.995111</td>\n",
       "      <td>0.300352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trained</th>\n",
       "      <td>1.437468</td>\n",
       "      <td>0.139217</td>\n",
       "      <td>1.197596</td>\n",
       "      <td>0.057580</td>\n",
       "      <td>0.996259</td>\n",
       "      <td>0.062851</td>\n",
       "      <td>2.766986</td>\n",
       "      <td>0.353195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">delaney</th>\n",
       "      <th>atom</th>\n",
       "      <td>0.256822</td>\n",
       "      <td>0.066553</td>\n",
       "      <td>0.502963</td>\n",
       "      <td>0.062842</td>\n",
       "      <td>0.398124</td>\n",
       "      <td>0.054175</td>\n",
       "      <td>1.537749</td>\n",
       "      <td>0.356133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trained</th>\n",
       "      <td>0.275567</td>\n",
       "      <td>0.032773</td>\n",
       "      <td>0.524117</td>\n",
       "      <td>0.029835</td>\n",
       "      <td>0.411398</td>\n",
       "      <td>0.026875</td>\n",
       "      <td>1.506096</td>\n",
       "      <td>0.179285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">lipo</th>\n",
       "      <th>atom</th>\n",
       "      <td>0.469780</td>\n",
       "      <td>0.037663</td>\n",
       "      <td>0.684882</td>\n",
       "      <td>0.027110</td>\n",
       "      <td>0.534820</td>\n",
       "      <td>0.026146</td>\n",
       "      <td>2.424410</td>\n",
       "      <td>0.236294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trained</th>\n",
       "      <td>0.505605</td>\n",
       "      <td>0.043201</td>\n",
       "      <td>0.710452</td>\n",
       "      <td>0.029737</td>\n",
       "      <td>0.559253</td>\n",
       "      <td>0.025182</td>\n",
       "      <td>2.363595</td>\n",
       "      <td>0.171029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          mean_squared_error            \\\n",
       "                                        mean       std   \n",
       "task            tokenizer                                \n",
       "bace_regression atom                0.967360  0.376090   \n",
       "                trained             0.991998  0.314165   \n",
       "clearance       atom                1.409964  0.133037   \n",
       "                trained             1.437468  0.139217   \n",
       "delaney         atom                0.256822  0.066553   \n",
       "                trained             0.275567  0.032773   \n",
       "lipo            atom                0.469780  0.037663   \n",
       "                trained             0.505605  0.043201   \n",
       "\n",
       "                          rectified_mean_squared_error            \\\n",
       "                                                  mean       std   \n",
       "task            tokenizer                                          \n",
       "bace_regression atom                          0.965747  0.188635   \n",
       "                trained                       0.984352  0.153754   \n",
       "clearance       atom                          1.186215  0.054133   \n",
       "                trained                       1.197596  0.057580   \n",
       "delaney         atom                          0.502963  0.062842   \n",
       "                trained                       0.524117  0.029835   \n",
       "lipo            atom                          0.684882  0.027110   \n",
       "                trained                       0.710452  0.029737   \n",
       "\n",
       "                          mean_absolute_error           max_error            \n",
       "                                         mean       std      mean       std  \n",
       "task            tokenizer                                                    \n",
       "bace_regression atom                 0.786028  0.178571  2.890720  0.362113  \n",
       "                trained              0.808318  0.149562  2.719623  0.309469  \n",
       "clearance       atom                 0.949088  0.044784  2.995111  0.300352  \n",
       "                trained              0.996259  0.062851  2.766986  0.353195  \n",
       "delaney         atom                 0.398124  0.054175  1.537749  0.356133  \n",
       "                trained              0.411398  0.026875  1.506096  0.179285  \n",
       "lipo            atom                 0.534820  0.026146  2.424410  0.236294  \n",
       "                trained              0.559253  0.025182  2.363595  0.171029  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression_scores[regression_columns].drop([\"Unnamed: 0\", \"task_type\", \"learning_rate\", \"dropout\", \"seed\"], axis=\"columns\").groupby([\"task\", \"tokenizer\"]).agg([\"mean\", \"std\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a1dd67",
   "metadata": {},
   "source": [
    "## Compare dataset in regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ce241c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_751193/2489997319.py:1: FutureWarning: ['embedding', 'tokenizer', 'architecture'] did not aggregate successfully. If any error is raised this will raise in a future version of pandas. Drop these columns/ops to avoid this warning.\n",
      "  regression_scores[regression_columns].drop([\"Unnamed: 0\", \"task_type\", \"learning_rate\", \"dropout\", \"seed\"], axis=\"columns\").groupby([\"task\", \"dataset\"]).agg([\"mean\", \"std\"])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">mean_squared_error</th>\n",
       "      <th colspan=\"2\" halign=\"left\">rectified_mean_squared_error</th>\n",
       "      <th colspan=\"2\" halign=\"left\">mean_absolute_error</th>\n",
       "      <th colspan=\"2\" halign=\"left\">max_error</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>task</th>\n",
       "      <th>dataset</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">bace_regression</th>\n",
       "      <th>isomers</th>\n",
       "      <td>1.008407</td>\n",
       "      <td>0.359145</td>\n",
       "      <td>0.988698</td>\n",
       "      <td>0.177972</td>\n",
       "      <td>0.815144</td>\n",
       "      <td>0.168962</td>\n",
       "      <td>2.831548</td>\n",
       "      <td>0.312817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>standard</th>\n",
       "      <td>0.950951</td>\n",
       "      <td>0.331325</td>\n",
       "      <td>0.961400</td>\n",
       "      <td>0.165363</td>\n",
       "      <td>0.779201</td>\n",
       "      <td>0.159061</td>\n",
       "      <td>2.778795</td>\n",
       "      <td>0.377661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">clearance</th>\n",
       "      <th>isomers</th>\n",
       "      <td>1.424865</td>\n",
       "      <td>0.148814</td>\n",
       "      <td>1.192177</td>\n",
       "      <td>0.060579</td>\n",
       "      <td>0.982452</td>\n",
       "      <td>0.051146</td>\n",
       "      <td>2.868835</td>\n",
       "      <td>0.395421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>standard</th>\n",
       "      <td>1.422568</td>\n",
       "      <td>0.123773</td>\n",
       "      <td>1.191634</td>\n",
       "      <td>0.051403</td>\n",
       "      <td>0.962895</td>\n",
       "      <td>0.065458</td>\n",
       "      <td>2.893262</td>\n",
       "      <td>0.291517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">delaney</th>\n",
       "      <th>isomers</th>\n",
       "      <td>0.267587</td>\n",
       "      <td>0.045545</td>\n",
       "      <td>0.515486</td>\n",
       "      <td>0.043701</td>\n",
       "      <td>0.410622</td>\n",
       "      <td>0.042446</td>\n",
       "      <td>1.499400</td>\n",
       "      <td>0.214597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>standard</th>\n",
       "      <td>0.264802</td>\n",
       "      <td>0.060044</td>\n",
       "      <td>0.511595</td>\n",
       "      <td>0.056135</td>\n",
       "      <td>0.398901</td>\n",
       "      <td>0.043307</td>\n",
       "      <td>1.544445</td>\n",
       "      <td>0.335253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">lipo</th>\n",
       "      <th>isomers</th>\n",
       "      <td>0.489709</td>\n",
       "      <td>0.040562</td>\n",
       "      <td>0.699214</td>\n",
       "      <td>0.028801</td>\n",
       "      <td>0.549348</td>\n",
       "      <td>0.027534</td>\n",
       "      <td>2.385510</td>\n",
       "      <td>0.224257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>standard</th>\n",
       "      <td>0.485675</td>\n",
       "      <td>0.047847</td>\n",
       "      <td>0.696120</td>\n",
       "      <td>0.033469</td>\n",
       "      <td>0.544725</td>\n",
       "      <td>0.029237</td>\n",
       "      <td>2.402494</td>\n",
       "      <td>0.191164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         mean_squared_error            \\\n",
       "                                       mean       std   \n",
       "task            dataset                                 \n",
       "bace_regression isomers            1.008407  0.359145   \n",
       "                standard           0.950951  0.331325   \n",
       "clearance       isomers            1.424865  0.148814   \n",
       "                standard           1.422568  0.123773   \n",
       "delaney         isomers            0.267587  0.045545   \n",
       "                standard           0.264802  0.060044   \n",
       "lipo            isomers            0.489709  0.040562   \n",
       "                standard           0.485675  0.047847   \n",
       "\n",
       "                         rectified_mean_squared_error            \\\n",
       "                                                 mean       std   \n",
       "task            dataset                                           \n",
       "bace_regression isomers                      0.988698  0.177972   \n",
       "                standard                     0.961400  0.165363   \n",
       "clearance       isomers                      1.192177  0.060579   \n",
       "                standard                     1.191634  0.051403   \n",
       "delaney         isomers                      0.515486  0.043701   \n",
       "                standard                     0.511595  0.056135   \n",
       "lipo            isomers                      0.699214  0.028801   \n",
       "                standard                     0.696120  0.033469   \n",
       "\n",
       "                         mean_absolute_error           max_error            \n",
       "                                        mean       std      mean       std  \n",
       "task            dataset                                                     \n",
       "bace_regression isomers             0.815144  0.168962  2.831548  0.312817  \n",
       "                standard            0.779201  0.159061  2.778795  0.377661  \n",
       "clearance       isomers             0.982452  0.051146  2.868835  0.395421  \n",
       "                standard            0.962895  0.065458  2.893262  0.291517  \n",
       "delaney         isomers             0.410622  0.042446  1.499400  0.214597  \n",
       "                standard            0.398901  0.043307  1.544445  0.335253  \n",
       "lipo            isomers             0.549348  0.027534  2.385510  0.224257  \n",
       "                standard            0.544725  0.029237  2.402494  0.191164  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression_scores[regression_columns].drop([\"Unnamed: 0\", \"task_type\", \"learning_rate\", \"dropout\", \"seed\"], axis=\"columns\").groupby([\"task\", \"dataset\"]).agg([\"mean\", \"std\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c856407",
   "metadata": {},
   "source": [
    "## Compare architecture in regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae4f7978",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_751193/1938754049.py:1: FutureWarning: ['embedding', 'tokenizer', 'dataset'] did not aggregate successfully. If any error is raised this will raise in a future version of pandas. Drop these columns/ops to avoid this warning.\n",
      "  regression_scores[regression_columns].drop([\"Unnamed: 0\", \"task_type\", \"learning_rate\", \"dropout\", \"seed\"], axis=\"columns\").groupby([\"task\", \"architecture\"]).agg([\"mean\", \"std\"])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">mean_squared_error</th>\n",
       "      <th colspan=\"2\" halign=\"left\">rectified_mean_squared_error</th>\n",
       "      <th colspan=\"2\" halign=\"left\">mean_absolute_error</th>\n",
       "      <th colspan=\"2\" halign=\"left\">max_error</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>task</th>\n",
       "      <th>architecture</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">bace_regression</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.907954</td>\n",
       "      <td>0.353924</td>\n",
       "      <td>0.936478</td>\n",
       "      <td>0.178208</td>\n",
       "      <td>0.759462</td>\n",
       "      <td>0.171334</td>\n",
       "      <td>2.761754</td>\n",
       "      <td>0.356986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>1.051404</td>\n",
       "      <td>0.323481</td>\n",
       "      <td>1.013621</td>\n",
       "      <td>0.156816</td>\n",
       "      <td>0.834883</td>\n",
       "      <td>0.149127</td>\n",
       "      <td>2.848590</td>\n",
       "      <td>0.332570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">clearance</th>\n",
       "      <th>bart</th>\n",
       "      <td>1.380240</td>\n",
       "      <td>0.099450</td>\n",
       "      <td>1.174105</td>\n",
       "      <td>0.041965</td>\n",
       "      <td>0.977721</td>\n",
       "      <td>0.058497</td>\n",
       "      <td>2.723586</td>\n",
       "      <td>0.209901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>1.467193</td>\n",
       "      <td>0.153949</td>\n",
       "      <td>1.209706</td>\n",
       "      <td>0.062459</td>\n",
       "      <td>0.967626</td>\n",
       "      <td>0.060190</td>\n",
       "      <td>3.038511</td>\n",
       "      <td>0.383050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">delaney</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.292461</td>\n",
       "      <td>0.057735</td>\n",
       "      <td>0.538237</td>\n",
       "      <td>0.053222</td>\n",
       "      <td>0.424697</td>\n",
       "      <td>0.048164</td>\n",
       "      <td>1.644217</td>\n",
       "      <td>0.326078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.239928</td>\n",
       "      <td>0.030577</td>\n",
       "      <td>0.488843</td>\n",
       "      <td>0.031383</td>\n",
       "      <td>0.384826</td>\n",
       "      <td>0.024750</td>\n",
       "      <td>1.399628</td>\n",
       "      <td>0.149939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">lipo</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.471604</td>\n",
       "      <td>0.025216</td>\n",
       "      <td>0.686494</td>\n",
       "      <td>0.018380</td>\n",
       "      <td>0.535953</td>\n",
       "      <td>0.016389</td>\n",
       "      <td>2.357788</td>\n",
       "      <td>0.194851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.503780</td>\n",
       "      <td>0.052690</td>\n",
       "      <td>0.708840</td>\n",
       "      <td>0.036887</td>\n",
       "      <td>0.558120</td>\n",
       "      <td>0.033216</td>\n",
       "      <td>2.430216</td>\n",
       "      <td>0.215234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             mean_squared_error            \\\n",
       "                                           mean       std   \n",
       "task            architecture                                \n",
       "bace_regression bart                   0.907954  0.353924   \n",
       "                roberta                1.051404  0.323481   \n",
       "clearance       bart                   1.380240  0.099450   \n",
       "                roberta                1.467193  0.153949   \n",
       "delaney         bart                   0.292461  0.057735   \n",
       "                roberta                0.239928  0.030577   \n",
       "lipo            bart                   0.471604  0.025216   \n",
       "                roberta                0.503780  0.052690   \n",
       "\n",
       "                             rectified_mean_squared_error            \\\n",
       "                                                     mean       std   \n",
       "task            architecture                                          \n",
       "bace_regression bart                             0.936478  0.178208   \n",
       "                roberta                          1.013621  0.156816   \n",
       "clearance       bart                             1.174105  0.041965   \n",
       "                roberta                          1.209706  0.062459   \n",
       "delaney         bart                             0.538237  0.053222   \n",
       "                roberta                          0.488843  0.031383   \n",
       "lipo            bart                             0.686494  0.018380   \n",
       "                roberta                          0.708840  0.036887   \n",
       "\n",
       "                             mean_absolute_error           max_error            \n",
       "                                            mean       std      mean       std  \n",
       "task            architecture                                                    \n",
       "bace_regression bart                    0.759462  0.171334  2.761754  0.356986  \n",
       "                roberta                 0.834883  0.149127  2.848590  0.332570  \n",
       "clearance       bart                    0.977721  0.058497  2.723586  0.209901  \n",
       "                roberta                 0.967626  0.060190  3.038511  0.383050  \n",
       "delaney         bart                    0.424697  0.048164  1.644217  0.326078  \n",
       "                roberta                 0.384826  0.024750  1.399628  0.149939  \n",
       "lipo            bart                    0.535953  0.016389  2.357788  0.194851  \n",
       "                roberta                 0.558120  0.033216  2.430216  0.215234  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression_scores[regression_columns].drop([\"Unnamed: 0\", \"task_type\", \"learning_rate\", \"dropout\", \"seed\"], axis=\"columns\").groupby([\"task\", \"architecture\"]).agg([\"mean\", \"std\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafc6383",
   "metadata": {},
   "source": [
    "## Most drilled down classification scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2300efdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">ROC_AUC</th>\n",
       "      <th colspan=\"2\" halign=\"left\">average_precision</th>\n",
       "      <th colspan=\"2\" halign=\"left\">F1_score</th>\n",
       "      <th colspan=\"2\" halign=\"left\">accuracy_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>task</th>\n",
       "      <th>embedding</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>dataset</th>\n",
       "      <th>architecture</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"16\" valign=\"top\">bace_classification</th>\n",
       "      <th rowspan=\"8\" valign=\"top\">selfies</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">atom</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">isomers</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.769275</td>\n",
       "      <td>0.011299</td>\n",
       "      <td>0.842150</td>\n",
       "      <td>0.011934</td>\n",
       "      <td>0.720244</td>\n",
       "      <td>0.015553</td>\n",
       "      <td>0.677632</td>\n",
       "      <td>0.008058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.804094</td>\n",
       "      <td>0.018617</td>\n",
       "      <td>0.845736</td>\n",
       "      <td>0.020447</td>\n",
       "      <td>0.580103</td>\n",
       "      <td>0.186781</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.101814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">standard</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.784348</td>\n",
       "      <td>0.016069</td>\n",
       "      <td>0.831139</td>\n",
       "      <td>0.018996</td>\n",
       "      <td>0.777317</td>\n",
       "      <td>0.022930</td>\n",
       "      <td>0.722368</td>\n",
       "      <td>0.009758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.762717</td>\n",
       "      <td>0.088239</td>\n",
       "      <td>0.811296</td>\n",
       "      <td>0.093012</td>\n",
       "      <td>0.728955</td>\n",
       "      <td>0.076987</td>\n",
       "      <td>0.686842</td>\n",
       "      <td>0.045959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">trained</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">isomers</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.793007</td>\n",
       "      <td>0.017604</td>\n",
       "      <td>0.832298</td>\n",
       "      <td>0.034463</td>\n",
       "      <td>0.620905</td>\n",
       "      <td>0.220598</td>\n",
       "      <td>0.648684</td>\n",
       "      <td>0.112863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.806449</td>\n",
       "      <td>0.020975</td>\n",
       "      <td>0.861740</td>\n",
       "      <td>0.011981</td>\n",
       "      <td>0.691038</td>\n",
       "      <td>0.056888</td>\n",
       "      <td>0.686842</td>\n",
       "      <td>0.039910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">standard</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.683514</td>\n",
       "      <td>0.020922</td>\n",
       "      <td>0.723517</td>\n",
       "      <td>0.031026</td>\n",
       "      <td>0.736656</td>\n",
       "      <td>0.049896</td>\n",
       "      <td>0.671053</td>\n",
       "      <td>0.029787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.675435</td>\n",
       "      <td>0.096786</td>\n",
       "      <td>0.734636</td>\n",
       "      <td>0.095444</td>\n",
       "      <td>0.620736</td>\n",
       "      <td>0.120577</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>0.073995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">smiles</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">atom</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">isomers</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.824058</td>\n",
       "      <td>0.025953</td>\n",
       "      <td>0.871239</td>\n",
       "      <td>0.028443</td>\n",
       "      <td>0.782289</td>\n",
       "      <td>0.026419</td>\n",
       "      <td>0.738158</td>\n",
       "      <td>0.012655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.700906</td>\n",
       "      <td>0.055925</td>\n",
       "      <td>0.800213</td>\n",
       "      <td>0.034716</td>\n",
       "      <td>0.642468</td>\n",
       "      <td>0.072403</td>\n",
       "      <td>0.621053</td>\n",
       "      <td>0.030363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">standard</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.798406</td>\n",
       "      <td>0.015775</td>\n",
       "      <td>0.824287</td>\n",
       "      <td>0.021520</td>\n",
       "      <td>0.796603</td>\n",
       "      <td>0.012327</td>\n",
       "      <td>0.746053</td>\n",
       "      <td>0.016513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.736268</td>\n",
       "      <td>0.041916</td>\n",
       "      <td>0.803128</td>\n",
       "      <td>0.036480</td>\n",
       "      <td>0.587396</td>\n",
       "      <td>0.164591</td>\n",
       "      <td>0.611842</td>\n",
       "      <td>0.079221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">trained</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">isomers</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.788152</td>\n",
       "      <td>0.028186</td>\n",
       "      <td>0.827326</td>\n",
       "      <td>0.031704</td>\n",
       "      <td>0.659575</td>\n",
       "      <td>0.172281</td>\n",
       "      <td>0.667105</td>\n",
       "      <td>0.103040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.776014</td>\n",
       "      <td>0.008818</td>\n",
       "      <td>0.824411</td>\n",
       "      <td>0.011596</td>\n",
       "      <td>0.669244</td>\n",
       "      <td>0.129082</td>\n",
       "      <td>0.665789</td>\n",
       "      <td>0.077620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">standard</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.717464</td>\n",
       "      <td>0.097657</td>\n",
       "      <td>0.789323</td>\n",
       "      <td>0.094660</td>\n",
       "      <td>0.727251</td>\n",
       "      <td>0.089188</td>\n",
       "      <td>0.672368</td>\n",
       "      <td>0.083140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.612319</td>\n",
       "      <td>0.045664</td>\n",
       "      <td>0.681392</td>\n",
       "      <td>0.050063</td>\n",
       "      <td>0.618494</td>\n",
       "      <td>0.301143</td>\n",
       "      <td>0.602632</td>\n",
       "      <td>0.112959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"16\" valign=\"top\">bbbp</th>\n",
       "      <th rowspan=\"8\" valign=\"top\">selfies</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">atom</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">isomers</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.700992</td>\n",
       "      <td>0.031765</td>\n",
       "      <td>0.634281</td>\n",
       "      <td>0.027772</td>\n",
       "      <td>0.438364</td>\n",
       "      <td>0.083303</td>\n",
       "      <td>0.600980</td>\n",
       "      <td>0.030925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.700954</td>\n",
       "      <td>0.010090</td>\n",
       "      <td>0.647624</td>\n",
       "      <td>0.007904</td>\n",
       "      <td>0.368591</td>\n",
       "      <td>0.073253</td>\n",
       "      <td>0.598039</td>\n",
       "      <td>0.019912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">standard</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.685037</td>\n",
       "      <td>0.018241</td>\n",
       "      <td>0.616325</td>\n",
       "      <td>0.022771</td>\n",
       "      <td>0.410096</td>\n",
       "      <td>0.072866</td>\n",
       "      <td>0.587255</td>\n",
       "      <td>0.024362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.705463</td>\n",
       "      <td>0.017339</td>\n",
       "      <td>0.675533</td>\n",
       "      <td>0.013220</td>\n",
       "      <td>0.470938</td>\n",
       "      <td>0.049725</td>\n",
       "      <td>0.637255</td>\n",
       "      <td>0.016623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">trained</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">isomers</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.698487</td>\n",
       "      <td>0.013576</td>\n",
       "      <td>0.646824</td>\n",
       "      <td>0.014207</td>\n",
       "      <td>0.293036</td>\n",
       "      <td>0.103882</td>\n",
       "      <td>0.567647</td>\n",
       "      <td>0.022570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.702206</td>\n",
       "      <td>0.010034</td>\n",
       "      <td>0.658515</td>\n",
       "      <td>0.013348</td>\n",
       "      <td>0.541730</td>\n",
       "      <td>0.050755</td>\n",
       "      <td>0.655882</td>\n",
       "      <td>0.015656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">standard</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.704499</td>\n",
       "      <td>0.018070</td>\n",
       "      <td>0.656435</td>\n",
       "      <td>0.024636</td>\n",
       "      <td>0.364872</td>\n",
       "      <td>0.065158</td>\n",
       "      <td>0.592157</td>\n",
       "      <td>0.021198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.690799</td>\n",
       "      <td>0.009364</td>\n",
       "      <td>0.652402</td>\n",
       "      <td>0.004428</td>\n",
       "      <td>0.312384</td>\n",
       "      <td>0.026385</td>\n",
       "      <td>0.586275</td>\n",
       "      <td>0.010740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">smiles</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">atom</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">isomers</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.694903</td>\n",
       "      <td>0.028177</td>\n",
       "      <td>0.634812</td>\n",
       "      <td>0.030581</td>\n",
       "      <td>0.431574</td>\n",
       "      <td>0.050818</td>\n",
       "      <td>0.601961</td>\n",
       "      <td>0.025091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.745949</td>\n",
       "      <td>0.012082</td>\n",
       "      <td>0.698861</td>\n",
       "      <td>0.014906</td>\n",
       "      <td>0.416374</td>\n",
       "      <td>0.041082</td>\n",
       "      <td>0.622549</td>\n",
       "      <td>0.012969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">standard</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.691916</td>\n",
       "      <td>0.020196</td>\n",
       "      <td>0.624307</td>\n",
       "      <td>0.008944</td>\n",
       "      <td>0.362706</td>\n",
       "      <td>0.084231</td>\n",
       "      <td>0.586275</td>\n",
       "      <td>0.022623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.727276</td>\n",
       "      <td>0.020509</td>\n",
       "      <td>0.669265</td>\n",
       "      <td>0.005732</td>\n",
       "      <td>0.480388</td>\n",
       "      <td>0.073424</td>\n",
       "      <td>0.634314</td>\n",
       "      <td>0.026983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">trained</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">isomers</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.685268</td>\n",
       "      <td>0.009292</td>\n",
       "      <td>0.649486</td>\n",
       "      <td>0.012587</td>\n",
       "      <td>0.275920</td>\n",
       "      <td>0.009253</td>\n",
       "      <td>0.562745</td>\n",
       "      <td>0.004101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.729222</td>\n",
       "      <td>0.008917</td>\n",
       "      <td>0.696521</td>\n",
       "      <td>0.008568</td>\n",
       "      <td>0.522905</td>\n",
       "      <td>0.061042</td>\n",
       "      <td>0.651961</td>\n",
       "      <td>0.021922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">standard</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.713980</td>\n",
       "      <td>0.004987</td>\n",
       "      <td>0.644125</td>\n",
       "      <td>0.009306</td>\n",
       "      <td>0.328817</td>\n",
       "      <td>0.082732</td>\n",
       "      <td>0.577451</td>\n",
       "      <td>0.020032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.697158</td>\n",
       "      <td>0.012560</td>\n",
       "      <td>0.643626</td>\n",
       "      <td>0.014597</td>\n",
       "      <td>0.319044</td>\n",
       "      <td>0.059544</td>\n",
       "      <td>0.570588</td>\n",
       "      <td>0.011805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"16\" valign=\"top\">clintox</th>\n",
       "      <th rowspan=\"8\" valign=\"top\">selfies</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">atom</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">isomers</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.729855</td>\n",
       "      <td>0.099440</td>\n",
       "      <td>0.236194</td>\n",
       "      <td>0.119418</td>\n",
       "      <td>0.221300</td>\n",
       "      <td>0.233617</td>\n",
       "      <td>0.894595</td>\n",
       "      <td>0.057054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.879565</td>\n",
       "      <td>0.030548</td>\n",
       "      <td>0.389162</td>\n",
       "      <td>0.088828</td>\n",
       "      <td>0.343881</td>\n",
       "      <td>0.043834</td>\n",
       "      <td>0.897297</td>\n",
       "      <td>0.036823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">standard</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.607681</td>\n",
       "      <td>0.030823</td>\n",
       "      <td>0.114163</td>\n",
       "      <td>0.016063</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.932432</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.875652</td>\n",
       "      <td>0.008351</td>\n",
       "      <td>0.461439</td>\n",
       "      <td>0.072378</td>\n",
       "      <td>0.373235</td>\n",
       "      <td>0.084496</td>\n",
       "      <td>0.890541</td>\n",
       "      <td>0.049744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">trained</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">isomers</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.657246</td>\n",
       "      <td>0.053474</td>\n",
       "      <td>0.159815</td>\n",
       "      <td>0.043714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.932432</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.787681</td>\n",
       "      <td>0.045196</td>\n",
       "      <td>0.302416</td>\n",
       "      <td>0.121670</td>\n",
       "      <td>0.208158</td>\n",
       "      <td>0.140267</td>\n",
       "      <td>0.912162</td>\n",
       "      <td>0.011703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">standard</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.611159</td>\n",
       "      <td>0.031763</td>\n",
       "      <td>0.142336</td>\n",
       "      <td>0.040909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.932432</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.689420</td>\n",
       "      <td>0.030108</td>\n",
       "      <td>0.148461</td>\n",
       "      <td>0.033553</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.044721</td>\n",
       "      <td>0.921622</td>\n",
       "      <td>0.024174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">smiles</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">atom</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">isomers</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.742609</td>\n",
       "      <td>0.042546</td>\n",
       "      <td>0.208564</td>\n",
       "      <td>0.032270</td>\n",
       "      <td>0.036364</td>\n",
       "      <td>0.081312</td>\n",
       "      <td>0.931081</td>\n",
       "      <td>0.007402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.843913</td>\n",
       "      <td>0.038401</td>\n",
       "      <td>0.325689</td>\n",
       "      <td>0.064770</td>\n",
       "      <td>0.349841</td>\n",
       "      <td>0.070645</td>\n",
       "      <td>0.909459</td>\n",
       "      <td>0.028507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">standard</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.730000</td>\n",
       "      <td>0.111620</td>\n",
       "      <td>0.311035</td>\n",
       "      <td>0.122816</td>\n",
       "      <td>0.214167</td>\n",
       "      <td>0.201978</td>\n",
       "      <td>0.894595</td>\n",
       "      <td>0.051812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.725797</td>\n",
       "      <td>0.103337</td>\n",
       "      <td>0.254363</td>\n",
       "      <td>0.200975</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.247436</td>\n",
       "      <td>0.931081</td>\n",
       "      <td>0.007402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">trained</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">isomers</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.611304</td>\n",
       "      <td>0.038254</td>\n",
       "      <td>0.111922</td>\n",
       "      <td>0.013895</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.932432</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.644348</td>\n",
       "      <td>0.017885</td>\n",
       "      <td>0.277818</td>\n",
       "      <td>0.060308</td>\n",
       "      <td>0.133800</td>\n",
       "      <td>0.139934</td>\n",
       "      <td>0.929730</td>\n",
       "      <td>0.007704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">standard</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.593333</td>\n",
       "      <td>0.070672</td>\n",
       "      <td>0.131547</td>\n",
       "      <td>0.032337</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.932432</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.666087</td>\n",
       "      <td>0.045971</td>\n",
       "      <td>0.263490</td>\n",
       "      <td>0.102019</td>\n",
       "      <td>0.123136</td>\n",
       "      <td>0.123314</td>\n",
       "      <td>0.916216</td>\n",
       "      <td>0.040144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"16\" valign=\"top\">hiv</th>\n",
       "      <th rowspan=\"8\" valign=\"top\">selfies</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">atom</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">isomers</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.753252</td>\n",
       "      <td>0.021837</td>\n",
       "      <td>0.315859</td>\n",
       "      <td>0.024729</td>\n",
       "      <td>0.337904</td>\n",
       "      <td>0.045486</td>\n",
       "      <td>0.970621</td>\n",
       "      <td>0.001395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.765811</td>\n",
       "      <td>0.013750</td>\n",
       "      <td>0.280235</td>\n",
       "      <td>0.038105</td>\n",
       "      <td>0.257729</td>\n",
       "      <td>0.089119</td>\n",
       "      <td>0.970183</td>\n",
       "      <td>0.001658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">standard</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.754320</td>\n",
       "      <td>0.009778</td>\n",
       "      <td>0.312441</td>\n",
       "      <td>0.024876</td>\n",
       "      <td>0.357651</td>\n",
       "      <td>0.039760</td>\n",
       "      <td>0.970183</td>\n",
       "      <td>0.002013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.730058</td>\n",
       "      <td>0.018618</td>\n",
       "      <td>0.160934</td>\n",
       "      <td>0.030295</td>\n",
       "      <td>0.045901</td>\n",
       "      <td>0.035285</td>\n",
       "      <td>0.968088</td>\n",
       "      <td>0.000959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">trained</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">isomers</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.723933</td>\n",
       "      <td>0.010032</td>\n",
       "      <td>0.209019</td>\n",
       "      <td>0.053584</td>\n",
       "      <td>0.198156</td>\n",
       "      <td>0.085478</td>\n",
       "      <td>0.962241</td>\n",
       "      <td>0.011800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.732639</td>\n",
       "      <td>0.013848</td>\n",
       "      <td>0.255630</td>\n",
       "      <td>0.018028</td>\n",
       "      <td>0.138652</td>\n",
       "      <td>0.042242</td>\n",
       "      <td>0.969647</td>\n",
       "      <td>0.000443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">standard</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.740014</td>\n",
       "      <td>0.006295</td>\n",
       "      <td>0.291514</td>\n",
       "      <td>0.032354</td>\n",
       "      <td>0.338900</td>\n",
       "      <td>0.041351</td>\n",
       "      <td>0.968867</td>\n",
       "      <td>0.000556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.716211</td>\n",
       "      <td>0.023485</td>\n",
       "      <td>0.177045</td>\n",
       "      <td>0.052790</td>\n",
       "      <td>0.079613</td>\n",
       "      <td>0.072541</td>\n",
       "      <td>0.968185</td>\n",
       "      <td>0.000611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">smiles</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">atom</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">isomers</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.762933</td>\n",
       "      <td>0.012653</td>\n",
       "      <td>0.299089</td>\n",
       "      <td>0.030306</td>\n",
       "      <td>0.336971</td>\n",
       "      <td>0.060301</td>\n",
       "      <td>0.969062</td>\n",
       "      <td>0.001230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.753008</td>\n",
       "      <td>0.016492</td>\n",
       "      <td>0.267310</td>\n",
       "      <td>0.032837</td>\n",
       "      <td>0.216731</td>\n",
       "      <td>0.126309</td>\n",
       "      <td>0.970183</td>\n",
       "      <td>0.001176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">standard</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.756803</td>\n",
       "      <td>0.012756</td>\n",
       "      <td>0.265321</td>\n",
       "      <td>0.028009</td>\n",
       "      <td>0.286940</td>\n",
       "      <td>0.061045</td>\n",
       "      <td>0.968624</td>\n",
       "      <td>0.004943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.753544</td>\n",
       "      <td>0.004343</td>\n",
       "      <td>0.269251</td>\n",
       "      <td>0.040282</td>\n",
       "      <td>0.221292</td>\n",
       "      <td>0.054167</td>\n",
       "      <td>0.970426</td>\n",
       "      <td>0.001137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">trained</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">isomers</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.777508</td>\n",
       "      <td>0.010589</td>\n",
       "      <td>0.319871</td>\n",
       "      <td>0.020830</td>\n",
       "      <td>0.297990</td>\n",
       "      <td>0.127591</td>\n",
       "      <td>0.969160</td>\n",
       "      <td>0.002512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.771161</td>\n",
       "      <td>0.010406</td>\n",
       "      <td>0.219802</td>\n",
       "      <td>0.034656</td>\n",
       "      <td>0.230783</td>\n",
       "      <td>0.075474</td>\n",
       "      <td>0.966821</td>\n",
       "      <td>0.003173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">standard</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.759112</td>\n",
       "      <td>0.023452</td>\n",
       "      <td>0.299370</td>\n",
       "      <td>0.076617</td>\n",
       "      <td>0.283176</td>\n",
       "      <td>0.157391</td>\n",
       "      <td>0.969988</td>\n",
       "      <td>0.001601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.750078</td>\n",
       "      <td>0.006748</td>\n",
       "      <td>0.233595</td>\n",
       "      <td>0.040436</td>\n",
       "      <td>0.062000</td>\n",
       "      <td>0.100222</td>\n",
       "      <td>0.968624</td>\n",
       "      <td>0.000902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"16\" valign=\"top\">tox21</th>\n",
       "      <th rowspan=\"8\" valign=\"top\">selfies</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">atom</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">isomers</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.670904</td>\n",
       "      <td>0.028159</td>\n",
       "      <td>0.187289</td>\n",
       "      <td>0.024343</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.907908</td>\n",
       "      <td>0.000570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.638510</td>\n",
       "      <td>0.043679</td>\n",
       "      <td>0.160553</td>\n",
       "      <td>0.020651</td>\n",
       "      <td>0.020040</td>\n",
       "      <td>0.031924</td>\n",
       "      <td>0.905357</td>\n",
       "      <td>0.002614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">standard</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.694838</td>\n",
       "      <td>0.016069</td>\n",
       "      <td>0.191160</td>\n",
       "      <td>0.015543</td>\n",
       "      <td>0.005405</td>\n",
       "      <td>0.012087</td>\n",
       "      <td>0.908163</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.638296</td>\n",
       "      <td>0.042778</td>\n",
       "      <td>0.200533</td>\n",
       "      <td>0.038508</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.908163</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">trained</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">isomers</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.719023</td>\n",
       "      <td>0.013223</td>\n",
       "      <td>0.247390</td>\n",
       "      <td>0.015705</td>\n",
       "      <td>0.026256</td>\n",
       "      <td>0.026165</td>\n",
       "      <td>0.907398</td>\n",
       "      <td>0.001454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.702918</td>\n",
       "      <td>0.018289</td>\n",
       "      <td>0.247988</td>\n",
       "      <td>0.029034</td>\n",
       "      <td>0.030992</td>\n",
       "      <td>0.041515</td>\n",
       "      <td>0.908163</td>\n",
       "      <td>0.000902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">standard</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.673252</td>\n",
       "      <td>0.017876</td>\n",
       "      <td>0.173904</td>\n",
       "      <td>0.017226</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.907653</td>\n",
       "      <td>0.000699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.672164</td>\n",
       "      <td>0.007072</td>\n",
       "      <td>0.246884</td>\n",
       "      <td>0.013363</td>\n",
       "      <td>0.005479</td>\n",
       "      <td>0.012252</td>\n",
       "      <td>0.908418</td>\n",
       "      <td>0.000570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">smiles</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">atom</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">isomers</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.659121</td>\n",
       "      <td>0.038403</td>\n",
       "      <td>0.181978</td>\n",
       "      <td>0.012925</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.011180</td>\n",
       "      <td>0.906122</td>\n",
       "      <td>0.003202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.639537</td>\n",
       "      <td>0.017773</td>\n",
       "      <td>0.174443</td>\n",
       "      <td>0.007372</td>\n",
       "      <td>0.005195</td>\n",
       "      <td>0.011616</td>\n",
       "      <td>0.907143</td>\n",
       "      <td>0.001663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">standard</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.687092</td>\n",
       "      <td>0.015994</td>\n",
       "      <td>0.181592</td>\n",
       "      <td>0.017539</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.908163</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.674017</td>\n",
       "      <td>0.011870</td>\n",
       "      <td>0.180894</td>\n",
       "      <td>0.021112</td>\n",
       "      <td>0.010127</td>\n",
       "      <td>0.022644</td>\n",
       "      <td>0.907398</td>\n",
       "      <td>0.001711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">trained</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">isomers</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.697640</td>\n",
       "      <td>0.035221</td>\n",
       "      <td>0.188346</td>\n",
       "      <td>0.021170</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.908163</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.666979</td>\n",
       "      <td>0.057186</td>\n",
       "      <td>0.188410</td>\n",
       "      <td>0.044350</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.908163</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">standard</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.695868</td>\n",
       "      <td>0.017061</td>\n",
       "      <td>0.194579</td>\n",
       "      <td>0.019856</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.907908</td>\n",
       "      <td>0.000570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.698478</td>\n",
       "      <td>0.010051</td>\n",
       "      <td>0.192553</td>\n",
       "      <td>0.010164</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.908163</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                ROC_AUC  \\\n",
       "                                                                   mean   \n",
       "task                embedding tokenizer dataset  architecture             \n",
       "bace_classification selfies   atom      isomers  bart          0.769275   \n",
       "                                                 roberta       0.804094   \n",
       "                                        standard bart          0.784348   \n",
       "                                                 roberta       0.762717   \n",
       "                              trained   isomers  bart          0.793007   \n",
       "                                                 roberta       0.806449   \n",
       "                                        standard bart          0.683514   \n",
       "                                                 roberta       0.675435   \n",
       "                    smiles    atom      isomers  bart          0.824058   \n",
       "                                                 roberta       0.700906   \n",
       "                                        standard bart          0.798406   \n",
       "                                                 roberta       0.736268   \n",
       "                              trained   isomers  bart          0.788152   \n",
       "                                                 roberta       0.776014   \n",
       "                                        standard bart          0.717464   \n",
       "                                                 roberta       0.612319   \n",
       "bbbp                selfies   atom      isomers  bart          0.700992   \n",
       "                                                 roberta       0.700954   \n",
       "                                        standard bart          0.685037   \n",
       "                                                 roberta       0.705463   \n",
       "                              trained   isomers  bart          0.698487   \n",
       "                                                 roberta       0.702206   \n",
       "                                        standard bart          0.704499   \n",
       "                                                 roberta       0.690799   \n",
       "                    smiles    atom      isomers  bart          0.694903   \n",
       "                                                 roberta       0.745949   \n",
       "                                        standard bart          0.691916   \n",
       "                                                 roberta       0.727276   \n",
       "                              trained   isomers  bart          0.685268   \n",
       "                                                 roberta       0.729222   \n",
       "                                        standard bart          0.713980   \n",
       "                                                 roberta       0.697158   \n",
       "clintox             selfies   atom      isomers  bart          0.729855   \n",
       "                                                 roberta       0.879565   \n",
       "                                        standard bart          0.607681   \n",
       "                                                 roberta       0.875652   \n",
       "                              trained   isomers  bart          0.657246   \n",
       "                                                 roberta       0.787681   \n",
       "                                        standard bart          0.611159   \n",
       "                                                 roberta       0.689420   \n",
       "                    smiles    atom      isomers  bart          0.742609   \n",
       "                                                 roberta       0.843913   \n",
       "                                        standard bart          0.730000   \n",
       "                                                 roberta       0.725797   \n",
       "                              trained   isomers  bart          0.611304   \n",
       "                                                 roberta       0.644348   \n",
       "                                        standard bart          0.593333   \n",
       "                                                 roberta       0.666087   \n",
       "hiv                 selfies   atom      isomers  bart          0.753252   \n",
       "                                                 roberta       0.765811   \n",
       "                                        standard bart          0.754320   \n",
       "                                                 roberta       0.730058   \n",
       "                              trained   isomers  bart          0.723933   \n",
       "                                                 roberta       0.732639   \n",
       "                                        standard bart          0.740014   \n",
       "                                                 roberta       0.716211   \n",
       "                    smiles    atom      isomers  bart          0.762933   \n",
       "                                                 roberta       0.753008   \n",
       "                                        standard bart          0.756803   \n",
       "                                                 roberta       0.753544   \n",
       "                              trained   isomers  bart          0.777508   \n",
       "                                                 roberta       0.771161   \n",
       "                                        standard bart          0.759112   \n",
       "                                                 roberta       0.750078   \n",
       "tox21               selfies   atom      isomers  bart          0.670904   \n",
       "                                                 roberta       0.638510   \n",
       "                                        standard bart          0.694838   \n",
       "                                                 roberta       0.638296   \n",
       "                              trained   isomers  bart          0.719023   \n",
       "                                                 roberta       0.702918   \n",
       "                                        standard bart          0.673252   \n",
       "                                                 roberta       0.672164   \n",
       "                    smiles    atom      isomers  bart          0.659121   \n",
       "                                                 roberta       0.639537   \n",
       "                                        standard bart          0.687092   \n",
       "                                                 roberta       0.674017   \n",
       "                              trained   isomers  bart          0.697640   \n",
       "                                                 roberta       0.666979   \n",
       "                                        standard bart          0.695868   \n",
       "                                                 roberta       0.698478   \n",
       "\n",
       "                                                                         \\\n",
       "                                                                    std   \n",
       "task                embedding tokenizer dataset  architecture             \n",
       "bace_classification selfies   atom      isomers  bart          0.011299   \n",
       "                                                 roberta       0.018617   \n",
       "                                        standard bart          0.016069   \n",
       "                                                 roberta       0.088239   \n",
       "                              trained   isomers  bart          0.017604   \n",
       "                                                 roberta       0.020975   \n",
       "                                        standard bart          0.020922   \n",
       "                                                 roberta       0.096786   \n",
       "                    smiles    atom      isomers  bart          0.025953   \n",
       "                                                 roberta       0.055925   \n",
       "                                        standard bart          0.015775   \n",
       "                                                 roberta       0.041916   \n",
       "                              trained   isomers  bart          0.028186   \n",
       "                                                 roberta       0.008818   \n",
       "                                        standard bart          0.097657   \n",
       "                                                 roberta       0.045664   \n",
       "bbbp                selfies   atom      isomers  bart          0.031765   \n",
       "                                                 roberta       0.010090   \n",
       "                                        standard bart          0.018241   \n",
       "                                                 roberta       0.017339   \n",
       "                              trained   isomers  bart          0.013576   \n",
       "                                                 roberta       0.010034   \n",
       "                                        standard bart          0.018070   \n",
       "                                                 roberta       0.009364   \n",
       "                    smiles    atom      isomers  bart          0.028177   \n",
       "                                                 roberta       0.012082   \n",
       "                                        standard bart          0.020196   \n",
       "                                                 roberta       0.020509   \n",
       "                              trained   isomers  bart          0.009292   \n",
       "                                                 roberta       0.008917   \n",
       "                                        standard bart          0.004987   \n",
       "                                                 roberta       0.012560   \n",
       "clintox             selfies   atom      isomers  bart          0.099440   \n",
       "                                                 roberta       0.030548   \n",
       "                                        standard bart          0.030823   \n",
       "                                                 roberta       0.008351   \n",
       "                              trained   isomers  bart          0.053474   \n",
       "                                                 roberta       0.045196   \n",
       "                                        standard bart          0.031763   \n",
       "                                                 roberta       0.030108   \n",
       "                    smiles    atom      isomers  bart          0.042546   \n",
       "                                                 roberta       0.038401   \n",
       "                                        standard bart          0.111620   \n",
       "                                                 roberta       0.103337   \n",
       "                              trained   isomers  bart          0.038254   \n",
       "                                                 roberta       0.017885   \n",
       "                                        standard bart          0.070672   \n",
       "                                                 roberta       0.045971   \n",
       "hiv                 selfies   atom      isomers  bart          0.021837   \n",
       "                                                 roberta       0.013750   \n",
       "                                        standard bart          0.009778   \n",
       "                                                 roberta       0.018618   \n",
       "                              trained   isomers  bart          0.010032   \n",
       "                                                 roberta       0.013848   \n",
       "                                        standard bart          0.006295   \n",
       "                                                 roberta       0.023485   \n",
       "                    smiles    atom      isomers  bart          0.012653   \n",
       "                                                 roberta       0.016492   \n",
       "                                        standard bart          0.012756   \n",
       "                                                 roberta       0.004343   \n",
       "                              trained   isomers  bart          0.010589   \n",
       "                                                 roberta       0.010406   \n",
       "                                        standard bart          0.023452   \n",
       "                                                 roberta       0.006748   \n",
       "tox21               selfies   atom      isomers  bart          0.028159   \n",
       "                                                 roberta       0.043679   \n",
       "                                        standard bart          0.016069   \n",
       "                                                 roberta       0.042778   \n",
       "                              trained   isomers  bart          0.013223   \n",
       "                                                 roberta       0.018289   \n",
       "                                        standard bart          0.017876   \n",
       "                                                 roberta       0.007072   \n",
       "                    smiles    atom      isomers  bart          0.038403   \n",
       "                                                 roberta       0.017773   \n",
       "                                        standard bart          0.015994   \n",
       "                                                 roberta       0.011870   \n",
       "                              trained   isomers  bart          0.035221   \n",
       "                                                 roberta       0.057186   \n",
       "                                        standard bart          0.017061   \n",
       "                                                 roberta       0.010051   \n",
       "\n",
       "                                                              average_precision  \\\n",
       "                                                                           mean   \n",
       "task                embedding tokenizer dataset  architecture                     \n",
       "bace_classification selfies   atom      isomers  bart                  0.842150   \n",
       "                                                 roberta               0.845736   \n",
       "                                        standard bart                  0.831139   \n",
       "                                                 roberta               0.811296   \n",
       "                              trained   isomers  bart                  0.832298   \n",
       "                                                 roberta               0.861740   \n",
       "                                        standard bart                  0.723517   \n",
       "                                                 roberta               0.734636   \n",
       "                    smiles    atom      isomers  bart                  0.871239   \n",
       "                                                 roberta               0.800213   \n",
       "                                        standard bart                  0.824287   \n",
       "                                                 roberta               0.803128   \n",
       "                              trained   isomers  bart                  0.827326   \n",
       "                                                 roberta               0.824411   \n",
       "                                        standard bart                  0.789323   \n",
       "                                                 roberta               0.681392   \n",
       "bbbp                selfies   atom      isomers  bart                  0.634281   \n",
       "                                                 roberta               0.647624   \n",
       "                                        standard bart                  0.616325   \n",
       "                                                 roberta               0.675533   \n",
       "                              trained   isomers  bart                  0.646824   \n",
       "                                                 roberta               0.658515   \n",
       "                                        standard bart                  0.656435   \n",
       "                                                 roberta               0.652402   \n",
       "                    smiles    atom      isomers  bart                  0.634812   \n",
       "                                                 roberta               0.698861   \n",
       "                                        standard bart                  0.624307   \n",
       "                                                 roberta               0.669265   \n",
       "                              trained   isomers  bart                  0.649486   \n",
       "                                                 roberta               0.696521   \n",
       "                                        standard bart                  0.644125   \n",
       "                                                 roberta               0.643626   \n",
       "clintox             selfies   atom      isomers  bart                  0.236194   \n",
       "                                                 roberta               0.389162   \n",
       "                                        standard bart                  0.114163   \n",
       "                                                 roberta               0.461439   \n",
       "                              trained   isomers  bart                  0.159815   \n",
       "                                                 roberta               0.302416   \n",
       "                                        standard bart                  0.142336   \n",
       "                                                 roberta               0.148461   \n",
       "                    smiles    atom      isomers  bart                  0.208564   \n",
       "                                                 roberta               0.325689   \n",
       "                                        standard bart                  0.311035   \n",
       "                                                 roberta               0.254363   \n",
       "                              trained   isomers  bart                  0.111922   \n",
       "                                                 roberta               0.277818   \n",
       "                                        standard bart                  0.131547   \n",
       "                                                 roberta               0.263490   \n",
       "hiv                 selfies   atom      isomers  bart                  0.315859   \n",
       "                                                 roberta               0.280235   \n",
       "                                        standard bart                  0.312441   \n",
       "                                                 roberta               0.160934   \n",
       "                              trained   isomers  bart                  0.209019   \n",
       "                                                 roberta               0.255630   \n",
       "                                        standard bart                  0.291514   \n",
       "                                                 roberta               0.177045   \n",
       "                    smiles    atom      isomers  bart                  0.299089   \n",
       "                                                 roberta               0.267310   \n",
       "                                        standard bart                  0.265321   \n",
       "                                                 roberta               0.269251   \n",
       "                              trained   isomers  bart                  0.319871   \n",
       "                                                 roberta               0.219802   \n",
       "                                        standard bart                  0.299370   \n",
       "                                                 roberta               0.233595   \n",
       "tox21               selfies   atom      isomers  bart                  0.187289   \n",
       "                                                 roberta               0.160553   \n",
       "                                        standard bart                  0.191160   \n",
       "                                                 roberta               0.200533   \n",
       "                              trained   isomers  bart                  0.247390   \n",
       "                                                 roberta               0.247988   \n",
       "                                        standard bart                  0.173904   \n",
       "                                                 roberta               0.246884   \n",
       "                    smiles    atom      isomers  bart                  0.181978   \n",
       "                                                 roberta               0.174443   \n",
       "                                        standard bart                  0.181592   \n",
       "                                                 roberta               0.180894   \n",
       "                              trained   isomers  bart                  0.188346   \n",
       "                                                 roberta               0.188410   \n",
       "                                        standard bart                  0.194579   \n",
       "                                                 roberta               0.192553   \n",
       "\n",
       "                                                                         \\\n",
       "                                                                    std   \n",
       "task                embedding tokenizer dataset  architecture             \n",
       "bace_classification selfies   atom      isomers  bart          0.011934   \n",
       "                                                 roberta       0.020447   \n",
       "                                        standard bart          0.018996   \n",
       "                                                 roberta       0.093012   \n",
       "                              trained   isomers  bart          0.034463   \n",
       "                                                 roberta       0.011981   \n",
       "                                        standard bart          0.031026   \n",
       "                                                 roberta       0.095444   \n",
       "                    smiles    atom      isomers  bart          0.028443   \n",
       "                                                 roberta       0.034716   \n",
       "                                        standard bart          0.021520   \n",
       "                                                 roberta       0.036480   \n",
       "                              trained   isomers  bart          0.031704   \n",
       "                                                 roberta       0.011596   \n",
       "                                        standard bart          0.094660   \n",
       "                                                 roberta       0.050063   \n",
       "bbbp                selfies   atom      isomers  bart          0.027772   \n",
       "                                                 roberta       0.007904   \n",
       "                                        standard bart          0.022771   \n",
       "                                                 roberta       0.013220   \n",
       "                              trained   isomers  bart          0.014207   \n",
       "                                                 roberta       0.013348   \n",
       "                                        standard bart          0.024636   \n",
       "                                                 roberta       0.004428   \n",
       "                    smiles    atom      isomers  bart          0.030581   \n",
       "                                                 roberta       0.014906   \n",
       "                                        standard bart          0.008944   \n",
       "                                                 roberta       0.005732   \n",
       "                              trained   isomers  bart          0.012587   \n",
       "                                                 roberta       0.008568   \n",
       "                                        standard bart          0.009306   \n",
       "                                                 roberta       0.014597   \n",
       "clintox             selfies   atom      isomers  bart          0.119418   \n",
       "                                                 roberta       0.088828   \n",
       "                                        standard bart          0.016063   \n",
       "                                                 roberta       0.072378   \n",
       "                              trained   isomers  bart          0.043714   \n",
       "                                                 roberta       0.121670   \n",
       "                                        standard bart          0.040909   \n",
       "                                                 roberta       0.033553   \n",
       "                    smiles    atom      isomers  bart          0.032270   \n",
       "                                                 roberta       0.064770   \n",
       "                                        standard bart          0.122816   \n",
       "                                                 roberta       0.200975   \n",
       "                              trained   isomers  bart          0.013895   \n",
       "                                                 roberta       0.060308   \n",
       "                                        standard bart          0.032337   \n",
       "                                                 roberta       0.102019   \n",
       "hiv                 selfies   atom      isomers  bart          0.024729   \n",
       "                                                 roberta       0.038105   \n",
       "                                        standard bart          0.024876   \n",
       "                                                 roberta       0.030295   \n",
       "                              trained   isomers  bart          0.053584   \n",
       "                                                 roberta       0.018028   \n",
       "                                        standard bart          0.032354   \n",
       "                                                 roberta       0.052790   \n",
       "                    smiles    atom      isomers  bart          0.030306   \n",
       "                                                 roberta       0.032837   \n",
       "                                        standard bart          0.028009   \n",
       "                                                 roberta       0.040282   \n",
       "                              trained   isomers  bart          0.020830   \n",
       "                                                 roberta       0.034656   \n",
       "                                        standard bart          0.076617   \n",
       "                                                 roberta       0.040436   \n",
       "tox21               selfies   atom      isomers  bart          0.024343   \n",
       "                                                 roberta       0.020651   \n",
       "                                        standard bart          0.015543   \n",
       "                                                 roberta       0.038508   \n",
       "                              trained   isomers  bart          0.015705   \n",
       "                                                 roberta       0.029034   \n",
       "                                        standard bart          0.017226   \n",
       "                                                 roberta       0.013363   \n",
       "                    smiles    atom      isomers  bart          0.012925   \n",
       "                                                 roberta       0.007372   \n",
       "                                        standard bart          0.017539   \n",
       "                                                 roberta       0.021112   \n",
       "                              trained   isomers  bart          0.021170   \n",
       "                                                 roberta       0.044350   \n",
       "                                        standard bart          0.019856   \n",
       "                                                 roberta       0.010164   \n",
       "\n",
       "                                                               F1_score  \\\n",
       "                                                                   mean   \n",
       "task                embedding tokenizer dataset  architecture             \n",
       "bace_classification selfies   atom      isomers  bart          0.720244   \n",
       "                                                 roberta       0.580103   \n",
       "                                        standard bart          0.777317   \n",
       "                                                 roberta       0.728955   \n",
       "                              trained   isomers  bart          0.620905   \n",
       "                                                 roberta       0.691038   \n",
       "                                        standard bart          0.736656   \n",
       "                                                 roberta       0.620736   \n",
       "                    smiles    atom      isomers  bart          0.782289   \n",
       "                                                 roberta       0.642468   \n",
       "                                        standard bart          0.796603   \n",
       "                                                 roberta       0.587396   \n",
       "                              trained   isomers  bart          0.659575   \n",
       "                                                 roberta       0.669244   \n",
       "                                        standard bart          0.727251   \n",
       "                                                 roberta       0.618494   \n",
       "bbbp                selfies   atom      isomers  bart          0.438364   \n",
       "                                                 roberta       0.368591   \n",
       "                                        standard bart          0.410096   \n",
       "                                                 roberta       0.470938   \n",
       "                              trained   isomers  bart          0.293036   \n",
       "                                                 roberta       0.541730   \n",
       "                                        standard bart          0.364872   \n",
       "                                                 roberta       0.312384   \n",
       "                    smiles    atom      isomers  bart          0.431574   \n",
       "                                                 roberta       0.416374   \n",
       "                                        standard bart          0.362706   \n",
       "                                                 roberta       0.480388   \n",
       "                              trained   isomers  bart          0.275920   \n",
       "                                                 roberta       0.522905   \n",
       "                                        standard bart          0.328817   \n",
       "                                                 roberta       0.319044   \n",
       "clintox             selfies   atom      isomers  bart          0.221300   \n",
       "                                                 roberta       0.343881   \n",
       "                                        standard bart          0.000000   \n",
       "                                                 roberta       0.373235   \n",
       "                              trained   isomers  bart          0.000000   \n",
       "                                                 roberta       0.208158   \n",
       "                                        standard bart          0.000000   \n",
       "                                                 roberta       0.020000   \n",
       "                    smiles    atom      isomers  bart          0.036364   \n",
       "                                                 roberta       0.349841   \n",
       "                                        standard bart          0.214167   \n",
       "                                                 roberta       0.142857   \n",
       "                              trained   isomers  bart          0.000000   \n",
       "                                                 roberta       0.133800   \n",
       "                                        standard bart          0.000000   \n",
       "                                                 roberta       0.123136   \n",
       "hiv                 selfies   atom      isomers  bart          0.337904   \n",
       "                                                 roberta       0.257729   \n",
       "                                        standard bart          0.357651   \n",
       "                                                 roberta       0.045901   \n",
       "                              trained   isomers  bart          0.198156   \n",
       "                                                 roberta       0.138652   \n",
       "                                        standard bart          0.338900   \n",
       "                                                 roberta       0.079613   \n",
       "                    smiles    atom      isomers  bart          0.336971   \n",
       "                                                 roberta       0.216731   \n",
       "                                        standard bart          0.286940   \n",
       "                                                 roberta       0.221292   \n",
       "                              trained   isomers  bart          0.297990   \n",
       "                                                 roberta       0.230783   \n",
       "                                        standard bart          0.283176   \n",
       "                                                 roberta       0.062000   \n",
       "tox21               selfies   atom      isomers  bart          0.000000   \n",
       "                                                 roberta       0.020040   \n",
       "                                        standard bart          0.005405   \n",
       "                                                 roberta       0.000000   \n",
       "                              trained   isomers  bart          0.026256   \n",
       "                                                 roberta       0.030992   \n",
       "                                        standard bart          0.000000   \n",
       "                                                 roberta       0.005479   \n",
       "                    smiles    atom      isomers  bart          0.005000   \n",
       "                                                 roberta       0.005195   \n",
       "                                        standard bart          0.000000   \n",
       "                                                 roberta       0.010127   \n",
       "                              trained   isomers  bart          0.000000   \n",
       "                                                 roberta       0.000000   \n",
       "                                        standard bart          0.000000   \n",
       "                                                 roberta       0.000000   \n",
       "\n",
       "                                                                         \\\n",
       "                                                                    std   \n",
       "task                embedding tokenizer dataset  architecture             \n",
       "bace_classification selfies   atom      isomers  bart          0.015553   \n",
       "                                                 roberta       0.186781   \n",
       "                                        standard bart          0.022930   \n",
       "                                                 roberta       0.076987   \n",
       "                              trained   isomers  bart          0.220598   \n",
       "                                                 roberta       0.056888   \n",
       "                                        standard bart          0.049896   \n",
       "                                                 roberta       0.120577   \n",
       "                    smiles    atom      isomers  bart          0.026419   \n",
       "                                                 roberta       0.072403   \n",
       "                                        standard bart          0.012327   \n",
       "                                                 roberta       0.164591   \n",
       "                              trained   isomers  bart          0.172281   \n",
       "                                                 roberta       0.129082   \n",
       "                                        standard bart          0.089188   \n",
       "                                                 roberta       0.301143   \n",
       "bbbp                selfies   atom      isomers  bart          0.083303   \n",
       "                                                 roberta       0.073253   \n",
       "                                        standard bart          0.072866   \n",
       "                                                 roberta       0.049725   \n",
       "                              trained   isomers  bart          0.103882   \n",
       "                                                 roberta       0.050755   \n",
       "                                        standard bart          0.065158   \n",
       "                                                 roberta       0.026385   \n",
       "                    smiles    atom      isomers  bart          0.050818   \n",
       "                                                 roberta       0.041082   \n",
       "                                        standard bart          0.084231   \n",
       "                                                 roberta       0.073424   \n",
       "                              trained   isomers  bart          0.009253   \n",
       "                                                 roberta       0.061042   \n",
       "                                        standard bart          0.082732   \n",
       "                                                 roberta       0.059544   \n",
       "clintox             selfies   atom      isomers  bart          0.233617   \n",
       "                                                 roberta       0.043834   \n",
       "                                        standard bart          0.000000   \n",
       "                                                 roberta       0.084496   \n",
       "                              trained   isomers  bart          0.000000   \n",
       "                                                 roberta       0.140267   \n",
       "                                        standard bart          0.000000   \n",
       "                                                 roberta       0.044721   \n",
       "                    smiles    atom      isomers  bart          0.081312   \n",
       "                                                 roberta       0.070645   \n",
       "                                        standard bart          0.201978   \n",
       "                                                 roberta       0.247436   \n",
       "                              trained   isomers  bart          0.000000   \n",
       "                                                 roberta       0.139934   \n",
       "                                        standard bart          0.000000   \n",
       "                                                 roberta       0.123314   \n",
       "hiv                 selfies   atom      isomers  bart          0.045486   \n",
       "                                                 roberta       0.089119   \n",
       "                                        standard bart          0.039760   \n",
       "                                                 roberta       0.035285   \n",
       "                              trained   isomers  bart          0.085478   \n",
       "                                                 roberta       0.042242   \n",
       "                                        standard bart          0.041351   \n",
       "                                                 roberta       0.072541   \n",
       "                    smiles    atom      isomers  bart          0.060301   \n",
       "                                                 roberta       0.126309   \n",
       "                                        standard bart          0.061045   \n",
       "                                                 roberta       0.054167   \n",
       "                              trained   isomers  bart          0.127591   \n",
       "                                                 roberta       0.075474   \n",
       "                                        standard bart          0.157391   \n",
       "                                                 roberta       0.100222   \n",
       "tox21               selfies   atom      isomers  bart          0.000000   \n",
       "                                                 roberta       0.031924   \n",
       "                                        standard bart          0.012087   \n",
       "                                                 roberta       0.000000   \n",
       "                              trained   isomers  bart          0.026165   \n",
       "                                                 roberta       0.041515   \n",
       "                                        standard bart          0.000000   \n",
       "                                                 roberta       0.012252   \n",
       "                    smiles    atom      isomers  bart          0.011180   \n",
       "                                                 roberta       0.011616   \n",
       "                                        standard bart          0.000000   \n",
       "                                                 roberta       0.022644   \n",
       "                              trained   isomers  bart          0.000000   \n",
       "                                                 roberta       0.000000   \n",
       "                                        standard bart          0.000000   \n",
       "                                                 roberta       0.000000   \n",
       "\n",
       "                                                              accuracy_score  \\\n",
       "                                                                        mean   \n",
       "task                embedding tokenizer dataset  architecture                  \n",
       "bace_classification selfies   atom      isomers  bart               0.677632   \n",
       "                                                 roberta            0.631579   \n",
       "                                        standard bart               0.722368   \n",
       "                                                 roberta            0.686842   \n",
       "                              trained   isomers  bart               0.648684   \n",
       "                                                 roberta            0.686842   \n",
       "                                        standard bart               0.671053   \n",
       "                                                 roberta            0.605263   \n",
       "                    smiles    atom      isomers  bart               0.738158   \n",
       "                                                 roberta            0.621053   \n",
       "                                        standard bart               0.746053   \n",
       "                                                 roberta            0.611842   \n",
       "                              trained   isomers  bart               0.667105   \n",
       "                                                 roberta            0.665789   \n",
       "                                        standard bart               0.672368   \n",
       "                                                 roberta            0.602632   \n",
       "bbbp                selfies   atom      isomers  bart               0.600980   \n",
       "                                                 roberta            0.598039   \n",
       "                                        standard bart               0.587255   \n",
       "                                                 roberta            0.637255   \n",
       "                              trained   isomers  bart               0.567647   \n",
       "                                                 roberta            0.655882   \n",
       "                                        standard bart               0.592157   \n",
       "                                                 roberta            0.586275   \n",
       "                    smiles    atom      isomers  bart               0.601961   \n",
       "                                                 roberta            0.622549   \n",
       "                                        standard bart               0.586275   \n",
       "                                                 roberta            0.634314   \n",
       "                              trained   isomers  bart               0.562745   \n",
       "                                                 roberta            0.651961   \n",
       "                                        standard bart               0.577451   \n",
       "                                                 roberta            0.570588   \n",
       "clintox             selfies   atom      isomers  bart               0.894595   \n",
       "                                                 roberta            0.897297   \n",
       "                                        standard bart               0.932432   \n",
       "                                                 roberta            0.890541   \n",
       "                              trained   isomers  bart               0.932432   \n",
       "                                                 roberta            0.912162   \n",
       "                                        standard bart               0.932432   \n",
       "                                                 roberta            0.921622   \n",
       "                    smiles    atom      isomers  bart               0.931081   \n",
       "                                                 roberta            0.909459   \n",
       "                                        standard bart               0.894595   \n",
       "                                                 roberta            0.931081   \n",
       "                              trained   isomers  bart               0.932432   \n",
       "                                                 roberta            0.929730   \n",
       "                                        standard bart               0.932432   \n",
       "                                                 roberta            0.916216   \n",
       "hiv                 selfies   atom      isomers  bart               0.970621   \n",
       "                                                 roberta            0.970183   \n",
       "                                        standard bart               0.970183   \n",
       "                                                 roberta            0.968088   \n",
       "                              trained   isomers  bart               0.962241   \n",
       "                                                 roberta            0.969647   \n",
       "                                        standard bart               0.968867   \n",
       "                                                 roberta            0.968185   \n",
       "                    smiles    atom      isomers  bart               0.969062   \n",
       "                                                 roberta            0.970183   \n",
       "                                        standard bart               0.968624   \n",
       "                                                 roberta            0.970426   \n",
       "                              trained   isomers  bart               0.969160   \n",
       "                                                 roberta            0.966821   \n",
       "                                        standard bart               0.969988   \n",
       "                                                 roberta            0.968624   \n",
       "tox21               selfies   atom      isomers  bart               0.907908   \n",
       "                                                 roberta            0.905357   \n",
       "                                        standard bart               0.908163   \n",
       "                                                 roberta            0.908163   \n",
       "                              trained   isomers  bart               0.907398   \n",
       "                                                 roberta            0.908163   \n",
       "                                        standard bart               0.907653   \n",
       "                                                 roberta            0.908418   \n",
       "                    smiles    atom      isomers  bart               0.906122   \n",
       "                                                 roberta            0.907143   \n",
       "                                        standard bart               0.908163   \n",
       "                                                 roberta            0.907398   \n",
       "                              trained   isomers  bart               0.908163   \n",
       "                                                 roberta            0.908163   \n",
       "                                        standard bart               0.907908   \n",
       "                                                 roberta            0.908163   \n",
       "\n",
       "                                                                         \n",
       "                                                                    std  \n",
       "task                embedding tokenizer dataset  architecture            \n",
       "bace_classification selfies   atom      isomers  bart          0.008058  \n",
       "                                                 roberta       0.101814  \n",
       "                                        standard bart          0.009758  \n",
       "                                                 roberta       0.045959  \n",
       "                              trained   isomers  bart          0.112863  \n",
       "                                                 roberta       0.039910  \n",
       "                                        standard bart          0.029787  \n",
       "                                                 roberta       0.073995  \n",
       "                    smiles    atom      isomers  bart          0.012655  \n",
       "                                                 roberta       0.030363  \n",
       "                                        standard bart          0.016513  \n",
       "                                                 roberta       0.079221  \n",
       "                              trained   isomers  bart          0.103040  \n",
       "                                                 roberta       0.077620  \n",
       "                                        standard bart          0.083140  \n",
       "                                                 roberta       0.112959  \n",
       "bbbp                selfies   atom      isomers  bart          0.030925  \n",
       "                                                 roberta       0.019912  \n",
       "                                        standard bart          0.024362  \n",
       "                                                 roberta       0.016623  \n",
       "                              trained   isomers  bart          0.022570  \n",
       "                                                 roberta       0.015656  \n",
       "                                        standard bart          0.021198  \n",
       "                                                 roberta       0.010740  \n",
       "                    smiles    atom      isomers  bart          0.025091  \n",
       "                                                 roberta       0.012969  \n",
       "                                        standard bart          0.022623  \n",
       "                                                 roberta       0.026983  \n",
       "                              trained   isomers  bart          0.004101  \n",
       "                                                 roberta       0.021922  \n",
       "                                        standard bart          0.020032  \n",
       "                                                 roberta       0.011805  \n",
       "clintox             selfies   atom      isomers  bart          0.057054  \n",
       "                                                 roberta       0.036823  \n",
       "                                        standard bart          0.000000  \n",
       "                                                 roberta       0.049744  \n",
       "                              trained   isomers  bart          0.000000  \n",
       "                                                 roberta       0.011703  \n",
       "                                        standard bart          0.000000  \n",
       "                                                 roberta       0.024174  \n",
       "                    smiles    atom      isomers  bart          0.007402  \n",
       "                                                 roberta       0.028507  \n",
       "                                        standard bart          0.051812  \n",
       "                                                 roberta       0.007402  \n",
       "                              trained   isomers  bart          0.000000  \n",
       "                                                 roberta       0.007704  \n",
       "                                        standard bart          0.000000  \n",
       "                                                 roberta       0.040144  \n",
       "hiv                 selfies   atom      isomers  bart          0.001395  \n",
       "                                                 roberta       0.001658  \n",
       "                                        standard bart          0.002013  \n",
       "                                                 roberta       0.000959  \n",
       "                              trained   isomers  bart          0.011800  \n",
       "                                                 roberta       0.000443  \n",
       "                                        standard bart          0.000556  \n",
       "                                                 roberta       0.000611  \n",
       "                    smiles    atom      isomers  bart          0.001230  \n",
       "                                                 roberta       0.001176  \n",
       "                                        standard bart          0.004943  \n",
       "                                                 roberta       0.001137  \n",
       "                              trained   isomers  bart          0.002512  \n",
       "                                                 roberta       0.003173  \n",
       "                                        standard bart          0.001601  \n",
       "                                                 roberta       0.000902  \n",
       "tox21               selfies   atom      isomers  bart          0.000570  \n",
       "                                                 roberta       0.002614  \n",
       "                                        standard bart          0.000000  \n",
       "                                                 roberta       0.000000  \n",
       "                              trained   isomers  bart          0.001454  \n",
       "                                                 roberta       0.000902  \n",
       "                                        standard bart          0.000699  \n",
       "                                                 roberta       0.000570  \n",
       "                    smiles    atom      isomers  bart          0.003202  \n",
       "                                                 roberta       0.001663  \n",
       "                                        standard bart          0.000000  \n",
       "                                                 roberta       0.001711  \n",
       "                              trained   isomers  bart          0.000000  \n",
       "                                                 roberta       0.000000  \n",
       "                                        standard bart          0.000570  \n",
       "                                                 roberta       0.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_columns = list(classification_scores.columns[-0:])\n",
    "classification_columns.extend(classification_scores.columns[:-0])\n",
    "\n",
    "classification_scores[classification_columns].drop([\"Unnamed: 0\", \"task_type\", \"learning_rate\", \"dropout\", \"seed\"], axis=\"columns\").groupby([\"task\", \"embedding\", \"tokenizer\", \"dataset\", \"architecture\"]).agg([\"mean\", \"std\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce0af19",
   "metadata": {},
   "source": [
    "## Compare embeddings in classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3016691e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_751193/1861847883.py:1: FutureWarning: ['tokenizer', 'dataset', 'architecture'] did not aggregate successfully. If any error is raised this will raise in a future version of pandas. Drop these columns/ops to avoid this warning.\n",
      "  classification_scores[classification_columns].drop([\"Unnamed: 0\", \"task_type\", \"learning_rate\", \"dropout\", \"seed\"], axis=\"columns\").groupby([\"task\", \"embedding\"]).agg([\"mean\", \"std\"])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">ROC_AUC</th>\n",
       "      <th colspan=\"2\" halign=\"left\">average_precision</th>\n",
       "      <th colspan=\"2\" halign=\"left\">F1_score</th>\n",
       "      <th colspan=\"2\" halign=\"left\">accuracy_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>task</th>\n",
       "      <th>embedding</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">bace_classification</th>\n",
       "      <th>selfies</th>\n",
       "      <td>0.759855</td>\n",
       "      <td>0.066156</td>\n",
       "      <td>0.810314</td>\n",
       "      <td>0.067816</td>\n",
       "      <td>0.684494</td>\n",
       "      <td>0.125089</td>\n",
       "      <td>0.666283</td>\n",
       "      <td>0.067980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smiles</th>\n",
       "      <td>0.744198</td>\n",
       "      <td>0.077580</td>\n",
       "      <td>0.802665</td>\n",
       "      <td>0.066288</td>\n",
       "      <td>0.685415</td>\n",
       "      <td>0.153126</td>\n",
       "      <td>0.665625</td>\n",
       "      <td>0.084566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">bbbp</th>\n",
       "      <th>selfies</th>\n",
       "      <td>0.698555</td>\n",
       "      <td>0.017193</td>\n",
       "      <td>0.648492</td>\n",
       "      <td>0.023142</td>\n",
       "      <td>0.400001</td>\n",
       "      <td>0.100479</td>\n",
       "      <td>0.603186</td>\n",
       "      <td>0.033449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smiles</th>\n",
       "      <td>0.710709</td>\n",
       "      <td>0.025311</td>\n",
       "      <td>0.657626</td>\n",
       "      <td>0.029667</td>\n",
       "      <td>0.392216</td>\n",
       "      <td>0.098399</td>\n",
       "      <td>0.600980</td>\n",
       "      <td>0.035432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">clintox</th>\n",
       "      <th>selfies</th>\n",
       "      <td>0.729783</td>\n",
       "      <td>0.112011</td>\n",
       "      <td>0.244248</td>\n",
       "      <td>0.140037</td>\n",
       "      <td>0.145822</td>\n",
       "      <td>0.178455</td>\n",
       "      <td>0.914189</td>\n",
       "      <td>0.033073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smiles</th>\n",
       "      <td>0.694674</td>\n",
       "      <td>0.098455</td>\n",
       "      <td>0.235554</td>\n",
       "      <td>0.115675</td>\n",
       "      <td>0.125021</td>\n",
       "      <td>0.166694</td>\n",
       "      <td>0.922128</td>\n",
       "      <td>0.026809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">hiv</th>\n",
       "      <th>selfies</th>\n",
       "      <td>0.739530</td>\n",
       "      <td>0.021530</td>\n",
       "      <td>0.250335</td>\n",
       "      <td>0.066355</td>\n",
       "      <td>0.219313</td>\n",
       "      <td>0.128422</td>\n",
       "      <td>0.968502</td>\n",
       "      <td>0.004681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smiles</th>\n",
       "      <td>0.760518</td>\n",
       "      <td>0.015112</td>\n",
       "      <td>0.271701</td>\n",
       "      <td>0.049202</td>\n",
       "      <td>0.241985</td>\n",
       "      <td>0.121896</td>\n",
       "      <td>0.969111</td>\n",
       "      <td>0.002483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">tox21</th>\n",
       "      <th>selfies</th>\n",
       "      <td>0.676238</td>\n",
       "      <td>0.036409</td>\n",
       "      <td>0.206963</td>\n",
       "      <td>0.039682</td>\n",
       "      <td>0.011022</td>\n",
       "      <td>0.022976</td>\n",
       "      <td>0.907653</td>\n",
       "      <td>0.001409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smiles</th>\n",
       "      <td>0.677342</td>\n",
       "      <td>0.033682</td>\n",
       "      <td>0.185350</td>\n",
       "      <td>0.020969</td>\n",
       "      <td>0.002540</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>0.907653</td>\n",
       "      <td>0.001467</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                ROC_AUC           average_precision            \\\n",
       "                                   mean       std              mean       std   \n",
       "task                embedding                                                   \n",
       "bace_classification selfies    0.759855  0.066156          0.810314  0.067816   \n",
       "                    smiles     0.744198  0.077580          0.802665  0.066288   \n",
       "bbbp                selfies    0.698555  0.017193          0.648492  0.023142   \n",
       "                    smiles     0.710709  0.025311          0.657626  0.029667   \n",
       "clintox             selfies    0.729783  0.112011          0.244248  0.140037   \n",
       "                    smiles     0.694674  0.098455          0.235554  0.115675   \n",
       "hiv                 selfies    0.739530  0.021530          0.250335  0.066355   \n",
       "                    smiles     0.760518  0.015112          0.271701  0.049202   \n",
       "tox21               selfies    0.676238  0.036409          0.206963  0.039682   \n",
       "                    smiles     0.677342  0.033682          0.185350  0.020969   \n",
       "\n",
       "                               F1_score           accuracy_score            \n",
       "                                   mean       std           mean       std  \n",
       "task                embedding                                               \n",
       "bace_classification selfies    0.684494  0.125089       0.666283  0.067980  \n",
       "                    smiles     0.685415  0.153126       0.665625  0.084566  \n",
       "bbbp                selfies    0.400001  0.100479       0.603186  0.033449  \n",
       "                    smiles     0.392216  0.098399       0.600980  0.035432  \n",
       "clintox             selfies    0.145822  0.178455       0.914189  0.033073  \n",
       "                    smiles     0.125021  0.166694       0.922128  0.026809  \n",
       "hiv                 selfies    0.219313  0.128422       0.968502  0.004681  \n",
       "                    smiles     0.241985  0.121896       0.969111  0.002483  \n",
       "tox21               selfies    0.011022  0.022976       0.907653  0.001409  \n",
       "                    smiles     0.002540  0.009615       0.907653  0.001467  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_scores[classification_columns].drop([\"Unnamed: 0\", \"task_type\", \"learning_rate\", \"dropout\", \"seed\"], axis=\"columns\").groupby([\"task\", \"embedding\"]).agg([\"mean\", \"std\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94eb80f5",
   "metadata": {},
   "source": [
    "## Compare tokenizer in classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8260fe3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_751193/2433679349.py:1: FutureWarning: ['embedding', 'dataset', 'architecture'] did not aggregate successfully. If any error is raised this will raise in a future version of pandas. Drop these columns/ops to avoid this warning.\n",
      "  classification_scores[classification_columns].drop([\"Unnamed: 0\", \"task_type\", \"learning_rate\", \"dropout\", \"seed\"], axis=\"columns\").groupby([\"task\",\"tokenizer\"]).agg([\"mean\", \"std\"])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">ROC_AUC</th>\n",
       "      <th colspan=\"2\" halign=\"left\">average_precision</th>\n",
       "      <th colspan=\"2\" halign=\"left\">F1_score</th>\n",
       "      <th colspan=\"2\" halign=\"left\">accuracy_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>task</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">bace_classification</th>\n",
       "      <th>atom</th>\n",
       "      <td>0.772509</td>\n",
       "      <td>0.053676</td>\n",
       "      <td>0.828649</td>\n",
       "      <td>0.043528</td>\n",
       "      <td>0.701922</td>\n",
       "      <td>0.120616</td>\n",
       "      <td>0.679441</td>\n",
       "      <td>0.068191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trained</th>\n",
       "      <td>0.731544</td>\n",
       "      <td>0.082336</td>\n",
       "      <td>0.784330</td>\n",
       "      <td>0.078238</td>\n",
       "      <td>0.667987</td>\n",
       "      <td>0.154779</td>\n",
       "      <td>0.652467</td>\n",
       "      <td>0.082156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">bbbp</th>\n",
       "      <th>atom</th>\n",
       "      <td>0.706561</td>\n",
       "      <td>0.026957</td>\n",
       "      <td>0.650126</td>\n",
       "      <td>0.031877</td>\n",
       "      <td>0.422379</td>\n",
       "      <td>0.073536</td>\n",
       "      <td>0.608578</td>\n",
       "      <td>0.028313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trained</th>\n",
       "      <td>0.702703</td>\n",
       "      <td>0.016665</td>\n",
       "      <td>0.655992</td>\n",
       "      <td>0.020608</td>\n",
       "      <td>0.369839</td>\n",
       "      <td>0.113957</td>\n",
       "      <td>0.595588</td>\n",
       "      <td>0.038582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">clintox</th>\n",
       "      <th>atom</th>\n",
       "      <td>0.766884</td>\n",
       "      <td>0.108265</td>\n",
       "      <td>0.287576</td>\n",
       "      <td>0.140191</td>\n",
       "      <td>0.210206</td>\n",
       "      <td>0.191095</td>\n",
       "      <td>0.910135</td>\n",
       "      <td>0.037486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trained</th>\n",
       "      <td>0.657572</td>\n",
       "      <td>0.070872</td>\n",
       "      <td>0.192225</td>\n",
       "      <td>0.093349</td>\n",
       "      <td>0.060637</td>\n",
       "      <td>0.108929</td>\n",
       "      <td>0.926182</td>\n",
       "      <td>0.017539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">hiv</th>\n",
       "      <th>atom</th>\n",
       "      <td>0.753716</td>\n",
       "      <td>0.016714</td>\n",
       "      <td>0.271305</td>\n",
       "      <td>0.054502</td>\n",
       "      <td>0.257640</td>\n",
       "      <td>0.114743</td>\n",
       "      <td>0.969671</td>\n",
       "      <td>0.002174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trained</th>\n",
       "      <td>0.746332</td>\n",
       "      <td>0.024708</td>\n",
       "      <td>0.250731</td>\n",
       "      <td>0.062212</td>\n",
       "      <td>0.203659</td>\n",
       "      <td>0.130208</td>\n",
       "      <td>0.967942</td>\n",
       "      <td>0.004691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">tox21</th>\n",
       "      <th>atom</th>\n",
       "      <td>0.662789</td>\n",
       "      <td>0.034221</td>\n",
       "      <td>0.182305</td>\n",
       "      <td>0.022497</td>\n",
       "      <td>0.005721</td>\n",
       "      <td>0.015499</td>\n",
       "      <td>0.907302</td>\n",
       "      <td>0.001836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trained</th>\n",
       "      <td>0.690790</td>\n",
       "      <td>0.029792</td>\n",
       "      <td>0.210007</td>\n",
       "      <td>0.036801</td>\n",
       "      <td>0.007841</td>\n",
       "      <td>0.020364</td>\n",
       "      <td>0.908004</td>\n",
       "      <td>0.000718</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                ROC_AUC           average_precision            \\\n",
       "                                   mean       std              mean       std   \n",
       "task                tokenizer                                                   \n",
       "bace_classification atom       0.772509  0.053676          0.828649  0.043528   \n",
       "                    trained    0.731544  0.082336          0.784330  0.078238   \n",
       "bbbp                atom       0.706561  0.026957          0.650126  0.031877   \n",
       "                    trained    0.702703  0.016665          0.655992  0.020608   \n",
       "clintox             atom       0.766884  0.108265          0.287576  0.140191   \n",
       "                    trained    0.657572  0.070872          0.192225  0.093349   \n",
       "hiv                 atom       0.753716  0.016714          0.271305  0.054502   \n",
       "                    trained    0.746332  0.024708          0.250731  0.062212   \n",
       "tox21               atom       0.662789  0.034221          0.182305  0.022497   \n",
       "                    trained    0.690790  0.029792          0.210007  0.036801   \n",
       "\n",
       "                               F1_score           accuracy_score            \n",
       "                                   mean       std           mean       std  \n",
       "task                tokenizer                                               \n",
       "bace_classification atom       0.701922  0.120616       0.679441  0.068191  \n",
       "                    trained    0.667987  0.154779       0.652467  0.082156  \n",
       "bbbp                atom       0.422379  0.073536       0.608578  0.028313  \n",
       "                    trained    0.369839  0.113957       0.595588  0.038582  \n",
       "clintox             atom       0.210206  0.191095       0.910135  0.037486  \n",
       "                    trained    0.060637  0.108929       0.926182  0.017539  \n",
       "hiv                 atom       0.257640  0.114743       0.969671  0.002174  \n",
       "                    trained    0.203659  0.130208       0.967942  0.004691  \n",
       "tox21               atom       0.005721  0.015499       0.907302  0.001836  \n",
       "                    trained    0.007841  0.020364       0.908004  0.000718  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_scores[classification_columns].drop([\"Unnamed: 0\", \"task_type\", \"learning_rate\", \"dropout\", \"seed\"], axis=\"columns\").groupby([\"task\",\"tokenizer\"]).agg([\"mean\", \"std\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f61b32",
   "metadata": {},
   "source": [
    "## Compare dataset in classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17c81318",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_751193/1952053453.py:1: FutureWarning: ['embedding', 'tokenizer', 'architecture'] did not aggregate successfully. If any error is raised this will raise in a future version of pandas. Drop these columns/ops to avoid this warning.\n",
      "  classification_scores[classification_columns].drop([\"Unnamed: 0\", \"task_type\", \"learning_rate\", \"dropout\", \"seed\"], axis=\"columns\").groupby([\"task\", \"dataset\",]).agg([\"mean\", \"std\"])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">ROC_AUC</th>\n",
       "      <th colspan=\"2\" halign=\"left\">average_precision</th>\n",
       "      <th colspan=\"2\" halign=\"left\">F1_score</th>\n",
       "      <th colspan=\"2\" halign=\"left\">accuracy_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>task</th>\n",
       "      <th>dataset</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">bace_classification</th>\n",
       "      <th>isomers</th>\n",
       "      <td>0.782745</td>\n",
       "      <td>0.043118</td>\n",
       "      <td>0.838139</td>\n",
       "      <td>0.031093</td>\n",
       "      <td>0.670733</td>\n",
       "      <td>0.133195</td>\n",
       "      <td>0.667105</td>\n",
       "      <td>0.074474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>standard</th>\n",
       "      <td>0.721309</td>\n",
       "      <td>0.082014</td>\n",
       "      <td>0.774840</td>\n",
       "      <td>0.077470</td>\n",
       "      <td>0.699176</td>\n",
       "      <td>0.144704</td>\n",
       "      <td>0.664803</td>\n",
       "      <td>0.078890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">bbbp</th>\n",
       "      <th>isomers</th>\n",
       "      <td>0.707248</td>\n",
       "      <td>0.024803</td>\n",
       "      <td>0.658366</td>\n",
       "      <td>0.029178</td>\n",
       "      <td>0.411062</td>\n",
       "      <td>0.108473</td>\n",
       "      <td>0.607721</td>\n",
       "      <td>0.037587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>standard</th>\n",
       "      <td>0.702016</td>\n",
       "      <td>0.019566</td>\n",
       "      <td>0.647752</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.381156</td>\n",
       "      <td>0.087088</td>\n",
       "      <td>0.596446</td>\n",
       "      <td>0.029980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">clintox</th>\n",
       "      <th>isomers</th>\n",
       "      <td>0.737065</td>\n",
       "      <td>0.102397</td>\n",
       "      <td>0.251447</td>\n",
       "      <td>0.110970</td>\n",
       "      <td>0.161668</td>\n",
       "      <td>0.171331</td>\n",
       "      <td>0.917399</td>\n",
       "      <td>0.028523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>standard</th>\n",
       "      <td>0.687391</td>\n",
       "      <td>0.105458</td>\n",
       "      <td>0.228354</td>\n",
       "      <td>0.142976</td>\n",
       "      <td>0.109174</td>\n",
       "      <td>0.170551</td>\n",
       "      <td>0.918919</td>\n",
       "      <td>0.032096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">hiv</th>\n",
       "      <th>isomers</th>\n",
       "      <td>0.755031</td>\n",
       "      <td>0.021805</td>\n",
       "      <td>0.270852</td>\n",
       "      <td>0.049485</td>\n",
       "      <td>0.251864</td>\n",
       "      <td>0.102983</td>\n",
       "      <td>0.968490</td>\n",
       "      <td>0.004869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>standard</th>\n",
       "      <td>0.745017</td>\n",
       "      <td>0.019772</td>\n",
       "      <td>0.251184</td>\n",
       "      <td>0.066414</td>\n",
       "      <td>0.209434</td>\n",
       "      <td>0.141724</td>\n",
       "      <td>0.969123</td>\n",
       "      <td>0.002084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">tox21</th>\n",
       "      <th>isomers</th>\n",
       "      <td>0.674329</td>\n",
       "      <td>0.041998</td>\n",
       "      <td>0.197050</td>\n",
       "      <td>0.037962</td>\n",
       "      <td>0.010935</td>\n",
       "      <td>0.022917</td>\n",
       "      <td>0.907302</td>\n",
       "      <td>0.001836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>standard</th>\n",
       "      <td>0.679251</td>\n",
       "      <td>0.026161</td>\n",
       "      <td>0.195262</td>\n",
       "      <td>0.028481</td>\n",
       "      <td>0.002626</td>\n",
       "      <td>0.009830</td>\n",
       "      <td>0.908004</td>\n",
       "      <td>0.000718</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               ROC_AUC           average_precision            \\\n",
       "                                  mean       std              mean       std   \n",
       "task                dataset                                                    \n",
       "bace_classification isomers   0.782745  0.043118          0.838139  0.031093   \n",
       "                    standard  0.721309  0.082014          0.774840  0.077470   \n",
       "bbbp                isomers   0.707248  0.024803          0.658366  0.029178   \n",
       "                    standard  0.702016  0.019566          0.647752  0.023438   \n",
       "clintox             isomers   0.737065  0.102397          0.251447  0.110970   \n",
       "                    standard  0.687391  0.105458          0.228354  0.142976   \n",
       "hiv                 isomers   0.755031  0.021805          0.270852  0.049485   \n",
       "                    standard  0.745017  0.019772          0.251184  0.066414   \n",
       "tox21               isomers   0.674329  0.041998          0.197050  0.037962   \n",
       "                    standard  0.679251  0.026161          0.195262  0.028481   \n",
       "\n",
       "                              F1_score           accuracy_score            \n",
       "                                  mean       std           mean       std  \n",
       "task                dataset                                                \n",
       "bace_classification isomers   0.670733  0.133195       0.667105  0.074474  \n",
       "                    standard  0.699176  0.144704       0.664803  0.078890  \n",
       "bbbp                isomers   0.411062  0.108473       0.607721  0.037587  \n",
       "                    standard  0.381156  0.087088       0.596446  0.029980  \n",
       "clintox             isomers   0.161668  0.171331       0.917399  0.028523  \n",
       "                    standard  0.109174  0.170551       0.918919  0.032096  \n",
       "hiv                 isomers   0.251864  0.102983       0.968490  0.004869  \n",
       "                    standard  0.209434  0.141724       0.969123  0.002084  \n",
       "tox21               isomers   0.010935  0.022917       0.907302  0.001836  \n",
       "                    standard  0.002626  0.009830       0.908004  0.000718  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_scores[classification_columns].drop([\"Unnamed: 0\", \"task_type\", \"learning_rate\", \"dropout\", \"seed\"], axis=\"columns\").groupby([\"task\", \"dataset\",]).agg([\"mean\", \"std\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319d8694",
   "metadata": {},
   "source": [
    "## Compare architecture in classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dca46bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_751193/1637909655.py:1: FutureWarning: ['embedding', 'tokenizer', 'dataset'] did not aggregate successfully. If any error is raised this will raise in a future version of pandas. Drop these columns/ops to avoid this warning.\n",
      "  classification_scores[classification_columns].drop([\"Unnamed: 0\", \"task_type\", \"learning_rate\", \"dropout\", \"seed\"], axis=\"columns\").groupby([\"task\",\"architecture\"]).agg([\"mean\", \"std\"])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">ROC_AUC</th>\n",
       "      <th colspan=\"2\" halign=\"left\">average_precision</th>\n",
       "      <th colspan=\"2\" halign=\"left\">F1_score</th>\n",
       "      <th colspan=\"2\" halign=\"left\">accuracy_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>task</th>\n",
       "      <th>architecture</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">bace_classification</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.769778</td>\n",
       "      <td>0.056550</td>\n",
       "      <td>0.817660</td>\n",
       "      <td>0.056367</td>\n",
       "      <td>0.727605</td>\n",
       "      <td>0.112487</td>\n",
       "      <td>0.692928</td>\n",
       "      <td>0.066883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.734275</td>\n",
       "      <td>0.081711</td>\n",
       "      <td>0.795319</td>\n",
       "      <td>0.074766</td>\n",
       "      <td>0.642304</td>\n",
       "      <td>0.150700</td>\n",
       "      <td>0.638980</td>\n",
       "      <td>0.076203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">bbbp</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.696886</td>\n",
       "      <td>0.020274</td>\n",
       "      <td>0.638324</td>\n",
       "      <td>0.022517</td>\n",
       "      <td>0.363173</td>\n",
       "      <td>0.088698</td>\n",
       "      <td>0.584559</td>\n",
       "      <td>0.024566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.712378</td>\n",
       "      <td>0.021861</td>\n",
       "      <td>0.667794</td>\n",
       "      <td>0.022497</td>\n",
       "      <td>0.429044</td>\n",
       "      <td>0.098575</td>\n",
       "      <td>0.619608</td>\n",
       "      <td>0.033813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">clintox</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.660399</td>\n",
       "      <td>0.085407</td>\n",
       "      <td>0.176947</td>\n",
       "      <td>0.089576</td>\n",
       "      <td>0.058979</td>\n",
       "      <td>0.138639</td>\n",
       "      <td>0.922804</td>\n",
       "      <td>0.029784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.764058</td>\n",
       "      <td>0.100336</td>\n",
       "      <td>0.302855</td>\n",
       "      <td>0.129910</td>\n",
       "      <td>0.211863</td>\n",
       "      <td>0.169256</td>\n",
       "      <td>0.913514</td>\n",
       "      <td>0.030225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">hiv</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.753484</td>\n",
       "      <td>0.020009</td>\n",
       "      <td>0.289060</td>\n",
       "      <td>0.050538</td>\n",
       "      <td>0.304711</td>\n",
       "      <td>0.092789</td>\n",
       "      <td>0.968593</td>\n",
       "      <td>0.004985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.746564</td>\n",
       "      <td>0.022201</td>\n",
       "      <td>0.232975</td>\n",
       "      <td>0.053766</td>\n",
       "      <td>0.156587</td>\n",
       "      <td>0.108411</td>\n",
       "      <td>0.969019</td>\n",
       "      <td>0.001821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">tox21</th>\n",
       "      <th>bart</th>\n",
       "      <td>0.687217</td>\n",
       "      <td>0.028532</td>\n",
       "      <td>0.193280</td>\n",
       "      <td>0.027245</td>\n",
       "      <td>0.004583</td>\n",
       "      <td>0.013108</td>\n",
       "      <td>0.907685</td>\n",
       "      <td>0.001345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta</th>\n",
       "      <td>0.666362</td>\n",
       "      <td>0.037730</td>\n",
       "      <td>0.199032</td>\n",
       "      <td>0.038661</td>\n",
       "      <td>0.008979</td>\n",
       "      <td>0.021806</td>\n",
       "      <td>0.907621</td>\n",
       "      <td>0.001525</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   ROC_AUC           average_precision  \\\n",
       "                                      mean       std              mean   \n",
       "task                architecture                                         \n",
       "bace_classification bart          0.769778  0.056550          0.817660   \n",
       "                    roberta       0.734275  0.081711          0.795319   \n",
       "bbbp                bart          0.696886  0.020274          0.638324   \n",
       "                    roberta       0.712378  0.021861          0.667794   \n",
       "clintox             bart          0.660399  0.085407          0.176947   \n",
       "                    roberta       0.764058  0.100336          0.302855   \n",
       "hiv                 bart          0.753484  0.020009          0.289060   \n",
       "                    roberta       0.746564  0.022201          0.232975   \n",
       "tox21               bart          0.687217  0.028532          0.193280   \n",
       "                    roberta       0.666362  0.037730          0.199032   \n",
       "\n",
       "                                            F1_score           accuracy_score  \\\n",
       "                                       std      mean       std           mean   \n",
       "task                architecture                                                \n",
       "bace_classification bart          0.056367  0.727605  0.112487       0.692928   \n",
       "                    roberta       0.074766  0.642304  0.150700       0.638980   \n",
       "bbbp                bart          0.022517  0.363173  0.088698       0.584559   \n",
       "                    roberta       0.022497  0.429044  0.098575       0.619608   \n",
       "clintox             bart          0.089576  0.058979  0.138639       0.922804   \n",
       "                    roberta       0.129910  0.211863  0.169256       0.913514   \n",
       "hiv                 bart          0.050538  0.304711  0.092789       0.968593   \n",
       "                    roberta       0.053766  0.156587  0.108411       0.969019   \n",
       "tox21               bart          0.027245  0.004583  0.013108       0.907685   \n",
       "                    roberta       0.038661  0.008979  0.021806       0.907621   \n",
       "\n",
       "                                            \n",
       "                                       std  \n",
       "task                architecture            \n",
       "bace_classification bart          0.066883  \n",
       "                    roberta       0.076203  \n",
       "bbbp                bart          0.024566  \n",
       "                    roberta       0.033813  \n",
       "clintox             bart          0.029784  \n",
       "                    roberta       0.030225  \n",
       "hiv                 bart          0.004985  \n",
       "                    roberta       0.001821  \n",
       "tox21               bart          0.001345  \n",
       "                    roberta       0.001525  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_scores[classification_columns].drop([\"Unnamed: 0\", \"task_type\", \"learning_rate\", \"dropout\", \"seed\"], axis=\"columns\").groupby([\"task\",\"architecture\"]).agg([\"mean\", \"std\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
