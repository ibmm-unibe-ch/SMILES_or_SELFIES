{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9Ub6r9w7_Ag",
        "outputId": "ca373d30-91ce-4ea5-8872-89aedf19a10e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (1.13.1+cu116)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (0.14.1+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision) (2.25.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision) (8.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision) (1.22.4)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (1.26.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (2022.12.7)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.26.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (4.0.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: rdkit in /usr/local/lib/python3.9/dist-packages (2022.9.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.9/dist-packages (from rdkit) (8.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from rdkit) (1.22.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: selfies in /usr/local/lib/python3.9/dist-packages (2.1.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision\n",
        "!pip install transformers\n",
        "!pip install rdkit\n",
        "!pip install selfies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykFEgov_2Bih"
      },
      "outputs": [],
      "source": [
        "from rdkit import Chem, DataStructs\n",
        "from rdkit.Chem import AllChem\n",
        "from scipy.spatial.distance import cdist, cosine\n",
        "from scipy.stats import pearsonr, spearmanr\n",
        "from tqdm import tqdm\n",
        "from rdkit import Chem\n",
        "\n",
        "import logging\n",
        "import random\n",
        "from pathlib import Path\n",
        "from typing import Dict, Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import selfies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2ntOb1gKPx1",
        "outputId": "79a79062-a546-4b63-afa8-b9e821280721"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-2.4201, -1.6458, -2.8918,  ..., -1.0299, -1.1793, -3.9012],\n",
            "        [-1.9452, -1.2956, -1.2577,  ..., -0.9928, -3.0398, -3.3492],\n",
            "        [-2.4343, -1.0198, -3.4737,  ..., -1.7579, -0.1069, -3.0434],\n",
            "        ...,\n",
            "        [-2.8679, -1.6012, -1.9900,  ..., -1.5939, -0.1952, -2.5827],\n",
            "        [-2.5229, -2.5303, -3.0652,  ..., -2.5504, -1.7058, -3.2212],\n",
            "        [-2.2633, -2.4018, -2.4664,  ..., -1.0951, -1.1390, -2.8362]],\n",
            "       grad_fn=<SelectBackward0>)\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForMaskedLM, AutoTokenizer, pipeline\n",
        "\n",
        "#any model weights from the link above will work here\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
        "input = tokenizer.encode(\"Cc1cc(=O)[nH]c(=S)[nH]1\", return_tensors=\"pt\")\n",
        "output  = model(input)\n",
        "print(output[-1][-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7QZUy9BcRQvg"
      },
      "outputs": [],
      "source": [
        "testmol = \"c1cc2ccc3cccc4ccc(c1)c2c34\"\n",
        "testmol2 = \"Cc1cc(=O)[nH]c(=S)[nH]1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebUTP-lgR8s5"
      },
      "outputs": [],
      "source": [
        "def get_pairwise_chemical_similarities(mol_start, mol_end):\n",
        "    morgan_start = AllChem.GetMorganFingerprint(mol_start, 2)  # 2 is closest to ECFP\n",
        "    morgan_end = AllChem.GetMorganFingerprint(mol_end, 2)  # 2 is closest to ECFP\n",
        "    morgan_dice = DataStructs.DiceSimilarity(morgan_start, morgan_end)\n",
        "    morgan_start = AllChem.GetMorganFingerprintAsBitVect(\n",
        "        mol_start, 2\n",
        "    )  # count is needed for Tanimoto\n",
        "    morgan_end = AllChem.GetMorganFingerprintAsBitVect(\n",
        "        mol_end, 2\n",
        "    )  # count is needed for Tanimoto\n",
        "    morgan_tanimoto = DataStructs.FingerprintSimilarity(morgan_start, morgan_end)\n",
        "    rdkit_start = Chem.RDKFingerprint(mol_start)\n",
        "    rdkit_end = Chem.RDKFingerprint(mol_end)\n",
        "    rdkit_distance = DataStructs.FingerprintSimilarity(rdkit_start, rdkit_end)\n",
        "    return morgan_dice, morgan_tanimoto, rdkit_distance\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlgxqcJq0Opu"
      },
      "outputs": [],
      "source": [
        "def compute_distances(start: pd.Series, end: pd.Series) -> Tuple[float, float]:\n",
        "    euclid = cdist([start], [end], \"euclid\")\n",
        "    manhattan = cdist([start], [end], \"cityblock\")\n",
        "    cos = cosine(start, end)\n",
        "    return euclid[0][0], manhattan[0][0], cos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4hXMxCY0RT3"
      },
      "outputs": [],
      "source": [
        "def get_correlation(distances1, distances2, prefix: Optional[str] = None):\n",
        "    pearson = pearsonr(distances1, distances2)\n",
        "    spearman = spearmanr(distances1, distances2)\n",
        "    pearson_string = f\"{prefix} Pearson\" if prefix else \"Pearson\"\n",
        "    spearman_string = f\"{prefix} Spearman\" if prefix else \"Spearman\"\n",
        "    return {pearson_string: pearson, spearman_string: spearman}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n77_Cl9N0VPU"
      },
      "outputs": [],
      "source": [
        "def canonize_smile(input_str: str, remove_identities: bool = True) -> str:\n",
        "    \"\"\"Canonize SMILES string\n",
        "    Args:\n",
        "        input_str (str): SMILES input string\n",
        "    Returns:\n",
        "        str: canonize SMILES string\n",
        "    \"\"\"\n",
        "    mol = Chem.MolFromSmiles(input_str)\n",
        "    if mol is None:\n",
        "        return None\n",
        "    if remove_identities:\n",
        "        [a.SetAtomMapNum(0) for a in mol.GetAtoms()]\n",
        "    return Chem.MolToSmiles(mol)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_k7xeMN70bZX"
      },
      "outputs": [],
      "source": [
        "def sample_synonym(\n",
        "    seed: int = None, min_size: int = 5, max_size: int = 30\n",
        ") -> Tuple[str, str, str, str]:\n",
        "    random.seed(seed)\n",
        "    found_fitting = False\n",
        "    alphabet = (\n",
        "        selfies.get_semantic_robust_alphabet()\n",
        "    )  # Gets the alphabet of robust symbols\n",
        "    while not found_fitting:\n",
        "        size = random.randint(min_size, max_size)\n",
        "        rnd_selfies = \"\".join(random.sample(list(alphabet), size))\n",
        "        rnd_smiles = selfies.decoder(rnd_selfies)\n",
        "        rnd_smiles = rnd_smiles.replace(\"-1\", \"-\").replace(\"+1\", \"+\")\n",
        "        canon_smiles = canonize_smile(rnd_smiles)\n",
        "        rnd_selfies = selfies.encoder(rnd_smiles)\n",
        "        canon_selfies = selfies.encoder(canon_smiles)\n",
        "        if (canon_smiles != rnd_smiles) and (canon_selfies != rnd_selfies):\n",
        "            found_fitting = True\n",
        "    return rnd_smiles, canon_smiles, rnd_selfies, canon_selfies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tv9If06b0dL8"
      },
      "outputs": [],
      "source": [
        "def sample_random_molecules(\n",
        "    amount: int = 1000, overcompensation_factor: int = 1.1\n",
        ") -> pd.DataFrame:\n",
        "    molecule_list = []\n",
        "    SEED = 6217\n",
        "    for seed in range(SEED, int(SEED + amount * overcompensation_factor)):\n",
        "        molecule_list.append(sample_synonym(seed))\n",
        "    df = pd.DataFrame(\n",
        "        molecule_list,\n",
        "        columns=[\"rnd_smiles\", \"canon_smiles\", \"rnd_selfies\", \"canon_selfies\"],\n",
        "    )\n",
        "    df.drop_duplicates([\"canon_smiles\"], inplace=True)\n",
        "    if df.shape[0] < amount:\n",
        "        logging.info(\n",
        "            f\"Overcompensation factor of {overcompensation_factor} was not enough.\"\n",
        "        )\n",
        "        return sample_random_molecules(amount, overcompensation_factor * 1.1)\n",
        "    return df.iloc[:amount]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJljW6gX0fSw"
      },
      "outputs": [],
      "source": [
        "def get_embeddings(tokenizer, SMILES_starts_nn, SMILES_ends_nn):\n",
        "    SMILES_starts = np.array(SMILES_starts_nn)\n",
        "    SMILES_ends = np.array(SMILES_ends_nn)\n",
        "    mol_starts_tokenized = [tokenizer.encode(SMILES_start, return_tensors=\"pt\") for SMILES_start in SMILES_starts]\n",
        "    mol_ends_tokenized = [tokenizer.encode(SMILES_end, return_tensors=\"pt\") for SMILES_end in SMILES_ends]\n",
        "    mol_starts_out = [model(input)[-1][-1] for input in mol_starts_tokenized]\n",
        "    mol_ends_out = [model(input)[-1][-1] for input in mol_ends_tokenized]\n",
        "    #https://stackoverflow.com/questions/63323464/how-to-get-the-correct-embedding-from-roberta-transformers \n",
        "    #return mol_starts_out[-1][-1], mol_ends_out[-1][-1]\n",
        "    return mol_starts_out, mol_ends_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qnaztphi-lc"
      },
      "outputs": [],
      "source": [
        "def get_chemical_similarities(SMILES_starts, SMILES_ends):\n",
        "    SEED = 6217\n",
        "    mol_starts = [Chem.MolFromSmiles(SMILES_start) for SMILES_start in SMILES_starts]\n",
        "    mol_ends = [Chem.MolFromSmiles(SMILES_end) for SMILES_end in SMILES_ends]\n",
        "    similarities = np.array(\n",
        "                [\n",
        "                    get_pairwise_chemical_similarities(mol_start, mol_end)\n",
        "                    for (mol_start, mol_end) in zip(mol_starts, mol_ends)\n",
        "                ])\n",
        "    #np.concatenate(\n",
        "        #[\n",
        "            #np.array(\n",
        "            #    [\n",
        "            #        get_pairwise_chemical_similarities(mol_start, mol_end)\n",
        "            #        for (mol_start, mol_end) in zip(mol_starts, mol_ends)\n",
        "            #    ]\n",
        "            #),\n",
        "            #np.array(\n",
        "            #    [\n",
        "             #       get_pairwise_chemical_similarities(\n",
        "             #           mol_start, mol_ends[SEED % len(mol_ends)]\n",
        "             #       )\n",
        "             #       for mol_start in mol_starts\n",
        "            #    ]\n",
        "           # ),\n",
        "       # ]#,\n",
        "        #axis=1,\n",
        "    #)\n",
        "    return similarities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4dHzXPM10zT"
      },
      "source": [
        "MAIN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKQaCOEm13dl",
        "outputId": "30f0736c-e4d6-4337-b791-210ced44af96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[15:51:26] WARNING: not removing hydrogen atom without neighbors\n",
            "[15:51:26] WARNING: not removing hydrogen atom without neighbors\n",
            "[15:51:26] WARNING: not removing hydrogen atom without neighbors\n",
            "[15:51:27] WARNING: not removing hydrogen atom without neighbors\n",
            "[15:51:27] WARNING: not removing hydrogen atom without neighbors\n",
            "[15:51:27] WARNING: not removing hydrogen atom without neighbors\n",
            "[15:51:28] WARNING: not removing hydrogen atom without neighbors\n",
            "[15:51:28] WARNING: not removing hydrogen atom without neighbors\n",
            "[15:51:28] WARNING: not removing hydrogen atom without neighbors\n",
            "[15:51:28] WARNING: not removing hydrogen atom without neighbors\n",
            "[15:51:28] WARNING: not removing hydrogen atom without neighbors\n",
            "[15:51:29] WARNING: not removing hydrogen atom without neighbors\n",
            "[15:51:29] WARNING: not removing hydrogen atom without neighbors\n",
            "[15:51:29] WARNING: not removing hydrogen atom without neighbors\n",
            "[15:51:30] WARNING: not removing hydrogen atom without neighbors\n",
            "[15:51:30] WARNING: not removing hydrogen atom without neighbors\n",
            "[15:51:30] WARNING: not removing hydrogen atom without neighbors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "500\n"
          ]
        }
      ],
      "source": [
        "#sample random mols\n",
        "rndm_mols = sample_random_molecules(1000)\n",
        "#print(rndm_mols.loc[:,\"rnd_smiles\"])\n",
        "#print(rndm_mols.loc[:531,\"rnd_smiles\"])\n",
        "rndm_smiles = rndm_mols.loc[:,\"rnd_smiles\"]\n",
        "#split SMILES in starts and ends\n",
        "top = int(np.ceil(rndm_smiles.shape[0] / 2))\n",
        "bottom = int(rndm_smiles.shape[0] - top)\n",
        "smiles_starts = rndm_smiles.head(top)\n",
        "smiles_ends = rndm_smiles.tail(bottom)\n",
        "\n",
        "#get chemical similarities of SMILES\n",
        "chemical_similarities = get_chemical_similarities(smiles_starts,smiles_ends)\n",
        "morgan_dice = chemical_similarities[:, 0]\n",
        "morgan_tanimoto = chemical_similarities[:, 1]\n",
        "rdkit_distance = chemical_similarities[:, 2]\n",
        "#print(\"sims:\", sims)\n",
        "print(len(morgan_dice))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hI4PSsgnkDhX",
        "outputId": "98e7aee6-176c-4758-ae38-da2a0964226b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len smiles 500\n",
            "lenembs 500\n",
            "lendistances 500\n",
            "correlation morgan_dice and euclid\n",
            "{'Pearson': PearsonRResult(statistic=-0.2005003353464174, pvalue=6.240839434236782e-06), 'Spearman': SignificanceResult(statistic=-0.2239168620727789, pvalue=4.219573115203927e-07)}\n",
            "correlation morgan_dice and manhattan\n",
            "{'Pearson': PearsonRResult(statistic=-0.21016856044986923, pvalue=2.12875379912294e-06), 'Spearman': SignificanceResult(statistic=-0.23193513689950032, pvalue=1.5633747350410637e-07)}\n",
            "correlation morgan_dice and cosine\n",
            "{'Pearson': PearsonRResult(statistic=-0.19315278143490988, pvalue=1.3661004444972695e-05), 'Spearman': SignificanceResult(statistic=-0.2243577734008039, pvalue=3.9991513858116745e-07)}\n",
            "correlation morgan_tanimoto and euclid\n",
            "{'Pearson': PearsonRResult(statistic=-0.2083041663796247, pvalue=2.6298669340162356e-06), 'Spearman': SignificanceResult(statistic=-0.23349190225743494, pvalue=1.2838154109773206e-07)}\n",
            "correlation morgan_tanimoto and manhattan\n",
            "{'Pearson': PearsonRResult(statistic=-0.2185358092575176, pvalue=8.050386815894869e-07), 'Spearman': SignificanceResult(statistic=-0.24232479668213322, pvalue=4.088767971640514e-08)}\n",
            "correlation morgan_tanimoto and cosine\n",
            "{'Pearson': PearsonRResult(statistic=-0.20109192766510167, pvalue=5.851897687923358e-06), 'Spearman': SignificanceResult(statistic=-0.23421514185960823, pvalue=1.1709765542966922e-07)}\n",
            "correlation rdkit_distance and euclid\n",
            "{'Pearson': PearsonRResult(statistic=-0.13274937841070156, pvalue=0.0029383990577715445), 'Spearman': SignificanceResult(statistic=-0.23001759681939785, pvalue=1.9889613515131018e-07)}\n",
            "correlation rdkit_distance and manhattan\n",
            "{'Pearson': PearsonRResult(statistic=-0.13547803467306654, pvalue=0.0023989330207202287), 'Spearman': SignificanceResult(statistic=-0.23881225338170206, pvalue=6.47944015794379e-08)}\n",
            "correlation rdkit_distance and cosine\n",
            "{'Pearson': PearsonRResult(statistic=-0.12709877022366267, pvalue=0.004420915512853505), 'Spearman': SignificanceResult(statistic=-0.23093423789818346, pvalue=1.7731910841979492e-07)}\n"
          ]
        }
      ],
      "source": [
        "#get model\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
        "#input = tokenizer.encode(\"Cc1cc(=O)[nH]c(=S)[nH]1\", return_tensors=\"pt\")\n",
        "#output  = model(input)\n",
        "print(\"len smiles\",len(smiles_starts))\n",
        "#get their embeddings\n",
        "smiles_starts_embs,smiles_ends_embs = get_embeddings(tokenizer,smiles_starts,smiles_ends)\n",
        "#for sm in smiles_starts:\n",
        "#  print(sm)\n",
        "print(\"lenembs\",len(smiles_starts_embs))\n",
        "#turn to numpy arrays\n",
        "smiles_starts_embs = [em.detach().numpy() for em in smiles_starts_embs]\n",
        "smiles_ends_embs = [em.detach().numpy() for em in smiles_ends_embs]\n",
        "\n",
        "#smiles_ends_embs = smiles_ends_embs.numpy()\n",
        "#print(\"startslen\",len(smiles_starts_embs))\n",
        "#print(\"endslen\",len(smiles_ends_embs))\n",
        "#print(\"last token\",len(smiles_starts_embs[0][-1]))\n",
        "\n",
        "#compute pairwise distances of embeddings\n",
        "#dist_test=compute_distances(smiles_starts_embs[-1],smiles_starts_embs[-1])\n",
        "#print(dist_test)\n",
        "#it=1\n",
        "#for num in smiles_ends_embs:\n",
        "#  print(it)\n",
        "#  print(\"num\",len(num[-1]))\n",
        "distances = [compute_distances(emb_start[-1], emb_end[-1]) for (emb_start, emb_end) in zip(smiles_starts_embs, smiles_ends_embs)]\n",
        "#print(\"distances\",distances)\n",
        "print(\"lendistances\",len(distances))\n",
        "distances_des= ['euclid', 'manhattan', 'cosine']\n",
        "sims=[morgan_dice, morgan_tanimoto, rdkit_distance] \n",
        "sims_des=['morgan_dice', 'morgan_tanimoto', 'rdkit_distance'] \n",
        "iti=0\n",
        "for sim in sims:\n",
        "  for pos in range(len(distances_des)):\n",
        "    #print(pos)\n",
        "    print(f\"correlation {sims_des[iti]} and {distances_des[pos]}\")\n",
        "    cor=get_correlation(sim,[distance[pos] for distance in distances])\n",
        "    print(cor)\n",
        "  iti+=1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing\n"
      ],
      "metadata": {
        "id": "PHWYTZ92j9cP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input1 = tokenizer.encode(\"Cc1cc(=O)[nH]c(=S)[nH]1\", return_tensors='pt')\n",
        "output1  = model(input1)\n",
        "print(output1[-1][-1])\n",
        "input2 = tokenizer.encode(\"Cc1cc(=O)[nH]c(=S)[nH]1\", return_tensors='pt')\n",
        "output2  = model(input2)\n",
        "print(output2[-1][-1])\n",
        "conv = (output2[-1][-1]).detach().numpy()\n",
        "print(pd.DataFrame(conv))\n",
        "test = cdist(pd.DataFrame(conv),pd.DataFrame(conv),\"euclid\")\n",
        "print(\"euclid cdist test\",test)\n",
        "\n",
        "\n",
        "starttensor = output1[-1][-1].detach().numpy()\n",
        "#print(starttensor)\n",
        "startpd = pd.DataFrame(starttensor)\n",
        "#https://stackoverflow.com/questions/63323464/how-to-get-the-correct-embedding-from-roberta-transformers\n",
        "\n",
        "end = output2[-1][-1]\n",
        "euclid = cdist(starttensor, starttensor, \"euclid\")\n",
        "print(\"euclid\",euclid[0][0])\n",
        "manhattan = cdist(starttensor, starttensor, \"cityblock\")\n",
        "print(\"manha\",manhattan[0][0])\n",
        "cos = cdist(starttensor, starttensor, \"cosine\")\n",
        "print(\"cos\",cos[0][0])"
      ],
      "metadata": {
        "id": "LKhwOiCBj_Xb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "smil=[\"Cc1cc(=O)[nH]c(=S)[nH]1\"]\n",
        "smin = pd.DataFrame(smil)\n",
        "smiin=np.array(smin.iloc[:1])\n",
        "input2 = [tokenizer.encode(SMI, return_tensors=\"pt\") for SMI in smiin]\n",
        "#input2 = tokenizer.encode(smiin, return_tensors='pt')\n",
        "output2  = model(input2)\n",
        "conv = (output2[-1][-1]).detach().squeeze().numpy()\n",
        "dist_test = compute_distances(conv,conv)\n",
        "print(dist_test)"
      ],
      "metadata": {
        "id": "YGqrFpY0kIoH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}