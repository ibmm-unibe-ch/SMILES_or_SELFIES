{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention readout for examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jgut/miniconda3/envs/SMILES_OR_SELFIES/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Skipped loading some Tensorflow models, missing a dependency. No module named 'tensorflow'\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'torch_geometric'\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. cannot import name 'DMPNN' from 'deepchem.models.torch_models' (/home/jgut/miniconda3/envs/SMILES_OR_SELFIES/lib/python3.10/site-packages/deepchem/models/torch_models/__init__.py)\n",
      "Skipped loading modules with pytorch-lightning dependency, missing a dependency. No module named 'pytorch_lightning'\n",
      "Skipped loading some Jax models, missing a dependency. No module named 'jax'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from constants import TASK_MODEL_PATH, TASK_PATH, TOKENIZER_PATH, FAIRSEQ_PREPROCESS_PATH, MODEL_PATH\n",
    "from scoring import load_model\n",
    "from fairseq.data import Dictionary\n",
    "from preprocessing import canonize_smile, translate_selfie\n",
    "import re\n",
    "from tokenisation import get_tokenizer, tokenize_with_space\n",
    "from fairseq_utils import compute_attention_output\n",
    "pd.set_option('display.max_columns', 500)\n",
    "CUDA_DEVICE = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12.2\n"
     ]
    }
   ],
   "source": [
    "import fairseq\n",
    "print(fairseq.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/jgut/SMILES_or_SELFIES/fairseq/smiles_atom_isomers/checkpoint_last.pt\n"
     ]
    }
   ],
   "source": [
    "model_identifier = \"smiles_atom_isomers\"\n",
    "print(MODEL_PATH/model_identifier/\"checkpoint_last.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dictionary(target_dict_path):\n",
    "    return Dictionary.load(str(target_dict_path))\n",
    "\n",
    "def gather_attention_model(input, model_identifier):\n",
    "    tokenizer = get_tokenizer(TOKENIZER_PATH/model_identifier)\n",
    "    source_dictionary = get_dictionary(FAIRSEQ_PREPROCESS_PATH/model_identifier/\"dict.txt\")\n",
    "    print(TASK_PATH/\"delaney\"/model_identifier)\n",
    "    model = load_model(TASK_MODEL_PATH/\"delaney\"/model_identifier/\"1e-05_0.2_based_norm\"/\"checkpoint_best.pt\", TASK_PATH/\"delaney\"/model_identifier)\n",
    "    preprocessed = model.encode(input)\n",
    "    print(preprocessed)\n",
    "    attended = compute_attention_output([preprocessed], model, [input], source_dictionary, tokenizer)\n",
    "    return attended\n",
    "\n",
    "def to_markdown(molecule, smiles_atom, smiles_sentencepiece, selfies_atom, selfies_sentencepiece):\n",
    "    md = \"\"\n",
    "    md+=f'## Molecule'+\"\"\"\\ \"\"\"\n",
    "    input_data = np.array([(letter[1], f\"{letter[0]-1/len(molecule):.3f}\") for letter in molecule])\n",
    "    md+=f'{\"\".join(input_data[:,0])}'+\"\"\"\\ \"\"\"\n",
    "    md+=f'## SMILES atomwise'+\"\"\"\\ \"\"\"\n",
    "    input_data = np.array([(letter[1], f\"{letter[0]-1/len(smiles_atom):.3f}\") for letter in smiles_atom])\n",
    "    input_data2 = np.array([(letter[1], f\"{letter[0]:.3f}\") for letter in smiles_atom])\n",
    "    df = pd.concat([pd.DataFrame(data = input_data[:,1], index = input_data[:,0]).transpose(),pd.DataFrame(data = input_data2[:,1], index = input_data2[:,0]).transpose()])\n",
    "    md += df.to_markdown()+\"\"\"\\ \"\"\"\n",
    "    md+=f'## SELFIES atomwise'+\"\"\"\\ \"\"\"\n",
    "    input_data = np.array([(letter[1], f\"{letter[0]-1/len(selfies_atom):.3f}\") for letter in selfies_atom])\n",
    "    input_data2 = np.array([(letter[1], f\"{letter[0]:.3f}\") for letter in selfies_atom])\n",
    "    df = pd.concat([pd.DataFrame(data = input_data[:,1], index = input_data[:,0]).transpose(),pd.DataFrame(data = input_data2[:,1], index = input_data2[:,0]).transpose()])\n",
    "    md += df.to_markdown()+\"\"\"\\ \"\"\"\n",
    "    md+=f'## SMILES SentencePiece'+\"\"\"\\ \"\"\"\n",
    "    input_data = np.array([(letter[1], f\"{letter[0]-1/len(smiles_sentencepiece):.3f}\") for letter in smiles_sentencepiece])\n",
    "    input_data2 = np.array([(letter[1], f\"{letter[0]:.3f}\") for letter in smiles_sentencepiece])\n",
    "    df = pd.concat([pd.DataFrame(data = input_data[:,1], index = input_data[:,0]).transpose(),pd.DataFrame(data = input_data2[:,1], index = input_data2[:,0]).transpose()]) \n",
    "    md += df.to_markdown()+\"\"\"\\ \"\"\"\n",
    "    md+=f'## SELFIES SentencePiece'+\"\"\"\\ \"\"\"\n",
    "    input_data = np.array([(letter[1], f\"{letter[0]-1/len(selfies_sentencepiece):.3f}\") for letter in selfies_sentencepiece])\n",
    "    input_data2 = np.array([(letter[1], f\"{letter[0]:.3f}\") for letter in selfies_sentencepiece])\n",
    "    df = pd.concat([pd.DataFrame(data = input_data[:,1], index = input_data[:,0]).transpose(),pd.DataFrame(data = input_data2[:,1], index = input_data2[:,0]).transpose()]) \n",
    "    md += df.to_markdown()+\"\"\"\\ \"\"\"\n",
    "    return md\n",
    "\n",
    "def gather_attention(SMILES):\n",
    "    SMILES = canonize_smile(SMILES)\n",
    "    SELFIES = translate_selfie(SMILES)\n",
    "    smiles_atom = gather_attention_model(SMILES, \"smiles_atom_isomers\")\n",
    "    smiles_sentencepiece = gather_attention_model(SMILES, \"smiles_sentencepiece_isomers\")\n",
    "    selfies_atom = gather_attention_model(SELFIES, \"selfies_atom_isomers\")\n",
    "    selfies_sentencepiece = gather_attention_model(SELFIES, \"selfies_sentencepiece_isomers\")\n",
    "    markdown = to_markdown(smiles_atom, smiles_sentencepiece, selfies_atom, selfies_sentencepiece)\n",
    "    return markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/jgut/SMILES_or_SELFIES/task/delaney/smiles_atom_isomers\n",
      "tensor([0, 3, 3, 3, 2])\n",
      "tensor([[0, 3, 3, 3, 2]])\n",
      "tensor([[2, 0, 3, 3, 3]])\n",
      "RobertaHubInterface(\n",
      "  (model): RobertaModel(\n",
      "    (encoder): RobertaEncoder(\n",
      "      (sentence_encoder): TransformerEncoder(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (embed_tokens): Embedding(433, 768, padding_idx=1)\n",
      "        (embed_positions): LearnedPositionalEmbedding(514, 768, padding_idx=1)\n",
      "        (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (layers): ModuleList(\n",
      "          (0-11): 12 x TransformerEncoderLayerBase(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (dropout_module): FairseqDropout()\n",
      "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout_module): FairseqDropout()\n",
      "            (activation_dropout_module): FairseqDropout()\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (lm_head): RobertaLMHead(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (classification_heads): ModuleDict(\n",
      "      (sentence_classification_head): RobertaClassificationHead(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (out_proj): Linear(in_features=768, out_features=1, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "(tensor([[[ 0.0504,  0.0000, -0.6268,  ...,  0.8105, -0.4244, -0.6945],\n",
      "         [-0.0061,  0.0000, -0.1173,  ...,  0.7358, -0.5082, -0.2713],\n",
      "         [-0.0673,  0.0000, -0.2071,  ...,  0.6758, -0.4040, -0.4225],\n",
      "         [-0.3242,  0.0000, -0.4813,  ...,  0.7311, -0.3391, -0.5213],\n",
      "         [-0.0791,  0.0000, -0.2847,  ...,  0.6861, -0.4971, -0.8739]]],\n",
      "       grad_fn=<AddBackward0>), {'inner_states': [tensor([[[-1.8837,  0.2365, -2.0417,  ..., -0.4933,  0.8513,  0.2970]],\n",
      "\n",
      "        [[-1.0057, -0.0743,  0.3382,  ..., -1.1529,  1.2504, -0.8178]],\n",
      "\n",
      "        [[-0.7335, -0.5600,  2.0334,  ..., -1.1157,  1.4975, -1.1655]],\n",
      "\n",
      "        [[-0.9070, -0.8964, -0.5339,  ..., -0.3531,  2.0599, -0.7843]],\n",
      "\n",
      "        [[ 0.2510, -0.2778,  0.2316,  ..., -0.4620, -0.8278, -0.6542]]],\n",
      "       grad_fn=<TransposeBackward0>), tensor([[[-1.9297,  0.3952, -2.1190,  ..., -0.6957,  1.1052,  0.2380]],\n",
      "\n",
      "        [[-1.6309,  0.5375, -0.2216,  ..., -1.2862,  1.0616, -0.9204]],\n",
      "\n",
      "        [[-1.1542, -0.4532,  2.0239,  ..., -1.6306,  1.6542, -1.4877]],\n",
      "\n",
      "        [[-1.1956, -0.0688, -0.6805,  ..., -0.6243,  2.1488, -0.3918]],\n",
      "\n",
      "        [[ 0.2688,  0.0869,  0.0658,  ..., -0.4605, -0.8783, -0.5278]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-1.9234,  0.4530, -2.2557,  ..., -1.1497,  1.0793,  0.6691]],\n",
      "\n",
      "        [[-1.7990,  0.3778, -0.1809,  ..., -1.8129,  0.6351, -0.3525]],\n",
      "\n",
      "        [[-1.1076, -0.4072,  2.4620,  ..., -2.0285,  1.2336, -0.7337]],\n",
      "\n",
      "        [[-1.5718,  0.0984, -0.5136,  ..., -1.0650,  1.3330,  0.0579]],\n",
      "\n",
      "        [[ 0.0047, -0.0356, -0.1787,  ..., -0.7143, -0.9036,  0.0181]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-1.6074,  0.7178, -2.0614,  ..., -1.6673,  1.7870,  0.1878]],\n",
      "\n",
      "        [[-0.9202,  0.2253,  0.0304,  ..., -1.4268,  0.4092, -0.2713]],\n",
      "\n",
      "        [[-0.6674, -0.7104,  2.5122,  ..., -1.6891,  0.9493, -0.6463]],\n",
      "\n",
      "        [[-1.4545,  0.2482, -0.1457,  ..., -1.0298,  1.8079, -0.0734]],\n",
      "\n",
      "        [[ 0.1009,  0.4386,  0.4836,  ..., -1.0413, -0.8132, -0.2482]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.8472,  1.1478, -2.5341,  ..., -1.7191,  1.4598, -0.7400]],\n",
      "\n",
      "        [[-0.1374,  0.8275, -0.4607,  ..., -1.0757,  0.0759, -0.3687]],\n",
      "\n",
      "        [[ 0.2310, -0.0991,  2.1159,  ..., -1.3779,  0.3721, -1.0745]],\n",
      "\n",
      "        [[-0.3280,  1.0524, -0.6330,  ..., -0.5971,  1.5020, -0.5246]],\n",
      "\n",
      "        [[ 0.7614,  0.8969,  0.0167,  ..., -0.8825, -0.8348, -0.6304]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.3402,  1.2045, -1.8735,  ..., -1.1344,  1.6352, -0.6254]],\n",
      "\n",
      "        [[ 0.2779,  1.2951, -0.5333,  ..., -0.0983,  0.4542, -0.4275]],\n",
      "\n",
      "        [[ 0.2155,  0.4469,  1.7509,  ..., -0.5901,  0.3913, -0.9620]],\n",
      "\n",
      "        [[-0.4863,  1.2482, -0.3686,  ...,  0.2310,  1.5799, -0.5409]],\n",
      "\n",
      "        [[ 0.8902,  0.5096, -0.0638,  ...,  0.0914, -1.0825, -0.7369]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.7928,  1.1099, -1.1055,  ..., -1.3191,  2.0073, -1.0956]],\n",
      "\n",
      "        [[ 0.3202,  1.6223, -0.2880,  ..., -0.3069,  0.7970, -0.6290]],\n",
      "\n",
      "        [[-0.1378,  0.7529,  1.8273,  ..., -0.5286,  0.8344, -1.8005]],\n",
      "\n",
      "        [[-0.4781,  1.2206, -0.1999,  ..., -0.0072,  2.0898, -1.2132]],\n",
      "\n",
      "        [[ 0.9471,  0.9033,  0.0126,  ..., -0.6505, -0.4767, -0.8436]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-1.5717,  0.7074, -1.1224,  ..., -1.2808,  1.5718, -0.7602]],\n",
      "\n",
      "        [[-0.0176,  1.7088, -0.9864,  ..., -0.7778,  0.2741, -0.7963]],\n",
      "\n",
      "        [[-0.6961,  0.8805,  1.1603,  ..., -1.0325,  0.0419, -1.3519]],\n",
      "\n",
      "        [[-0.4483,  1.2009, -0.7333,  ...,  0.0450,  1.1678, -1.0063]],\n",
      "\n",
      "        [[ 0.5865,  0.6031, -0.3503,  ..., -1.0576, -0.4848, -0.8143]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-1.9402,  0.3491, -0.7331,  ..., -0.3015,  1.6645, -0.8738]],\n",
      "\n",
      "        [[-0.1128,  1.1558, -0.7273,  ...,  0.1230,  1.0384, -0.7187]],\n",
      "\n",
      "        [[-0.3884,  0.4724,  1.2078,  ..., -0.1810,  1.0663, -1.7132]],\n",
      "\n",
      "        [[-0.6389,  0.6848, -0.4556,  ...,  0.5443,  1.5333, -0.8476]],\n",
      "\n",
      "        [[ 0.1881, -0.1407, -0.5769,  ..., -0.1861, -0.1443, -1.0012]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.9443,  0.3239,  0.5390,  ..., -0.3701,  1.2493, -0.2879]],\n",
      "\n",
      "        [[ 0.5340,  1.3130, -0.4042,  ..., -0.6023,  0.7198, -0.6517]],\n",
      "\n",
      "        [[ 0.2272,  0.6140,  1.3217,  ..., -0.7090,  0.5590, -1.5056]],\n",
      "\n",
      "        [[ 0.0671,  0.7595, -0.4583,  ..., -0.1565,  1.1475, -0.6819]],\n",
      "\n",
      "        [[ 0.7854, -0.0256, -0.0634,  ..., -0.4516, -0.4631, -0.3947]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-1.1102,  0.2640,  0.3222,  ..., -0.3192,  1.2311,  0.6497]],\n",
      "\n",
      "        [[ 0.3088,  1.1308, -0.0301,  ..., -0.3515,  1.0006,  0.8001]],\n",
      "\n",
      "        [[ 0.0098,  0.2050,  1.1841,  ..., -0.4347,  0.6813, -0.3017]],\n",
      "\n",
      "        [[ 0.0654,  0.4238, -0.2154,  ..., -0.1680,  1.1018,  0.4577]],\n",
      "\n",
      "        [[ 0.6066, -0.2434, -0.1629,  ..., -0.5168, -0.5582,  0.2856]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.2693,  0.1658,  0.2813,  ..., -0.2354,  1.3287,  0.0460]],\n",
      "\n",
      "        [[ 0.9623,  0.6702,  0.0947,  ..., -0.2903,  1.1047,  0.2448]],\n",
      "\n",
      "        [[ 0.5384, -0.0692,  0.8027,  ..., -0.2503,  0.6823, -0.3828]],\n",
      "\n",
      "        [[ 0.6292,  0.0960, -0.0859,  ..., -0.0654,  1.0691,  0.0473]],\n",
      "\n",
      "        [[ 1.0971, -0.4183, -0.2431,  ..., -0.2315,  0.0596, -0.1521]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-1.2542,  0.0089,  0.0533,  ..., -1.2285,  0.8196, -0.2908]],\n",
      "\n",
      "        [[-0.3421,  0.4605,  0.2058,  ..., -1.0921,  0.7241, -0.2811]],\n",
      "\n",
      "        [[-0.6114, -0.1599,  0.5615,  ..., -1.0873,  0.1306, -0.8042]],\n",
      "\n",
      "        [[-0.4629, -0.0677,  0.0373,  ..., -0.7351,  0.4687, -0.4209]],\n",
      "\n",
      "        [[ 0.0840, -0.7175, -0.2108,  ..., -1.0045, -0.3491, -0.4582]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)]})\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgather_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCCOCC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 44\u001b[0m, in \u001b[0;36mgather_attention\u001b[0;34m(SMILES)\u001b[0m\n\u001b[1;32m     42\u001b[0m SMILES \u001b[38;5;241m=\u001b[39m canonize_smile(SMILES)\n\u001b[1;32m     43\u001b[0m SELFIES \u001b[38;5;241m=\u001b[39m translate_selfie(SMILES)\n\u001b[0;32m---> 44\u001b[0m smiles_atom \u001b[38;5;241m=\u001b[39m \u001b[43mgather_attention_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSMILES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msmiles_atom_isomers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m smiles_sentencepiece \u001b[38;5;241m=\u001b[39m gather_attention_model(SMILES, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msmiles_sentencepiece_isomers\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     46\u001b[0m selfies_atom \u001b[38;5;241m=\u001b[39m gather_attention_model(SELFIES, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mselfies_atom_isomers\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 11\u001b[0m, in \u001b[0;36mgather_attention_model\u001b[0;34m(input, model_identifier)\u001b[0m\n\u001b[1;32m      9\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(preprocessed)\n\u001b[0;32m---> 11\u001b[0m attended \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_attention_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpreprocessed\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_dictionary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attended\n",
      "File \u001b[0;32m/data/jgut/SMILES_or_SELFIES/fairseq_utils.py:251\u001b[0m, in \u001b[0;36mcompute_attention_output\u001b[0;34m(dataset, model, text, source_dictionary, tokenizer)\u001b[0m\n\u001b[1;32m    249\u001b[0m     extra \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmodel(sample\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device), return_all_hiddens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, features_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;66;03m#None, prev_output_tokens)\u001b[39;00m\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28mprint\u001b[39m(extra)\n\u001b[0;32m--> 251\u001b[0m     token_attention \u001b[38;5;241m=\u001b[39m \u001b[43mextra\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    252\u001b[0m     dataset_attentions\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(token_attention, parsed_tokens)))\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dataset_attentions\n",
      "\u001b[0;31mTypeError\u001b[0m: tuple indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "gather_attention(\"CCOCC\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
