# Smiles or Selfies

## Training pipeline

1. Download data with _make_
1. Preprocess with _preprocessing.py_
1. Create tokenisers with _tokenisation.py_
1. Parse with _parsing.py_
1. Run `fairseq-preprocess --only-source --destdir fairseq_preprocess/selfies_atom_isomers --trainpref processed/selfies_atom_isomers  --validpref processed/selfies_atom_isomers_val`
1. Run `fairseq-train fairseq_preprocess/selfies_atom_isomers --save-dir fairseq/selfies_atom_isomers --wandb-project pre-train --batch-size 32 --tokens-per-sample 512 --total-num-update 500000 --max-update 500000 --warmup-updates 1500 --task masked_lm --save-interval 1 --arch bert_base --optimizer adam --lr-scheduler polynomial_decay --lr 1e-05 --dropout 0.1 --criterion masked_lm --max-tokens 3200 --weight-decay 0.01 --attention-dropout 0.2 --clip-norm 1.0 --skip-invalid-size-inputs-valid-test --log-format json --log-interval 1000 --save-interval-updates 5000 --keep-interval-updates 1 --update-freq 4 --seed 4 --distributed-world-size 1 --no-epoch-checkpoints --dataset-impl mmap --num-workers 4`
1. Create MolNet datasets with _dataset.py_
1. Run finetuning with _finetuning.py_ giving the CUDA-GPU and the model configuration
1. Get scores with _scoring.py_
1. Translate model from fairseq to huggingface`python3 fairseq_to_huggingface.py ~/GitHub/SMILES_or_SELFIES/fairseq/smiles_trained/checkpoint_best.pt huggingface_models/smiles_trained --hf_config bart-base`

## Training SpanBERT
# Preprocessing
Step 4) `python preprocess.py --only-source --trainpref /data/ifender/SMILES_or_SELFIES/spanbert/SpanBERT/pretraining/preprocessed_data/orig/selfies_atom_isomers --validpref /data/ifender/SMILES_or_SELFIES/spanbert/SpanBERT/pretraining/preprocessed_data/orig/selfies_atom_isomers_val --srcdict /data/ifender/SMILES_or_SELFIES/spanbert/SpanBERT/pretraining/preprocessed_data/orig/dict.txt --destdir /data/ifender/SMILES_or_SELFIES/spanbert/SpanBERT/pretraining/preprocessed_data --padding-factor 1`
# preprocessing trial 2, but no srcdict
`python preprocess.py --only-source --trainpref /data/ifender/SMILES_or_SELFIES/spanbert/SpanBERT/pretraining/preprocessed_data/orig/selfies_atom_isomers --validpref /data/ifender/SMILES_or_SELFIES/spanbert/SpanBERT/pretraining/preprocessed_data/orig/selfies_atom_isomers_val  --destdir /data/ifender/SMILES_or_SELFIES/spanbert/SpanBERT/pretraining/preprocessed_data --padding-factor 1`
# Pretraining
Step 5) running on the newly processed data:
`python train.py /data/ifender/SMILES_or_SELFIES/spanbert/SpanBERT/pretraining/preprocessed_data/ --total-num-update 2400000 --max-update 2400000  --save-interval 1 --arch cased_bert_pair --task  span_bert --optimizer adam --lr-scheduler polynomial_decay --lr 0.0000001 --min-lr 1e-09  --criterion span_bert_loss  --max-tokens 4096 --tokens-per-sample 512 --weight-decay 0.01  --skip-invalid-size-inputs-valid-test --log-format json --log-interval 2000 --save-interval-updates 50000 --keep-interval-updates 50000 --update-freq 1 --seed 1 --save-dir /data/ifender/SMILES_or_SELFIES/selfies_atom_isomers/ --warmup-updates 10000 --schemes [\"pair_span\"] --span-lower 1 --span-upper 10 --validate-interval 1  --clip-norm 1.0 --geometric-p 0.2 --adam-eps 1e-8 --short-seq-prob 0.0 --replacement-method span --clamp-attention --no-nsp --pair-loss-weight 1.0 --max-pair-targets 15 --pair-positional-embedding-size 200 --endpoints external`
# changing max-tokens, changing tokens per sample to 32 as well
`python train.py /data/ifender/SMILES_or_SELFIES/spanbert/SpanBERT/pretraining/preprocessed_data --total-num-update 2400000 --max-update 2400000  --save-interval 1 --arch cased_bert_pair --task  span_bert --optimizer adam --lr-scheduler polynomial_decay --lr 0.01 --min-lr 1e-09  --criterion span_bert_loss  --max-tokens 32 --tokens-per-sample 8 --weight-decay 0.01 --log-format json --log-interval 200 --save-interval-updates 50 --keep-interval-updates 50 --update-freq 1 --seed 1 --save-dir /data/ifender/SMILES_or_SELFIES/selfies_atom_isomers/ --warmup-updates 100 --schemes [\"pair_span\"] --span-lower 1 --span-upper 10 --validate-interval 1  --clip-norm 1.0 --geometric-p 0.2 --adam-eps 1e-8 --short-seq-prob 0.0 --replacement-method span --clamp-attention --no-nsp --pair-loss-weight 1.0 --max-pair-targets 15 --pair-positional-embedding-size 200 --endpoints external`
(and the original SpanBERT pretraining was: 
"python train.py /path/to/preprocessed_data --total-num-update 2400000 --max-update 2400000  --save-interval 1 --arch cased_bert_pair_large --task  span_bert --optimizer adam --lr-scheduler polynomial_decay --lr 0.0001 --min-lr 1e-09  --criterion span_bert_loss  --max-tokens 4096 --tokens-per-sample 512 --weight-decay 0.01  --skip-invalid-size-inputs-valid-test --log-format json --log-interval 2000 --save-interval-updates 50000 --keep-interval-updates 50000 --update-freq 1 --seed 1 --save-dir /path/to/checkpoint_dir --fp16 --warmup-updates 10000 --schemes [\"pair_span\"] --distributed-port 12580 --distributed-world-size 32 --span-lower 1 --span-upper 10 --validate-interval 1  --clip-norm 1.0 --geometric-p 0.2 --adam-eps 1e-8 --short-seq-prob 0.0 --replacement-method span --clamp-attention --no-nsp --pair-loss-weight 1.0 --max-pair-targets 15 --pair-positional-embedding-size 200 --endpoints external", notice how --distributed-port 12580 --distributed-world-size 32 as well as --fp16 were left out in our training)
Step 8) Potentially changes to finetuning


### with old preprocessed data
'python train.py /data/ifender/SMILES_or_SELFIES/spanbert/SpanBERT/pretraining/preprocessed_data/selfies_atom_isomers/old/ --total-num-update 2400000 --max-update 2400000  --save-interval 1 --arch cased_bert_pair --task  span_bert --optimizer adam --lr-scheduler polynomial_decay --lr 0.0001 --min-lr 1e-09  --criterion span_bert_loss  --max-tokens 4096 --tokens-per-sample 512 --weight-decay 0.01  --skip-invalid-size-inputs-valid-test --log-format json --log-interval 2000 --save-interval-updates 50000 --keep-interval-updates 50000 --update-freq 1 --seed 1 --save-dir /data/ifender/SMILES_or_SELFIES/selfies_atom_isomers/ --warmup-updates 10000 --schemes [\"pair_span\"] --span-lower 1 --span-upper 10 --validate-interval 1  --clip-norm 1.0 --geometric-p 0.2 --adam-eps 1e-8 --short-seq-prob 0.0 --replacement-method span --clamp-attention --no-nsp --pair-loss-weight 1.0 --max-pair-targets 15 --pair-positional-embedding-size 200 --endpoints external'
## with Janniks roiginal
python train.py /data/ifender/SMILES_or_SELFIES/spanbert/SpanBERT/pretraining/preprocessed_data/selfies_atom_isomers/old/ --total-num-update 2400000 --max-update 2400000  --save-interval 1 --arch cased_bert_pair --task  span_bert --optimizer adam --lr-scheduler polynomial_decay --lr 0.0001 --min-lr 1e-09  --criterion span_bert_loss  --max-tokens 4096 --tokens-per-sample 512 --weight-decay 0.01  --skip-invalid-size-inputs-valid-test --log-format json --log-interval 2000 --save-interval-updates 50000 --keep-interval-updates 50000 --update-freq 1 --seed 1 --save-dir /data/ifender/SMILES_or_SELFIES/selfies_atom_isomers/ --warmup-updates 10000 --schemes [\"pair_span\"] --span-lower 1 --span-upper 10 --validate-interval 1  --clip-norm 1.0 --geometric-p 0.2 --adam-eps 1e-8 --short-seq-prob 0.0 --replacement-method span --clamp-attention --no-nsp --pair-loss-weight 1.0 --max-pair-targets 15 --pair-positional-embedding-size 200 --endpoints external
## with Janniks SMILES preprocessed data standard
python train.py /data/ifender/SMILES_or_SELFIES/prepr_data/fairseq_preprocessed/ --total-num-update 2400000 --max-update 2400000  --save-interval 1 --arch cased_bert_pair --task  span_bert --optimizer adam --lr-scheduler polynomial_decay --lr 0.0001 --min-lr 1e-09  --criterion span_bert_loss  --max-tokens 4096 --tokens-per-sample 512 --weight-decay 0.01  --skip-invalid-size-inputs-valid-test --log-format json --log-interval 2000 --save-interval-updates 50000 --keep-interval-updates 50000 --update-freq 1 --seed 1 --save-dir /data/ifender/SMILES_or_SELFIES/train_out --warmup-updates 10000 --schemes [\"pair_span\"] --span-lower 1 --span-upper 10 --validate-interval 1  --clip-norm 1.0 --geometric-p 0.2 --adam-eps 1e-8 --short-seq-prob 0.0 --replacement-method span --clamp-attention --no-nsp --pair-loss-weight 1.0 --max-pair-targets 15 --pair-positional-embedding-size 200 --endpoints external

## original preprocessing:
python preprocess.py --only-source --trainpref /path/to/train.txt --validpref path/to/valid.txt --srcdict /path/to/dict.txt --destdir /path/to/destination_dir --padding-factor 1 --workers 48
# we dont have a sourcedict, so do without and preprocess again, also leave out the workers
# and not adjusted to fit preprocessing here:
python preprocess.py --only-source --trainpref /data/ifender/SMILES_or_SELFIES/prepr_data/notfairs_preprocessed/smiles_atom_standard --validpref /data/ifender/SMILES_or_SELFIES/prepr_data/notfairs_preprocessed/smiles_atom_standard_val --destdir /data/ifender/SMILES_or_SELFIES/prepr_data/fairseq_preprocessed/new_smilesatomstd/ --padding-factor 1

## for minimal examples:
python preprocess.py --only-source --trainpref /data/ifender/SMILES_or_SELFIES/prepr_data/notfairs_preprocessed/smiles_atom_std_min --validpref /data/ifender/SMILES_or_SELFIES/prepr_data/notfairs_preprocessed/smiles_atom_std_val_min --destdir /data/ifender/SMILES_or_SELFIES/prepr_data/fairseq_preprocessed/smilesatomstd_min/ --padding-factor 1
--> that worked!
----> zip it!
## preprocessing again but this time trying to include the previously created dict.txt, but keep tokenised data:
python preprocess.py --only-source --trainpref /data/ifender/SMILES_or_SELFIES/prepr_data/notfairs_preprocessed/smiles_atom_std_min --validpref /data/ifender/SMILES_or_SELFIES/prepr_data/notfairs_preprocessed/smiles_atom_std_val_min --srcdict /data/ifender/SMILES_or_SELFIES/prepr_data/fairseq_preprocessed/smilesatomstd_min/dict.txt --destdir /data/ifender/SMILES_or_SELFIES/prepr_data/fairseq_preprocessed/smilesatomstd_min2/ --padding-factor 1
## train on this data again
python train.py /data/ifender/SMILES_or_SELFIES/prepr_data/fairseq_preprocessed/smilesatomstd_min2/ --total-num-update 2400000 --max-update 2400000  --save-interval 1 --arch cased_bert_pair --task  span_bert --optimizer adam --lr-scheduler polynomial_decay --lr 0.0001 --min-lr 1e-09  --criterion span_bert_loss  --max-tokens 4096 --tokens-per-sample 512 --weight-decay 0.01  --skip-invalid-size-inputs-valid-test --log-format json --log-interval 2000 --save-interval-updates 50000 --keep-interval-updates 50000 --update-freq 1 --seed 1 --save-dir /data/ifender/SMILES_or_SELFIES/train_out --warmup-updates 10000 --schemes [\"pair_span\"] --span-lower 1 --span-upper 10 --validate-interval 1  --clip-norm 1.0 --geometric-p 0.2 --adam-eps 1e-8 --short-seq-prob 0.0 --replacement-method span --clamp-attention --no-nsp --pair-loss-weight 1.0 --max-pair-targets 15 --pair-positional-embedding-size 200 --endpoints external

## for medium (1%) example
python preprocess.py --only-source --trainpref /data/ifender/SMILES_or_SELFIES/prepr_data/notfairs_preprocessed/smiles_atom_standard_med --validpref /data/ifender/SMILES_or_SELFIES/prepr_data/notfairs_preprocessed/smiles_atom_std_val_min --destdir /data/ifender/SMILES_or_SELFIES/prepr_data/fairseq_preprocessed/smilesatomstd_min/ --padding-factor 1

## try training again with newly prepreocessed da
python train.py /data/ifender/SMILES_or_SELFIES/prepr_data/fairseq_preprocessed/new_smilesatomstd/ --total-num-update 2400000 --max-update 2400000  --save-interval 1 --arch cased_bert_pair --task  span_bert --optimizer adam --lr-scheduler polynomial_decay --lr 0.0001 --min-lr 1e-09  --criterion span_bert_loss  --max-tokens 4096 --tokens-per-sample 512 --weight-decay 0.01  --skip-invalid-size-inputs-valid-test --log-format json --log-interval 2000 --save-interval-updates 50000 --keep-interval-updates 50000 --update-freq 1 --seed 1 --save-dir /data/ifender/SMILES_or_SELFIES/train_out/ --warmup-updates 10000 --schemes [\"pair_span\"] --span-lower 1 --span-upper 10 --validate-interval 1  --clip-norm 1.0 --geometric-p 0.2 --adam-eps 1e-8 --short-seq-prob 0.0 --replacement-method span --clamp-attention --no-nsp --pair-loss-weight 1.0 --max-pair-targets 15 --pair-positional-embedding-size 200 --endpoints external

## for just testing
python spanbert_datasetreading_test.py /data/ifender/SMILES_or_SELFIES/prepr_data/fairseq_preprocessed/new_smilesatomstd/ --total-num-update 2400000 --max-update 2400000  --save-interval 1 --arch cased_bert_pair_large --task  span_bert --optimizer adam --lr-scheduler polynomial_decay --lr 0.0001 --min-lr 1e-09 --criterion span_bert_loss --max-tokens 4096 --tokens-per-sample 512 --weight-decay 0.01  --skip-invalid-size-inputs-valid-test --log-format json --log-interval 2000 --save-interval-updates 50000 --keep-interval-updates 50000 --update-freq 1 --seed 1 --save-dir /data/ifender/SMILES_or_SELFIES/train_out/ --warmup-updates 10000 --schemes [\"pair_span\"] --span-lower 1 --span-upper 10 --validate-interval 1  --clip-norm 1.0 --geometric-p 0.2 --adam-eps 1e-8 --short-seq-prob 0.0 --replacement-method span --clamp-attention --no-nsp --pair-loss-weight 1.0 --max-pair-targets 15 --pair-positional-embedding-size 200 --endpoints external


###### TEST SET from wiki ###########
#test on a miniversion of wikiset from https://github.com/facebookresearch/fairseq/blob/main/examples/roberta/README.pretraining.md
## for tokenising the test set
python bpe_tokenize.py /data/ifender/SMILES_or_SELFIES/prepr_data/notfairs_preprocessed/wikitest/wikitext-103-raw/mini/wiki.train_mini.raw /data/ifender/SMILES_or_SELFIES/prepr_data/notfairs_preprocessed/wikitest/spanbert_bpe_tokenised/wikimini_bpe_toknised.txt
python bpe_tokenize.py /data/ifender/SMILES_or_SELFIES/prepr_data/notfairs_preprocessed/wikitest/wikitext-103-raw/mini/wiki.valid_mini.raw /data/ifender/SMILES_or_SELFIES/prepr_data/notfairs_preprocessed/wikitest/spanbert_bpe_tokenised/wikimini_bpe_toknised_val.txt
## for preprocessing wikitestset
python preprocess.py --only-source --trainpref /data/ifender/SMILES_or_SELFIES/prepr_data/notfairs_preprocessed/wikitest/spanbert_bpe_tokenised/wikimini_bpe_toknised.txt --validpref /data/ifender/SMILES_or_SELFIES/prepr_data/notfairs_preprocessed/wikitest/spanbert_bpe_tokenised/wikimini_bpe_toknised_val.txt --destdir /data/ifender/SMILES_or_SELFIES/prepr_data/fairseq_preprocessed/new_wikimini2/ --padding-factor 1

## with dict?
python preprocess.py --only-source --trainpref /data/ifender/SMILES_or_SELFIES/prepr_data/notfairs_preprocessed/wikitest/spanbert_bpe_tokenised/wikimini_bpe_toknised.txt --validpref /data/ifender/SMILES_or_SELFIES/prepr_data/notfairs_preprocessed/wikitest/spanbert_bpe_tokenised/wikimini_bpe_toknised_val.txt --srcdict /data/ifender/SMILES_or_SELFIES/prepr_data/fairseq_preprocessed/new_wikimini2/dict.txt --destdir /data/ifender/SMILES_or_SELFIES/prepr_data/fairseq_preprocessed/new_wikimini2/ --padding-factor 1
## testing training with this dataset 

## tokenise whole set - Tokenization This step assumes that the corpus is formatted as one-sentence-per-line with line breaks indicating the end of a document.
# awk -F'[.!?]' '{for(i=1;i<=NF;i++) if($i) print $i "."}' your_input_file.txt > output_sentences.txt
# grep -vE '^\s*\.$' your_file.txt > filtered_file.txt


## format corpus as one sentecne per line, then tokenise
python bpe_tokenize.py /data/ifender/SMILES_or_SELFIES/prepr_data/notfairs_preprocessed/wikitest/wikitext-103-raw/mini/wiki.train_mini_onesentperl_nodot.raw /data/ifender/SMILES_or_SELFIES/prepr_data/notfairs_preprocessed/wikitest/spanbert_bpe_tokenised/wikimini_train_onesentplnodot_bpe_tokenised.txt
# do same for validation set:  wiki.valid_mini_onesentperl_nodot.raw
python bpe_tokenize.py /data/ifender/SMILES_or_SELFIES/prepr_data/notfairs_preprocessed/wikitest/wikitext-103-raw/mini/wiki.valid_mini_onesentperl_nodot.raw /data/ifender/SMILES_or_SELFIES/prepr_data/notfairs_preprocessed/wikitest/spanbert_bpe_tokenised/wikimini_valid_onesentplnodot_bpe_tokenised.txt
# then preprpocess valid and train
python preprocess.py --only-source --trainpref /data/ifender/SMILES_or_SELFIES/prepr_data/notfairs_preprocessed/wikitest/spanbert_bpe_tokenised/wikimini_train_onesentplnodot_bpe_tokenised.txt --validpref /data/ifender/SMILES_or_SELFIES/prepr_data/notfairs_preprocessed/wikitest/spanbert_bpe_tokenised/wikimini_valid_onesentplnodot_bpe_tokenised.txt --destdir /data/ifender/SMILES_or_SELFIES/prepr_data/fairseq_preprocessed/onesentpl_nodot_valtrain/ --padding-factor 1
# dict.txt looks fine, two columns as expected
# then train on that data:
python train.py /data/ifender/SMILES_or_SELFIES/prepr_data/fairseq_preprocessed/onesentpl_nodot_valtrain/ --total-num-update 2400000 --max-update 2400000  --save-interval 1 --arch cased_bert_pair --task  span_bert --optimizer adam --lr-scheduler polynomial_decay --lr 0.0001 --min-lr 1e-09  --criterion span_bert_loss  --max-tokens 4096 --tokens-per-sample 512 --weight-decay 0.01  --skip-invalid-size-inputs-valid-test --log-format json --log-interval 2000 --save-interval-updates 50000 --keep-interval-updates 50000 --update-freq 1 --seed 1 --save-dir /data/ifender/SMILES_or_SELFIES/train_out/ --warmup-updates 10000 --schemes [\"pair_span\"] --span-lower 1 --span-upper 10 --validate-interval 1  --clip-norm 1.0 --geometric-p 0.2 --adam-eps 1e-8 --short-seq-prob 0.0 --replacement-method span --clamp-attention --no-nsp --pair-loss-weight 1.0 --max-pair-targets 15 --pair-positional-embedding-size 200 --endpoints external
# different lr --> no difference
python train.py /data/ifender/SMILES_or_SELFIES/prepr_data/fairseq_preprocessed/onesentpl_nodot_valtrain/ --total-num-update 2400000 --max-update 2400000  --save-interval 1 --arch cased_bert_pair --task  span_bert --optimizer adam --lr-scheduler polynomial_decay --lr 0.01 --min-lr 1e-09  --criterion span_bert_loss  --max-tokens 4096 --tokens-per-sample 512 --weight-decay 0.01  --skip-invalid-size-inputs-valid-test --log-format json --log-interval 2000 --save-interval-updates 50000 --keep-interval-updates 50000 --update-freq 1 --seed 1 --save-dir /data/ifender/SMILES_or_SELFIES/train_out/ --warmup-updates 10000 --schemes [\"pair_span\"] --span-lower 1 --span-upper 10 --validate-interval 1  --clip-norm 1.0 --geometric-p 0.2 --adam-eps 1e-8 --short-seq-prob 0.0 --replacement-method span --clamp-attention --no-nsp --pair-loss-weight 1.0 --max-pair-targets 15 --pair-positional-embedding-size 200 --endpoints external



# then fairseq preprocess with pad1
python preprocess.py --only-source --trainpref /data/ifender/SMILES_or_SELFIES/prepr_data/notfairs_preprocessed/wikitest/spanbert_bpe_tokenised/wikimini_train_onesentplnodot_bpe_tokenised.txt --validpref /data/ifender/SMILES_or_SELFIES/prepr_data/notfairs_preprocessed/wikitest/spanbert_bpe_tokenised/wikimini_train_onesentplnodot_bpe_tokenised.txt --destdir /data/ifender/SMILES_or_SELFIES/prepr_data/fairseq_preprocessed/onesentpl_nodot/pad1/ --padding-factor 1
# then fairseq preprocess with nopad
python preprocess.py --only-source --trainpref /data/ifender/SMILES_or_SELFIES/prepr_data/notfairs_preprocessed/wikitest/spanbert_bpe_tokenised/wikimini_train_onesentplnodot_bpe_tokenised.txt --validpref /data/ifender/SMILES_or_SELFIES/prepr_data/notfairs_preprocessed/wikitest/spanbert_bpe_tokenised/wikimini_train_onesentplnodot_bpe_tokenised.txt --destdir /data/ifender/SMILES_or_SELFIES/prepr_data/fairseq_preprocessed/onesentpl_nodot/nopad/

# test training again on either dataset, but only training set exists
python train.py /data/ifender/SMILES_or_SELFIES/prepr_data/fairseq_preprocessed/onesentpl_nodot/pad1/ --total-num-update 2400000 --max-update 2400000  --save-interval 1 --arch cased_bert_pair --task  span_bert --optimizer adam --lr-scheduler polynomial_decay --lr 0.0001 --min-lr 1e-09  --criterion span_bert_loss  --max-tokens 4096 --tokens-per-sample 512 --weight-decay 0.01  --skip-invalid-size-inputs-valid-test --log-format json --log-interval 2000 --save-interval-updates 50000 --keep-interval-updates 50000 --update-freq 1 --seed 1 --save-dir /data/ifender/SMILES_or_SELFIES/train_out/ --warmup-updates 10000 --schemes [\"pair_span\"] --span-lower 1 --span-upper 10 --validate-interval 1  --clip-norm 1.0 --geometric-p 0.2 --adam-eps 1e-8 --short-seq-prob 0.0 --replacement-method span --clamp-attention --no-nsp --pair-loss-weight 1.0 --max-pair-targets 15 --pair-positional-embedding-size 200 --endpoints external
------> also loss 0

python train.py /data/ifender/SMILES_or_SELFIES/prepr_data/fairseq_preprocessed/onesentpl_nodot/nopad/ --total-num-update 2400000 --max-update 2400000  --save-interval 1 --arch cased_bert_pair --task  span_bert --optimizer adam --lr-scheduler polynomial_decay --lr 0.0001 --min-lr 1e-09  --criterion span_bert_loss  --max-tokens 4096 --tokens-per-sample 512 --weight-decay 0.01  --skip-invalid-size-inputs-valid-test --log-format json --log-interval 2000 --save-interval-updates 50000 --keep-interval-updates 50000 --update-freq 1 --seed 1 --save-dir /data/ifender/SMILES_or_SELFIES/train_out/ --warmup-updates 10000 --schemes [\"pair_span\"] --span-lower 1 --span-upper 10 --validate-interval 1  --clip-norm 1.0 --geometric-p 0.2 --adam-eps 1e-8 --short-seq-prob 0.0 --replacement-method span --clamp-attention --no-nsp --pair-loss-weight 1.0 --max-pair-targets 15 --pair-positional-embedding-size 200 --endpoints external


# lower max tokens, no change
# architecture chnage, no change
# learning rate change, no change
# no nps left out, also no change
python train.py /data/ifender/SMILES_or_SELFIES/prepr_data/fairseq_preprocessed/onesentpl_nodot/nopad/ --total-num-update 2400000 --max-update 2400000  --save-interval 1 --arch bert_pair --task span_bert --optimizer adam --lr-scheduler polynomial_decay --lr 0.01 --min-lr 1e-09  --criterion span_bert_loss  --max-tokens 512 --tokens-per-sample 512 --weight-decay 0.01  --skip-invalid-size-inputs-valid-test --log-format json --log-interval 2000 --save-interval-updates 50000 --keep-interval-updates 50000 --update-freq 1 --seed 1 --save-dir /data/ifender/SMILES_or_SELFIES/train_out/ --warmup-updates 10000 --schemes [\"pair_span\"] --span-lower 1 --span-upper 10 --validate-interval 1  --clip-norm 1.0 --geometric-p 0.2 --adam-eps 1e-8 --short-seq-prob 0.0 --replacement-method span --clamp-attention --pair-loss-weight 1.0 --max-pair-targets 15 --pair-positional-embedding-size 200 --endpoints external


## trying a minimal train example
python train.py /data/ifender/SMILES_or_SELFIES/prepr_data/fairseq_preprocessed/new_smilesatomstd/ --seed 42 --no-nsp --arch cased_bert_pair --max-tokens 4096 --tokens-per-sample 512 --task span_bert --schemes [\"pair_span\"] --span-lower 1 --span-upper 10 --criterion span_bert_loss --save-dir /data/ifender/SMILES_or_SELFIES/train_out/





######################### further testing ####################
/data/ifender/SMILES_or_SELFIES/spanbert/SpanBERT/pretraining/train_out
python spanbert_datasetreading_test.py /data/ifender/SMILES_or_SELFIES/prepr_data/fairseq_preprocessed/new_smilesatomstd/ --total-num-update 2400000 --max-update 2400000  --save-interval 1 --arch cased_bert_pair_large --task  span_bert --optimizer adam --lr-scheduler polynomial_decay --lr 0.0001 --min-lr 1e-09 --criterion span_bert_loss --max-tokens 4096 --tokens-per-sample 512 --weight-decay 0.01  --skip-invalid-size-inputs-valid-test --log-format json --log-interval 2000 --save-interval-updates 50000 --keep-interval-updates 50000 --update-freq 1 --seed 1 --save-dir /data/ifender/SMILES_or_SELFIES/spanbert/SpanBERT/pretraining/train_out/ --warmup-updates 10000 --schemes [\"pair_span\"] --span-lower 1 --span-upper 10 --validate-interval 1  --clip-norm 1.0 --geometric-p 0.2 --adam-eps 1e-8 --short-seq-prob 0.0 --replacement-method span --clamp-attention --no-nsp --pair-loss-weight 1.0 --max-pair-targets 15 --pair-positional-embedding-size 200 --endpoints external


python train.py /data/ifender/SMILES_or_SELFIES/spanbert/SpanBERT/pretraining/preprocessed_data/selfies_atom_isomers/old/ --save-interval 1 --arch cased_bert_pair --task  span_bert --optimizer adam --lr 0.0001 --min-lr 1e-09  --criterion span_bert_loss  --max-tokens 4096 --tokens-per-sample 512 --weight-decay 0.01  --skip-invalid-size-inputs-valid-test --log-format json --log-interval 2000 --save-interval-updates 50000 --keep-interval-updates 50000 --update-freq 1 --seed 1 --save-dir /data/ifender/SMILES_or_SELFIES/selfies_atom_isomers/ --schemes [\"pair_span\"] --span-lower 1 --span-upper 10 --validate-interval 1  --clip-norm 1.0 --geometric-p 0.2 --adam-eps 1e-8 --short-seq-prob 0.0 --replacement-method span --clamp-attention --no-nsp --pair-loss-weight 1.0 --max-pair-targets 15 --pair-positional-embedding-size 200 --endpoints external



##### newest testing with break mode enabled ######
python spanbert_datasetreading_test.py /data/ifender/SMILES_or_SELFIES/prepr_data/fairseq_preprocessed/onesentpl_nodot_valtrain/ --total-num-update 2400000 --max-update 2400000  --save-interval 1 --arch cased_bert_pair --task  span_bert --optimizer adam --lr-scheduler polynomial_decay --lr 0.0001 --min-lr 1e-09  --criterion span_bert_loss  --max-tokens 4096 --tokens-per-sample 512 --weight-decay 0.01  --skip-invalid-size-inputs-valid-test --log-format json --log-interval 2000 --save-interval-updates 50000 --keep-interval-updates 50000 --update-freq 1 --seed 1 --save-dir /data/ifender/SMILES_or_SELFIES/train_out/ --warmup-updates 10000 --schemes [\"pair_span\"] --span-lower 1 --span-upper 10 --validate-interval 1  --clip-norm 1.0 --geometric-p 0.2 --adam-eps 1e-8 --short-seq-prob 0.0 --replacement-method span --clamp-attention --no-nsp --pair-loss-weight 1.0 --max-pair-targets 15 --pair-positional-embedding-size 200 --endpoints external --break-mode sentence

#### run on janniks preprocessed data directly
python train.py /data/ifender/SMILES_or_SELFIES/prepr_data/fairseq_preprocessed/janniks/ --total-num-update 2400000 --max-update 2400000  --save-interval 1 --arch cased_bert_pair --task  span_bert --optimizer adam --lr-scheduler polynomial_decay --lr 0.0001 --min-lr 1e-09  --criterion span_bert_loss  --max-tokens 4096 --tokens-per-sample 512 --weight-decay 0.01  --skip-invalid-size-inputs-valid-test --log-format json --log-interval 2000 --save-interval-updates 50000 --keep-interval-updates 50000 --update-freq 1 --seed 1 --save-dir /data/ifender/SMILES_or_SELFIES/train_out/ --warmup-updates 10000 --schemes [\"pair_span\"] --span-lower 1 --span-upper 10 --validate-interval 1  --clip-norm 1.0 --geometric-p 0.2 --adam-eps 1e-8 --short-seq-prob 0.0 --replacement-method span --clamp-attention --no-nsp --pair-loss-weight 1.0 --max-pair-targets 15 --pair-positional-embedding-size 200 --endpoints external --break-mode sentence
__. does not work on this data, not because "magic" cannot be asserted, but because when SpanBERT people write the train-data file they write internal info into it which is size of dataset

###### tokenise datasets again, that have been one-sentence per line and two new lines at the end, see what happens---> DOES NOT WORK AT ALL
# train
python bpe_tokenize.py /data/ifender/SMILES_or_SELFIES/prepr_data/notfairs_preprocessed/wikitest/wikitext-103-raw/mini/wiki.train_mini_onesentperl_nodot_end.raw /data/ifender/SMILES_or_SELFIES/prepr_data/notfairs_preprocessed/wikitest/spanbert_bpe_tokenised/wiki.train_mini_onesentperl_nodot_end_bpetoksd.raw
# valid
python bpe_tokenize.py /data/ifender/SMILES_or_SELFIES/prepr_data/notfairs_preprocessed/wikitest/wikitext-103-raw/mini/wiki.valid_mini_onesentperl_nodot_end.raw /data/ifender/SMILES_or_SELFIES/prepr_data/notfairs_preprocessed/wikitest/spanbert_bpe_tokenised/wiki.valid_mini_onesentperl_nodot_end_bpetoksd.raw
### fairseq preprocess with no pad
python preprocess.py --only-source --trainpref /data/ifender/SMILES_or_SELFIES/prepr_data/notfairs_preprocessed/wikitest/spanbert_bpe_tokenised/wiki.train_mini_onesentperl_nodot_end_bpetoksd.raw --validpref /data/ifender/SMILES_or_SELFIES/prepr_data/notfairs_preprocessed/wikitest/spanbert_bpe_tokenised/wiki.valid_mini_onesentperl_nodot_end_bpetoksd.raw --destdir /data/ifender/SMILES_or_SELFIES/prepr_data/fairseq_preprocessed/onesentpl_nodot_end_valtrain_nopad/

### fairseq preprocess with pad=1
python preprocess.py --only-source --trainpref /data/ifender/SMILES_or_SELFIES/prepr_data/notfairs_preprocessed/wikitest/spanbert_bpe_tokenised/wiki.train_mini_onesentperl_nodot_end_bpetoksd.raw --validpref /data/ifender/SMILES_or_SELFIES/prepr_data/notfairs_preprocessed/wikitest/spanbert_bpe_tokenised/wiki.valid_mini_onesentperl_nodot_end_bpetoksd.raw --destdir /data/ifender/SMILES_or_SELFIES/prepr_data/fairseq_preprocessed/onesentpl_nodot_end_valtrain_pad1/ --padding-factor 1

## train on either again, no break-mode
python train.py /data/ifender/SMILES_or_SELFIES/prepr_data/fairseq_preprocessed/onesentpl_nodot_end_valtrain_nopad/ --total-num-update 2400000 --max-update 2400000  --save-interval 1 --arch cased_bert_pair_large --task  span_bert --optimizer adam --lr-scheduler polynomial_decay --lr 0.0001 --min-lr 1e-09  --criterion span_bert_loss  --max-tokens 4096 --tokens-per-sample 512 --weight-decay 0.01  --skip-invalid-size-inputs-valid-test --log-format json --log-interval 2000 --save-interval-updates 50000 --keep-interval-updates 50000 --update-freq 1 --seed 1 --save-dir /data/ifender/SMILES_or_SELFIES/spanbert/SpanBERT/pretraining/train_out/ --warmup-updates 10000 --schemes [\"pair_span\"] --span-lower 1 --span-upper 10 --validate-interval 1  --clip-norm 1.0 --geometric-p 0.2 --adam-eps 1e-8 --short-seq-prob 0.0 --replacement-method span --clamp-attention --no-nsp --pair-loss-weight 1.0 --max-pair-targets 15 --pair-positional-embedding-size 200 --endpoints external
__does not work at all e.g. RuntimeError: CUDA error: device-side assert triggered

--does not work at all: /opt/conda/conda-bld/pytorch_1699449183005/work/aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [30,0,0], thread: [0,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
## with break maode
python train.py /data/ifender/SMILES_or_SELFIES/prepr_data/fairseq_preprocessed/onesentpl_nodot_end_valtrain_nopad/ --total-num-update 2400000 --max-update 2400000  --save-interval 1 --arch cased_bert_pair --task  span_bert --optimizer adam --lr-scheduler polynomial_decay --lr 0.0001 --min-lr 1e-09  --criterion span_bert_loss  --max-tokens 4096 --tokens-per-sample 512 --weight-decay 0.01  --skip-invalid-size-inputs-valid-test --log-format json --log-interval 2000 --save-interval-updates 50000 --keep-interval-updates 50000 --update-freq 1 --seed 1 --save-dir /data/ifender/SMILES_or_SELFIES/spanbert/SpanBERT/pretraining/train_out/ --warmup-updates 10000 --schemes [\"pair_span\"] --span-lower 1 --span-upper 10 --validate-interval 1  --clip-norm 1.0 --geometric-p 0.2 --adam-eps 1e-8 --short-seq-prob 0.0 --replacement-method span --clamp-attention --no-nsp --pair-loss-weight 1.0 --max-pair-targets 15 --pair-positional-embedding-size 200 --endpoints external --break-mode sentence
--> /opt/conda/conda-bld/pytorch_1699449183005/work/aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [8,0,0], thread: [64,0,0] Assertion `srcIndex < srcSelectDimSize` failed.
