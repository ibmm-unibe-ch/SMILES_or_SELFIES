{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f622a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from constants import TASK_MODEL_PATH, TOKENIZER_SUFFIXES, MOLNET_DIRECTORY, PROJECT_PATH, REACTION_PREDICTION_DIRECTORY, DESCRIPTORS\n",
    "\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "classification_scores = []\n",
    "regression_scores = []\n",
    "TOKENIZER_SUFFIXES+=[\"smiles_isomers_atom\",\"smiles_isomers_sentencepiece\",\"selfies_isomers_atom\",\"selfies_isomers_sentencepiece\"]\n",
    "for tokenizer_suffix in TOKENIZER_SUFFIXES:\n",
    "    for task in MOLNET_DIRECTORY.keys():\n",
    "        task_tokenizer_path = TASK_MODEL_PATH/task/tokenizer_suffix\n",
    "        if task.startswith(\"bace\") and not (\"isomer\" in tokenizer_suffix) :\n",
    "            task_tokenizer_path = Path(\"/data/jgut/SoS_models/task\")/task/tokenizer_suffix\n",
    "        print(task_tokenizer_path)\n",
    "        if task_tokenizer_path.exists():\n",
    "            for hyperparameter_path in glob(str(task_tokenizer_path) + \"/*\", recursive=True):\n",
    "                scores_path = hyperparameter_path+\"/scores.csv\"\n",
    "                print(scores_path)\n",
    "                if not Path(scores_path).is_file():\n",
    "                    continue\n",
    "                new_score_df = pd.read_csv(scores_path)\n",
    "                if list(new_score_df.task_type)[0] == \"classification\":\n",
    "                    classification_scores.append(new_score_df)\n",
    "                else:\n",
    "                    regression_scores.append(new_score_df)\n",
    "\n",
    "regression_scores = pd.concat(regression_scores, axis = 0, sort = False)\n",
    "classification_scores = pd.concat(classification_scores, axis = 0, sort = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a936e48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(regression_scores.columns[-3:])\n",
    "columns.extend(regression_scores.columns[:-3])\n",
    "\n",
    "regression_scores.sort_values([\"task\", \"tokenizer\"])[columns].sort_values([\"mean_absolute_error\"]).drop([\"model_size\", \"Unnamed: 0\", \"task_type\"], axis=\"columns\").groupby([\"task\", \"tokenizer\"]).first()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0afa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotting import plot_scores\n",
    "columns = list(classification_scores.columns[-3:])\n",
    "columns.extend(classification_scores.columns[:-3])\n",
    "tasks = classification_scores.task.unique()\n",
    "tokenizers = classification_scores.tokenizer.unique()\n",
    "scores = {}\n",
    "for task in tasks:\n",
    "    for tokenizer in tokenizers:\n",
    "        scores[tokenizer] = scores.get(tokenizer, []) + [max(classification_scores[(classification_scores[\"tokenizer\"]==tokenizer)&(classification_scores[\"task\"]== task)][\"ROC_AUC\"])]\n",
    "plot_scores(scores, tasks, \"AUROC\", Path(\"test/classification_report.svg\"))\n",
    "#classification_scores.sort_values([\"task\", \"tokenizer\"])[columns].sort_values([\"ROC_AUC\"], ascending=False).drop([\"model_size\", \"Unnamed: 0\", \"task_type\"], axis=\"columns\").groupby([\"task\", \"tokenizer\"]).first()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b55348a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotting import plot_scores\n",
    "columns = list(regression_scores.columns[-3:])\n",
    "columns.extend(regression_scores.columns[:-3])\n",
    "tasks = regression_scores.task.unique()\n",
    "tokenizers = regression_scores.tokenizer.unique()\n",
    "scores = {}\n",
    "for task in tasks:\n",
    "    for tokenizer in tokenizers:\n",
    "        scores[tokenizer] = scores.get(tokenizer, []) + [max(regression_scores[(regression_scores[\"tokenizer\"]==tokenizer)&(regression_scores[\"task\"]== task)][\"rectified_mean_squared_error\"])]\n",
    "plot_scores(scores, tasks, \"RMSE\", Path(\"test/regression_report.svg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5488bd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG, display\n",
    "from pathlib import Path\n",
    "\n",
    "def show_svg():\n",
    "    \n",
    "    display(SVG(Path(\"test/test_pca.svg\")))\n",
    "    display(SVG(Path(\"test/test_umap.svg\")))\n",
    "        \n",
    "show_svg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702bca99",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(classification_scores[(classification_scores[\"tokenizer\"]==\"smiles_atom\")&(classification_scores[\"task\"]== \"hiv\")][\"ROC_AUC\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d71354",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from constants import REACTION_PREDICTION_DIRECTORY, TOKENIZER_SUFFIXES, PROJECT_PATH\n",
    "pd.options.display.float_format = '{:,.3f}'.format\n",
    "dfs = []\n",
    "for task in [\"lef\"]:\n",
    "    for tokenizer in [\"selfies_atom\", \"selfies_sentencepiece\", \"smiles_sentencepiece\"]:\n",
    "        dfs.append(pd.read_csv(PROJECT_PATH/\"reaction_prediction_beam_neu\"/task/tokenizer/\"output.csv\"))\n",
    "df = pd.concat(dfs)\n",
    "df[\"top_perc_0\"]=0\n",
    "df[\"valid_perc_0\"]=0\n",
    "df[\"unk_perc_0\"]=0\n",
    "for i in range(1,11):\n",
    "    df[\"top_perc_\"+str(i)] =df[\"top_\"+str(i)]/df[\"all_samples\"]+df[\"top_perc_\"+str(i-1)]\n",
    "    df[\"valid_perc_\"+str(i)] =df[\"valid_\"+str(i)]/df[\"all_samples\"]\n",
    "    df[\"unk_perc_\"+str(i)] =df[\"unk_\"+str(i)]/df[\"all_samples\"]\n",
    "df.sort_values([\"task\", \"model\"])[[i for i in df.columns if ((\"perc\" in i) and (\"0\" not in i))or i in [\"model\", \"task\"]]].to_markdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76f1305",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptors = [ 'NumAliphaticCarbocycles',\n",
    " 'NumAliphaticHeterocycles',\n",
    " 'NumAliphaticRings',\n",
    " 'NumAromaticCarbocycles',\n",
    " 'NumAromaticHeterocycles',\n",
    " 'NumAromaticRings',\n",
    " 'NumHAcceptors',\n",
    " 'NumHDonors',\n",
    " 'NumHeteroatoms',\n",
    " 'NumRotatableBonds',\n",
    " 'NumSaturatedCarbocycles',\n",
    " 'NumSaturatedHeterocycles',\n",
    " 'NumSaturatedRings',\n",
    " 'RingCount',]\n",
    "indexes = [str(DESCRIPTORS.index(descriptor)) for descriptor in descriptors]\n",
    "test = pd.read_csv(\"processed/10m_deduplicated.csv\", skiprows=0, usecols=indexes)\n",
    "for index, descriptor in enumerate(descriptors):\n",
    "    amount = sum(test[indexes[index]].gt(0))\n",
    "    print(f\"The stats for descriptor {descriptor}\")\n",
    "    print(f\"Amount of mols with trait: {amount}, mols without trait: {len(test)-amount}\")\n",
    "    print(f\"This is {amount/len(test):.3f}, mols without trait: {1-amount/len(test):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15219e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from constants import DESCRIPTORS\n",
    "\n",
    "descriptors = [\n",
    " 'NumAliphaticHeterocycles',\n",
    " 'NumAromaticHeterocycles',\n",
    " 'NumSaturatedHeterocycles',]\n",
    "indexes = [str(DESCRIPTORS.index(descriptor)) for descriptor in descriptors]\n",
    "test = pd.read_csv(\"processed/10m_deduplicated.csv\", skiprows=0, usecols=indexes)\n",
    "\n",
    "amount = sum((test[indexes[0]]+test[indexes[1]]+test[indexes[2]]).gt(0))\n",
    "print(f\"The stats for descriptor heterocycles\")\n",
    "print(f\"Amount of mols with trait: {amount}, mols without trait: {len(test)-amount}\")\n",
    "print(f\"This is {amount/len(test):.3f}, mols without trait: {1-amount/len(test):.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
