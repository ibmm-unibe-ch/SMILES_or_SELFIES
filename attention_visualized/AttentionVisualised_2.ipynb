{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Attention readout and visualisation"
      ],
      "metadata": {
        "id": "Ehp34W93un4F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCbbZ90nKVIS",
        "outputId": "f01f6045-0f4a-4eb2-9039-6c800f791ae5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rdkit in /usr/local/lib/python3.10/dist-packages (2023.9.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rdkit) (1.23.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit) (9.4.0)\n",
            "Requirement already satisfied: prody in /usr/local/lib/python3.10/dist-packages (2.4.1)\n",
            "Requirement already satisfied: numpy<1.24,>=1.10 in /usr/local/lib/python3.10/dist-packages (from prody) (1.23.5)\n",
            "Requirement already satisfied: biopython<=1.79 in /usr/local/lib/python3.10/dist-packages (from prody) (1.79)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from prody) (3.1.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from prody) (1.11.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from prody) (67.7.2)\n",
            "Requirement already satisfied: biopython in /usr/local/lib/python3.10/dist-packages (1.79)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biopython) (1.23.5)\n",
            "Requirement already satisfied: cairosvg in /usr/local/lib/python3.10/dist-packages (2.7.1)\n",
            "Requirement already satisfied: cairocffi in /usr/local/lib/python3.10/dist-packages (from cairosvg) (1.7.0)\n",
            "Requirement already satisfied: cssselect2 in /usr/local/lib/python3.10/dist-packages (from cairosvg) (0.7.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from cairosvg) (0.7.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from cairosvg) (9.4.0)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from cairosvg) (1.3.0)\n",
            "Requirement already satisfied: cffi>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from cairocffi->cairosvg) (1.16.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from cssselect2->cairosvg) (0.5.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.1.0->cairocffi->cairosvg) (2.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install rdkit\n",
        "!pip install prody\n",
        "!pip install biopython\n",
        "!pip install cairosvg"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from prody import *\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem.Draw import rdMolDraw2D\n",
        "from rdkit.Chem import Draw\n",
        "from rdkit.Chem import AllChem\n",
        "from rdkit.Chem import rdmolops\n",
        "from rdkit import RDConfig\n",
        "import os\n",
        "import sys\n",
        "#plotting\n",
        "from IPython.display import SVG\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.colors as mcolors\n",
        "from collections import namedtuple\n",
        "import torch\n",
        "from IPython.display import SVG\n",
        "import cairosvg\n",
        "from IPython.display import Image, display\n",
        "import io\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "LEs0mviXKYo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''to import to get attention from model\n",
        "from attention_readout import gather_attention, gather_attention_model, to_markdown\n",
        "from constants import PREDICTION_MODEL_PATH'''"
      ],
      "metadata": {
        "id": "pO95aIvjIXMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' to import to get attention from model\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from constants import FAIRSEQ_PREPROCESS_PATH, PREDICTION_MODEL_PATH, TOKENIZER_PATH, TASK_PATH, MOLNET_DIRECTORY, PARSING_REGEX\n",
        "from deepchem.feat import RawFeaturizer\n",
        "from fairseq_utils import compute_model_output, get_dictionary, generate_prev_output_tokens\n",
        "from preprocessing import canonize_smile, create_identities, translate_selfie\n",
        "from scoring import load_model#, load_dataset\n",
        "from tokenisation import get_tokenizer\n",
        "import torch\n",
        "import re '''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "uo61nLYisQ_x",
        "outputId": "8a234b05-3cd0-4066-c917-76228be8d45d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' to import to get attention from model\\nfrom pathlib import Path\\nfrom typing import List, Tuple\\n\\nimport numpy as np\\nimport pandas as pd\\nfrom constants import FAIRSEQ_PREPROCESS_PATH, PREDICTION_MODEL_PATH, TOKENIZER_PATH, TASK_PATH, MOLNET_DIRECTORY, PARSING_REGEX\\nfrom deepchem.feat import RawFeaturizer\\nfrom fairseq_utils import compute_model_output, get_dictionary, generate_prev_output_tokens\\nfrom preprocessing import canonize_smile, create_identities, translate_selfie\\nfrom scoring import load_model#, load_dataset\\nfrom tokenisation import get_tokenizer\\nimport torch\\nimport re '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions"
      ],
      "metadata": {
        "id": "ocjXuy29IXvG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalizeAttentionScores():\n",
        "def gather_normalized_attention(smiles, smiles_atom_path,smiles_sentencepiece_path,selfies_atom_path,selfies_sentencepiece_path):\n",
        "  gather_attention()\n",
        ""
      ],
      "metadata": {
        "id": "MSdL_m2tIVyQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "81e3049a-f2d2-4c66-cbb2-2c5eeda0d1a4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "expected an indented block after function definition on line 1 (<ipython-input-2-3988da06b405>, line 2)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-3988da06b405>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    def gather_normalized_attention(smiles, smiles_atom_path,smiles_sentencepiece_path,selfies_atom_path,selfies_sentencepiece_path):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after function definition on line 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_attention_output(#dataset: List[np.ndarray],\n",
        "    model, texts, source_dictionary, tokenizer=None\n",
        "):\n",
        "    \"\"\"Compute attention of whole dataset with model copied from fairseq\n",
        "    https://github.com/facebookresearch/fairseq/blob/main/fairseq/models/bart/hub_interface.py\n",
        "\n",
        "    Args:\n",
        "        #dataset (List[np.ndarray]): pre-processed dataset to get attention from\n",
        "        model (fairseq model): fairseq model\n",
        "        texts (List[str]): human readable string of samples\n",
        "        source_dictionary: source dictionary for fairseq\n",
        "        tokenizer (optional): HuggingFace tokenizer to tokenize. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        List[List[Tuple[float, str]]]: List[List[attention, token]]\n",
        "    \"\"\"\n",
        "    # https://github.com/facebookresearch/fairseq/blob/main/fairseq/models/bart/hub_interface.py\n",
        "    device = next(model.parameters()).device\n",
        "    dataset_attentions = []\n",
        "    for counter, text in enumerate(texts):\n",
        "        if tokenizer is None:\n",
        "            parsed_tokens = [\n",
        "                parsed_token\n",
        "                for parsed_token in re.split(PARSING_REGEX, text)\n",
        "                if parsed_token\n",
        "            ]\n",
        "        else:\n",
        "            parsed_tokens = tokenizer.convert_ids_to_tokens(\n",
        "                tokenizer(text).input_ids\n",
        "            )\n",
        "        sample = torch.tensor(tokenizer(text).input_ids)\n",
        "        prev_output_tokens = generate_prev_output_tokens(sample, source_dictionary).to(\n",
        "            device\n",
        "        )\n",
        "        # same as in predict\n",
        "        out= model.model(sample.unsqueeze(0).to(device), None)\n",
        "    return out\n",
        "\n",
        "def gather_attention_model(input_mols, tokenizer_suffix):\n",
        "    model_suffix = tokenizer_suffix+\"_roberta\"\n",
        "    fairseq_dict_path = TASK_PATH / \"bbbp\" /tokenizer_suffix\n",
        "    tokenizer = get_tokenizer(TOKENIZER_PATH /tokenizer_suffix)\n",
        "    print(fairseq_dict_path)\n",
        "    print(PREDICTION_MODEL_PATH/model_suffix/\"checkpoint_last.pt\")\n",
        "    model = load_model(Path(\"/data/jgut/SMILES_or_SELFIES/task/bbbp/smiles_atom_isomers/5e-05_0.2_seed_0_arch_roberta/checkpoint_last.pt\"), fairseq_dict_path,None)\n",
        "    source_dictionary = get_dictionary(FAIRSEQ_PREPROCESS_PATH/tokenizer_suffix/\"dict.txt\")\n",
        "    #preprocessed = model.encode(input_mols)\n",
        "    return compute_attention_output(model, [input_mols], source_dictionary, tokenizer)\n",
        "\n",
        "def gather_attention(SMILES):\n",
        "    SMILES = canonize_smile(SMILES)\n",
        "    SELFIES = translate_selfie(SMILES)[0]\n",
        "    return gather_attention_model(SMILES, \"smiles_atom_isomers\")"
      ],
      "metadata": {
        "id": "W_om2pONr7Rr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_SMILES(SMILES_tok):\n",
        "    \"\"\"Cleaning of SMILES tokens input from hydrogens and digits\n",
        "\n",
        "    Args:\n",
        "        SMILES_tok (_list_): List of SMILES_tokens for a given SMILES\n",
        "\n",
        "    Returns:\n",
        "        _list,list_: Processed SMILES_token list and list of positions in input tokens list that were kept\n",
        "        (needed to distinguish which embeddings are relevant)\n",
        "    \"\"\"\n",
        "    SMILES_tok_prep = list()\n",
        "    struc_toks = r\"()=:~1234567890#\"\n",
        "    posToKeep = list()\n",
        "    pos = 0\n",
        "    for i in range(len(SMILES_tok)):\n",
        "        # when it's an H in the SMILES, ignore, cannot deal\n",
        "        if SMILES_tok[i] != \"H\" and SMILES_tok[i] != \"h\" and not SMILES_tok[i].isdigit() and not SMILES_tok[i].isspace():\n",
        "            if any(elem in struc_toks for elem in SMILES_tok[i]) == False:\n",
        "                if SMILES_tok[i] != \"-\":\n",
        "                    SMILES_tok_prep.append(SMILES_tok[i])\n",
        "                    # keep pos where you keep SMILES token\n",
        "                    posToKeep.append(pos)\n",
        "        pos += 1\n",
        "    assert(len(posToKeep) == (len(SMILES_tok_prep))\n",
        "           ), f\"Length of positions-to-keep-array ({len(posToKeep)}) and length of SMILES_tok_prep ({len(SMILES_tok_prep)}) are not the same\"\n",
        "    print(\"SMILES_tok: \", SMILES_tok)\n",
        "    print(\"posToKeep: \", posToKeep)\n",
        "    print(\"SMILES_tok_prep: \", SMILES_tok_prep)\n",
        "\n",
        "    return SMILES_tok_prep, posToKeep\n"
      ],
      "metadata": {
        "id": "WcNcUlL7_0Zm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cleanAndAggregateAttentions(attention_tensor,posToKeep, howto=\"mean\"):\n",
        "    # filter attention tensor according to posToKeep to only keep attention for non-hydrogen atoms\n",
        "    filtered_attentions = attention_tensor[posToKeep]\n",
        "    print(len(filtered_attentions))\n",
        "    assert len(clean_smiles)==len(filtered_attentions), \"Assert failed\"\n",
        "    # condense attention depending on method chosen\n",
        "    if howto==\"mean\":\n",
        "      print(\"Returning mean normalized attentions\")\n",
        "      mean_values = filtered_attentions.mean(dim=1)\n",
        "      print(mean_values,mean_values.size())\n",
        "      min_val = mean_values.min()\n",
        "      max_val = mean_values.max()\n",
        "      # normalizing to values between 0 and 1\n",
        "      normalized_means = (mean_values - min_val) / (max_val - min_val)\n",
        "      print(f\"Normalized means: {normalized_means}\")\n",
        "      return normalized_means\n",
        "\n",
        "    if howto==\"zscore_norm\":\n",
        "      print(\"Returning z-score normalised and standardized attentions\")\n",
        "      means = filtered_attentions.mean(dim=1, keepdim=True)\n",
        "      stds = filtered_attentions.std(dim=1, keepdim=True)\n",
        "      # z-score normalization\n",
        "      standardized_attentions = (filtered_attentions - means) / stds\n",
        "      print(\"standardized attentions to ensure similar scales: \",standardized_attentions, len(standardized_attentions), len(standardized_attentions[0]))\n",
        "      mean_values = standardized_attentions.mean(dim=1)\n",
        "      print(\"mean attentions of standardized attentions:\",mean_values)\n",
        "      min_val = mean_values.min()\n",
        "      max_val = mean_values.max()\n",
        "      normalized_means = (mean_values - min_val) / (max_val - min_val)\n",
        "      print(\"normalized means where all values fall between 0 and 1: \",normalized_means)\n",
        "      return normalized_means\n"
      ],
      "metadata": {
        "id": "ui21fLE4EYjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "nM7eY-D4E3w4",
        "outputId": "584450ef-9ee8-4c00-e3e0-e31a4aa087c6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "expected ':' (<ipython-input-1-7bd850fc2ca5>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-7bd850fc2ca5>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    def normalizeAttentionScores()\u001b[0m\n\u001b[0m                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m expected ':'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def drawMoleculeWithAttention(orig_smiles,clean_smiles,norm_attentions):\n",
        "\n",
        "    # Generate a color map from 0-1 scaled to attentions\n",
        "    min_val = norm_attentions.min()\n",
        "    max_val = norm_attentions.max()\n",
        "    print(f\"Minimum and maximum attention values: {min_val}, {max_val}\")\n",
        "    norm = mcolors.Normalize(vmin=min_val, vmax=max_val)\n",
        "    scalar_map = plt.cm.ScalarMappable(norm=norm, cmap='viridis')\n",
        "\n",
        "    mol = Chem.MolFromSmiles(orig_smiles)\n",
        "    drawer = rdMolDraw2D.MolDraw2DCairo(300, 300)\n",
        "    mol_with_style = rdMolDraw2D.PrepareMolForDrawing(mol, kekulize=False)\n",
        "\n",
        "    # Set the drawing options\n",
        "    opts = drawer.drawOptions()\n",
        "\n",
        "    # Create a color map for the atoms\n",
        "    atom_colors = {i: scalar_map.to_rgba(norm_attentions[i])[:-1] for i in range(mol.GetNumAtoms())}  # Remove alpha channel\n",
        "\n",
        "    # Draw the molecule with colored atoms according to attention scores\n",
        "    AllChem.Compute2DCoords(mol)\n",
        "    d = rdMolDraw2D.MolDraw2DSVG(400, 400)\n",
        "    rdMolDraw2D.PrepareAndDrawMolecule(d, mol, highlightAtoms=range(mol.GetNumAtoms()),\n",
        "                                      highlightAtomColors=atom_colors)\n",
        "\n",
        "    d.FinishDrawing()\n",
        "    # get the SVG string\n",
        "    svg = d.GetDrawingText()\n",
        "    # fix the svg string and display it\n",
        "    display(SVG(svg.replace('svg:','')))\n",
        "\n",
        "    # Colorbar\n",
        "    plt.figure(figsize=(4.5, 0.5))\n",
        "    img = np.array([[min_val, max_val]])\n",
        "    plt.imshow(img, cmap='viridis')\n",
        "    plt.gca().set_visible(False)\n",
        "    cax = plt.axes([0.1, 0.2, 0.8, 0.6])\n",
        "    colba = plt.colorbar(cax=cax, orientation=\"horizontal\")\n",
        "    colba.set_label('Attention Score')\n",
        "    plt.savefig(\"test.png\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Epb-EaYNo64g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get attention tensor from model"
      ],
      "metadata": {
        "id": "CKq_ePScsCUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#attention_tensor = gather_attention(smiles)"
      ],
      "metadata": {
        "id": "BjL2NzaWsBhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get attention tensor - Random for now"
      ],
      "metadata": {
        "id": "5KqcEBFxIPC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attention_tensor = torch.randn(1, 1, 8, 433)\n",
        "print(attention_tensor[0].size())\n",
        "smiles = \"CC=CCOCC\"\n",
        "smiles_tok = list(smiles)\n",
        "print(smiles_tok)\n",
        "attentions = attention_tensor[0,0]\n",
        "print(attentions.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6b8GL5ozUvt",
        "outputId": "0ada1493-13d9-4cb7-8699-303dcb2f9f82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 8, 433])\n",
            "['C', 'C', '=', 'C', 'C', 'O', 'C', 'C']\n",
            "torch.Size([8, 433])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main"
      ],
      "metadata": {
        "id": "HR2Zt87NIarq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Choose SMILES and get only the atoms from it"
      ],
      "metadata": {
        "id": "Zmivm1QdIlXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get rid of structural topkens and hydrogens in SMILES\n",
        "clean_smiles,posToKeep = clean_SMILES(smiles_tok)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUHmlTHw-x6U",
        "outputId": "bfe6ea26-2fea-4305-e80c-674dc91b537e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SMILES_tok:  ['C', 'C', '=', 'C', 'C', 'O', 'C', 'C']\n",
            "posToKeep:  [0, 1, 3, 4, 5, 6, 7]\n",
            "SMILES_tok_prep:  ['C', 'C', 'C', 'C', 'O', 'C', 'C']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decide on the model to analyze and gather the attention for the previously chosen SMILES"
      ],
      "metadata": {
        "id": "bYgYMmZeItck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''gather_normalized_attention(\"CC=CCOCC\",\n",
        "                 smiles_atom_path=PREDICTION_MODEL_PATH/\"smiles_atom_isomers_bart\"/\"checkpoint_last.pt\",\n",
        "                 smiles_sentencepiece_path=PREDICTION_MODEL_PATH/\"smiles_trained_isomers_bart\"/\"checkpoint_last.pt\",\n",
        "                 selfies_atom_path=PREDICTION_MODEL_PATH/\"selfies_atom_isomers_bart\"/\"checkpoint_last.pt\",\n",
        "                 selfies_sentencepiece_path=PREDICTION_MODEL_PATH/\"selfies_trained_isomers_bart\"/\"checkpoint_last.pt\")'''\n"
      ],
      "metadata": {
        "id": "SAggJ1VhIj4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# normalise and standardise attentions in attention tensor, only keep attentions belonging to atoms\n",
        "#aggregated_attentions=cleanAndAggregateAttentions(attentions,posToKeep,\"zscore_norm\")\n",
        "aggregated_attentions=cleanAndAggregateAttentions(attentions,posToKeep,\"mean\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "va29ixU9BXpX",
        "outputId": "c633e76c-5c22-4da8-9d7b-975f4809d344"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7\n",
            "Returning mean normalized attentions\n",
            "tensor([ 0.0651, -0.0026,  0.0032, -0.0280,  0.0366,  0.0861, -0.0101]) torch.Size([7])\n",
            "Normalized means: tensor([0.8158, 0.2228, 0.2731, 0.0000, 0.5663, 1.0000, 0.1569])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(aggregated_attentions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJ4cgUkZVm-w",
        "outputId": "818ded69-5df2-4471-a5ec-ba1d448443db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.8158, 0.2228, 0.2731, 0.0000, 0.5663, 1.0000, 0.1569])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(aggregated_attentions2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geud-qxzYLhT",
        "outputId": "bd0468d0-0984-499b-9947-9cbea20eecdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.8158, 0.2228, 0.2731, 0.0000, 0.5663, 1.0000, 0.1569])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(aggregated_attentions.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGvuIa1TY-dc",
        "outputId": "e888ac80-271b-410f-916d-2542f02f01ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([7])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drawMoleculeWithAttention(smiles,clean_smiles,aggregated_attentions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        },
        "id": "d0v8O3zMpQk3",
        "outputId": "98bf6d04-9b72-43bb-e851-8f0ff8203430"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimum and maximum attention values: 0.0, 1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.SVG object>"
            ],
            "image/svg+xml": "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:rdkit=\"http://www.rdkit.org/xml\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" version=\"1.1\" baseProfile=\"full\" xml:space=\"preserve\" width=\"400px\" height=\"400px\" viewBox=\"0 0 400 400\">\n<!-- END OF HEADER -->\n<rect style=\"opacity:1.0;fill:#FFFFFF;stroke:none\" width=\"400.0\" height=\"400.0\" x=\"0.0\" y=\"0.0\"> </rect>\n<ellipse cx=\"32.9\" cy=\"215.8\" rx=\"12.9\" ry=\"12.9\" class=\"atom-0\" style=\"fill:#83D34B;fill-rule:evenodd;stroke:#83D34B;stroke-width:1.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n<ellipse cx=\"88.6\" cy=\"183.7\" rx=\"12.9\" ry=\"12.9\" class=\"atom-1\" style=\"fill:#3D4A89;fill-rule:evenodd;stroke:#3D4A89;stroke-width:1.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n<ellipse cx=\"144.3\" cy=\"215.8\" rx=\"12.9\" ry=\"12.9\" class=\"atom-2\" style=\"fill:#38578C;fill-rule:evenodd;stroke:#38578C;stroke-width:1.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n<ellipse cx=\"200.0\" cy=\"183.7\" rx=\"12.9\" ry=\"12.9\" class=\"atom-3\" style=\"fill:#440154;fill-rule:evenodd;stroke:#440154;stroke-width:1.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n<ellipse cx=\"255.7\" cy=\"216.0\" rx=\"12.9\" ry=\"13.2\" class=\"atom-4\" style=\"fill:#1E9F88;fill-rule:evenodd;stroke:#1E9F88;stroke-width:1.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n<ellipse cx=\"311.4\" cy=\"183.7\" rx=\"12.9\" ry=\"12.9\" class=\"atom-5\" style=\"fill:#FDE724;fill-rule:evenodd;stroke:#FDE724;stroke-width:1.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n<ellipse cx=\"367.1\" cy=\"215.8\" rx=\"12.9\" ry=\"12.9\" class=\"atom-6\" style=\"fill:#453681;fill-rule:evenodd;stroke:#453681;stroke-width:1.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n<path class=\"bond-0 atom-0 atom-1\" d=\"M 32.9,215.8 L 88.6,183.7\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n<path class=\"bond-1 atom-1 atom-2\" d=\"M 88.6,183.7 L 144.3,215.8\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n<path class=\"bond-1 atom-1 atom-2\" d=\"M 88.6,194.8 L 139.5,224.2\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n<path class=\"bond-2 atom-2 atom-3\" d=\"M 144.3,215.8 L 200.0,183.7\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n<path class=\"bond-3 atom-3 atom-4\" d=\"M 200.0,183.7 L 222.6,196.7\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n<path class=\"bond-3 atom-3 atom-4\" d=\"M 222.6,196.7 L 245.2,209.8\" style=\"fill:none;fill-rule:evenodd;stroke:#FF0000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n<path class=\"bond-4 atom-4 atom-5\" d=\"M 266.2,209.8 L 288.8,196.7\" style=\"fill:none;fill-rule:evenodd;stroke:#FF0000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n<path class=\"bond-4 atom-4 atom-5\" d=\"M 288.8,196.7 L 311.4,183.7\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n<path class=\"bond-5 atom-5 atom-6\" d=\"M 311.4,183.7 L 367.1,215.8\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n<path d=\"M 85.8,185.3 L 88.6,183.7 L 91.4,185.3\" style=\"fill:none;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1;\"/>\n<path d=\"M 141.5,214.2 L 144.3,215.8 L 147.1,214.2\" style=\"fill:none;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1;\"/>\n<path d=\"M 197.2,185.3 L 200.0,183.7 L 201.1,184.3\" style=\"fill:none;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1;\"/>\n<path d=\"M 310.3,184.3 L 311.4,183.7 L 314.2,185.3\" style=\"fill:none;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1;\"/>\n<path class=\"atom-4\" d=\"M 247.3 215.9 Q 247.3 211.5, 249.5 209.1 Q 251.7 206.6, 255.7 206.6 Q 259.8 206.6, 261.9 209.1 Q 264.1 211.5, 264.1 215.9 Q 264.1 220.3, 261.9 222.8 Q 259.7 225.3, 255.7 225.3 Q 251.7 225.3, 249.5 222.8 Q 247.3 220.3, 247.3 215.9 M 255.7 223.3 Q 258.5 223.3, 260.0 221.4 Q 261.5 219.5, 261.5 215.9 Q 261.5 212.3, 260.0 210.5 Q 258.5 208.7, 255.7 208.7 Q 252.9 208.7, 251.4 210.5 Q 249.9 212.3, 249.9 215.9 Q 249.9 219.6, 251.4 221.4 Q 252.9 223.3, 255.7 223.3 \" fill=\"#FF0000\"/>\n</svg>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 450x50 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAABdCAYAAABkfFssAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAATE0lEQVR4nO3de1BU5RsH8O/ZA7ssctE0EYwgIZLM8jaamKMlRUOWlk00OmSOlxzt7pSWJfwqzTFtutnNSKspzUq0kOxCMiRpNuSaBWIqXb1l5QgSwbLP749lz55dFmSPu4va9zPD+L7nvZ7HZR/34jmKiAiIiIgMMHX2BoiI6MzFJEJERIYxiRARkWFMIkREZBiTCBERGcYkQkREhjGJEBGRYUwiRERkWFgwJm1oaEBjY2MwpiYiolNgNpsRERERsPkCnkQaGhoQa+2GRjQEemoiIjpFvXr1Qk1NTcASScCTSGNjIxrRgCuUsQhDOBSTAijOd82cZQUwKc7OigLFZNLKMJmcf7raWsbBNc7V5tEPreaE9zhXR69+os0Hz/l9tjmroitr/fR1k74NHvvUzykec7r7ieucWvbvbPPRT3HN6bWeay2T73HuvbRT9nGu2nHvuq8y2mhDB+bw0dZ6X7q2tur6tdo4VyjSobWhCDyuDeTRT7z6usritZ67n+JrnGsFBVAU92qKr3Etx/V1/ThF3w7Xw14/zt1maqkDgAmijXX1NXmVTWjdZnK16db3btPK0NcdujGuNgcAQNWt5ewHqNra7nHOfrq6vqw4oECgtsxpUhxQAV3ZvZ73HNqYlvEmuOrOdvccDqha/PTjHNpY55zu81Fb1lb068EdIxWiq8MdBwCqAqgtDwhn3VVWYIK7rCoKTC2fVjjrznJdrSBp8E9obGw8fZOIe+JwhCnhUHRP6lrZV6LwbjN1NIm0Tg4+k4iPfv4lEdeTt+L+JCkYSUT/RBSEJNJ6XBCSSFtt6MAc/iYRrzatXbdWW+fqVxLRzR+SJKL1bT+JaAlHV1a0dtHGdTiJePU1kkQ8E0XHkojreNtJRP/E2vEkYmqVRMSjrK3XwSSiKuJ+ElcUqFB0SUTRntRVKC1jXXXRzdFS19ZDu0lE7WAScR9vO4m4YhpI/GCdiIgMYxIhIiLDmESIiMgwJhEiIjKMSYSIiAxjEiEiIsOYRIiIyDAmESIiMoxJhIiIDGMSISIiw5hEiIjIMCYRIiIyjEmEiIgMYxIhIiLDmESIiMgwJhEiIjKMSYSIiAxjEiEiIsOYRIiIyDAmESIiMoxJhIiIDGMSISIiw5hEiIjIMCYRIiIyjEmEiIgMYxIhIiLDwoI1sR1NgACKKHDlKmdZAURp6aVAEZNWhpicfwKAQ4Gi6NqUlh/AOZ+rrKCl7KorgPc4fVnc/cTVJvCc32dbS1VRAAc8+ym6uknfBo99anOa4DFO309c5wQAJtccPvq1CqWiD6tHKD32Ae9xPsr6c/WYw3vONspoow0dmMNHW+t96draquvXauNcoUiH1oYizr1Dd0zrJ159XWXxWs/dT/E1zrWCAiiKezXF17iW4/q6fpyib4frYa8f526TlrpzBwIFgEPX1+RVNqF1m8nVplvfu00rQ1936Ma42py/XKpuLWc/QNXWdo9z9tPV9WXFAQUCtWVOk+KA2hJXZ9m9nvcc2piW8Sa46s529xwOqK7z8Rjn0MY653Sfj9qytqJfD+4YqRBdHe44AFAVQG15QDjrrrICE9xlVVG0VwjOurNcV+vxSA6IgCcREUFUVBS21BU5DzQHegUiIjIqKioKIoFLJgFPIoqioK6uDr/++itiYmICPf0Z7/jx40hMTGR82sD4tI/xaR/j0z5XfBT9OxOnKGhvZ8XExPAvsR2MT/sYn/YxPu1jfEKHH6wTEZFhTCJERGRYwJOIxWJBXl4eLBZLoKc+KzA+7WN82sf4tI/xaV8w4qNIID+mJyKi/xS+nUVERIYxiRARkWFMIkREZBiTCBERGWYoiSxfvhzJycmIiIjAsGHDsH379nb7v/fee+jbty8iIiLQv39/FBcXG9rsmcKf+KxYsQIjR45Et27d0K1bN2RmZp40nmc6fx8/LmvWrIGiKBg/fnxwN9jJ/I3PsWPHMHv2bMTHx8NisSAtLe2s/h3zNz7PPPMMLrroIlitViQmJuK+++5DQ0NDiHYbWmVlZbj++uuRkJAARVGwfv36k44pLS3FoEGDYLFYkJqailWrVvm3qPhpzZo1Yjab5fXXX5cffvhBpk+fLl27dpXDhw/77F9eXi6qqsqSJUuksrJSHnnkEQkPD5ddu3b5u/QZwd/4TJw4UZYvXy47duyQqqoquf322yU2NlZ+++23EO88NPyNj0tNTY307t1bRo4cKePGjQvNZjuBv/H5999/ZciQIZKdnS1btmyRmpoaKS0tFZvNFuKdh4a/8Xn77bfFYrHI22+/LTU1NfLJJ59IfHy83HfffSHeeWgUFxfL/PnzZd26dQJACgsL2+2/f/9+iYyMlPvvv18qKyvl+eefF1VVZdOmTR1e0+8kMnToUJk9e7ZWb25uloSEBHnyySd99r/lllvkuuuu8zg2bNgwueOOO/xd+ozgb3y82e12iY6OljfeeCNYW+xURuJjt9slIyNDXnvtNZk8efJZnUT8jc9LL70kffr0kcbGxlBtsVP5G5/Zs2fLVVdd5XHs/vvvlxEjRgR1n6eDjiSRBx98UPr16+dxLCcnR7Kysjq8jl9vZzU2NqKiogKZmZnaMZPJhMzMTGzdutXnmK1bt3r0B4CsrKw2+5/JjMTHW319PZqamnDOOecEa5udxmh8HnvsMfTs2RNTp04NxTY7jZH4fPjhhxg+fDhmz56NuLg4XHLJJVi0aBGam8++y2cbiU9GRgYqKiq0t7z279+P4uJiZGdnh2TPp7tAPD/7dQHGo0ePorm5GXFxcR7H4+LisHv3bp9jDh065LP/oUOH/Fn6jGAkPt7mzp2LhISEVn+xZwMj8dmyZQsKCgpgs9lCsMPOZSQ++/fvxxdffIFJkyahuLgYe/fuxaxZs9DU1IS8vLxQbDtkjMRn4sSJOHr0KK644gqICOx2O2bOnImHH344FFs+7bX1/Hz8+HH8888/sFqtJ52D3846jSxevBhr1qxBYWEhIiIiOns7na62tha5ublYsWIFevTo0dnbOS05HA707NkTr776KgYPHoycnBzMnz8fL7/8cmdv7bRQWlqKRYsW4cUXX8S3336LdevWYePGjXj88cc7e2tnDb9eifTo0QOqquLw4cMexw8fPoxevXr5HNOrVy+/+p/JjMTHZenSpVi8eDE+//xzXHrppcHcZqfxNz779u3DTz/9hOuvv1475nA47xQXFhaG6upqpKSkBHfTIWTk8RMfH4/w8HCoqqodS09Px6FDh9DY2Aiz2RzUPYeSkfg8+uijyM3NxbRp0wAA/fv3x4kTJzBjxgzMnz8fJtN/+9/RbT0/x8TEdOhVCODnKxGz2YzBgwejpKREO+ZwOFBSUoLhw4f7HDN8+HCP/gDw2Weftdn/TGYkPgCwZMkSPP7449i0aROGDBkSiq12Cn/j07dvX+zatQs2m037ueGGG3DllVfCZrMhMTExlNsPOiOPnxEjRmDv3r1acgWAPXv2ID4+/qxKIICx+NTX17dKFK6EK7xsYGCen/39xH/NmjVisVhk1apVUllZKTNmzJCuXbvKoUOHREQkNzdX5s2bp/UvLy+XsLAwWbp0qVRVVUleXt5Z/xVff+KzePFiMZvN8v7778vBgwe1n9ra2s46haDyNz7ezvZvZ/kbn19++UWio6PlzjvvlOrqaikqKpKePXvKE0880VmnEFT+xicvL0+io6Nl9erVsn//fvn0008lJSVFbrnlls46haCqra2VHTt2yI4dOwSAPP3007Jjxw75+eefRURk3rx5kpubq/V3fcX3gQcekKqqKlm+fHnwv+IrIvL888/L+eefL2azWYYOHSrbtm3T2kaNGiWTJ0/26L927VpJS0sTs9ks/fr1k40bNxpZ9ozhT3ySkpIEQKufvLy80G88RPx9/Oid7UlExP/4fPXVVzJs2DCxWCzSp08fWbhwodjt9hDvOnT8iU9TU5Pk5+dLSkqKRERESGJiosyaNUv+/vvv0G88BDZv3uzz+cQVk8mTJ8uoUaNajRkwYICYzWbp06ePrFy50q81eSl4IiIy7L/9qRIREZ0SJhEiIjKMSYSIiAxjEiEiIsOYRIiIyDAmESIiMoxJhIiIDGMSof+s0aNH49577+3sbRCd0ZhEKCi2bt0KVVVx3XXXtWrLz8/HgAEDWh3v6O08/VVaWgpFUXDs2DGP4+vWrQvJ1VwLCwtx+eWXIzY2FtHR0ejXrx+TF501mEQoKAoKCnDXXXehrKwMBw4c6Ozt+HTOOecgOjo6qGuUlJQgJycHEyZMwPbt21FRUYGFCxeiqakpaGs2Nzd7XJCRKKgCcb0WIr3a2lqJioqS3bt3S05OjixcuFBrW7lyZavr+qxcubLVNcSSkpK0MevXr5eBAweKxWKRCy64QPLz86WpqUlrByArVqyQ8ePHi9VqldTUVNmwYYOIOO/N7r2e6zpCo0aNknvuuUeb56+//pLc3Fzp2rWrWK1Wufbaa2XPnj0ee4+NjZVNmzZJ3759pUuXLpKVlSUHDhxoMxb33HOPjB49+qQx+/DDD2XIkCFisVike/fuMn78eL/3tWHDBklPTxdVVaWmpkYaGhpkzpw5kpCQIJGRkTJ06FDZvHnzSfdC5A8mEQq4goICGTJkiIiIfPTRR5KSkiIOh0NEROrr62XOnDnSr18/7YrF9fX1cuTIES2hHDx4UI4cOSIiImVlZRITEyOrVq2Sffv2yaeffirJycmSn5+vrQdAzjvvPHnnnXfkxx9/lLvvvluioqLkzz//FLvdLh988IEAkOrqajl48KAcO3ZMRFonkRtuuEHS09OlrKxMbDabZGVlSWpqqnb/8pUrV0p4eLhkZmbKN998IxUVFZKeni4TJ05sMxZPPvmknHvuue1etbqoqEhUVZUFCxZIZWWl2Gw2WbRokd/7ysjIkPLyctm9e7ecOHFCpk2bJhkZGVJWViZ79+6Vp556SiwWi0cCIjpVTCIUcBkZGfLMM8+IiPMqqj169PD4F3BeXp5cdtllrcYBkMLCQo9jY8aM8XhCFRF56623JD4+3mPcI488otXr6uoEgHz88cci4r6yqfeVW/VJZM+ePQJAysvLtfajR4+K1WqVtWvXioj7VdTevXu1PsuXL5e4uLg2Y1FXVyfZ2dnaq6ucnBwpKCiQhoYGrc/w4cNl0qRJPsf7sy+bzab1+fnnn0VVVfn999895hszZow89NBDbe6XyF9+3dmQ6GSqq6uxfft2FBYWAnDegTAnJwcFBQUYPXq03/Pt3LkT5eXlWLhwoXasubkZDQ0NqK+vR2RkJAB43A2yS5cuiImJwZEjRzq8TlVVFcLCwjBs2DDtWPfu3XHRRRehqqpKOxYZGelxN8X4+Ph21+nSpQs2btyIffv2YfPmzdi2bRvmzJmDZ599Flu3bkVkZCRsNhumT59+Svsym80eMdi1axeam5uRlpbmMd+///6L7t27dyAiRB3DJEIBVVBQALvdjoSEBO2YiMBiseCFF15AbGysX/PV1dXhf//7H2666aZWbfr70IeHh3u0KYoSlA+Xfa0jHbibQkpKClJSUjBt2jTMnz8faWlpePfddzFlypQO34a0PVarFYqiaPW6ujqoqoqKigqPW+cCQFRU1CmvR+TCJEIBY7fb8eabb2LZsmW45pprPNrGjx+P1atXY+bMmTCbzWhubm41Pjw8vNXxQYMGobq6GqmpqYb35bpNrK81XdLT02G32/H1118jIyMDAPDnn3+iuroaF198seG1fUlOTkZkZCROnDgBwPkqqqSkBFOmTAnYvgYOHIjm5mYcOXIEI0eODOj+ifSYRChgioqK8Pfff2Pq1KmtXnFMmDABBQUFmDlzJpKTk1FTUwObzYbzzjsP0dHRsFgsSE5ORklJCUaMGAGLxYJu3bphwYIFGDt2LM4//3zcfPPNMJlM2LlzJ77//ns88cQTHdpXUlISFEVBUVERsrOzYbVaW/1r/MILL8S4ceMwffp0vPLKK4iOjsa8efPQu3dvjBs3znBM8vPzUV9fj+zsbCQlJeHYsWN47rnn0NTUhKuvvhoAkJeXhzFjxiAlJQW33nor7HY7iouLMXfuXMP7SktLw6RJk3Dbbbdh2bJlGDhwIP744w+UlJTg0ksv9fn/d4gM6ewPZejsMXbsWMnOzvbZ9vXXXwsA2blzpzQ0NMiECROka9eu2jeyRJxfc01NTZWwsDCPr/hu2rRJMjIyxGq1SkxMjAwdOlReffVVrR0+PpCPjY31uM3nY489Jr169RJFUU76Fd/Y2FixWq2SlZXl86u0eoWFhdLer9EXX3whEyZMkMTERDGbzRIXFyfXXnutfPnllx79PvjgA+0WpT169JCbbrrplPYlItLY2CgLFiyQ5ORkCQ8Pl/j4eLnxxhvlu+++a3O/RP7i7XGJiMgw/o91IiIyjEmEiIgMYxIhIiLDmESIiMgwJhEiIjKMSYSIiAxjEiEiIsOYRIiIyDAmESIiMoxJhIiIDGMSISIiw5hEiIjIsP8DtYAZoQExUoQAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SMILES to look into:\n",
        "### Looking into attention differences in symmetrical molecules:\n",
        "\n",
        "1) c1cc2ccc3cccc4ccc(c1)c2c34 (Highly symmetrical (4 benzol rings))\n",
        "\n",
        "2) O=CNC(N1C=CN(C(NC=O)C(Cl)(Cl)Cl)C=C1)C(Cl)(Cl)Cl (Also very symmetrical, espc. Ns, Cls and Os in similar positions interesting to compare attention)\n",
        "\n",
        "3) c1ccc(-c2ccc(-c3ccccc3)cc2)cc1 (three benzol rings in a chain)\n",
        "\n",
        "4) c1ccc2c(c1)sc1ccccc12 (2 benzol rings, one five-ring )\n",
        "\n",
        "### Looking into differences in hydrophobicity:\n",
        "\n",
        "5) C1=CC=CC=C1 (Benzol, Solubility in water: 1.84 g/L (30 °C) (see: https://en.wikipedia.org/wiki/Benzene))\n",
        "\n",
        "6) CC(OC1=C(C(=O)O)C=CC=C1)=O CC(=O)OC1C=CC=CC=1C(O)=O (Aspirin, Solubility in water 3g/L (see: https://en.wikipedia.org/wiki/Aspirin))\n",
        "\n",
        "7) C1=CC(=CC=C1C(=O)O)O (PHBA (found in: https://www.sciencedirect.com/science/article/pii/S095965262030247X#fig1), Solubility in Water 5g/L at 25 °C (https://pubchem.ncbi.nlm.nih.gov/compound/135#section=Melting-Point))\n",
        "\n",
        "8) CCCCOC(=O)C1=CC=C(C=C1)N (Butyl 4-aminobenzoate (see: https://pubchem.ncbi.nlm.nih.gov/compound/2482#section=Melting-Point), Solubility in Water: 0.14g/L (no temperature stated))"
      ],
      "metadata": {
        "id": "NEH_PxhTzA9M"
      }
    }
  ]
}