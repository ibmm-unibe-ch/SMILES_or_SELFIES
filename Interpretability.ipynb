{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f65e2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jgut/miniconda3/envs/SoS3/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Skipped loading some Tensorflow models, missing a dependency. No module named 'tensorflow'\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'torch_geometric'\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. cannot import name 'DMPNN' from 'deepchem.models.torch_models' (/home/jgut/miniconda3/envs/SoS3/lib/python3.10/site-packages/deepchem/models/torch_models/__init__.py)\n",
      "Skipped loading modules with pytorch-lightning dependency, missing a dependency. No module named 'pytorch_lightning'\n",
      "Skipped loading some Jax models, missing a dependency. No module named 'jax'\n"
     ]
    }
   ],
   "source": [
    "from captum.attr import IntegratedGradients\n",
    "import numpy as np\n",
    "from constants import TASK_MODEL_PATH, TASK_PATH\n",
    "from scoring import load_model, load_dataset\n",
    "from pathlib import Path\n",
    "CUDA_DEVICE = 3\n",
    "from fairseq.models.bart import BARTModel\n",
    "from fairseq.data import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4543320",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path =  Path(\"/data/SoS_models/\")/\"bbbp\"/\"smiles_atom\"/\"5e-05_0.3_based_norm\"/\"5e-05_0.3_based_norm\"/\"checkpoint_last.pt\"\n",
    "data_path = TASK_PATH/\"bbbp\"/\"smiles_atom\"\n",
    "\n",
    "model = BARTModel.from_pretrained(\n",
    "    str(model_path.parent),\n",
    "    data_name_or_path=str(data_path),\n",
    "    checkpoint_file=str(model_path.name),\n",
    "    layernorm_embedding=True,\n",
    ")\n",
    "model.to(\"cpu\")\n",
    "model.eval()\n",
    "model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffc7c19e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5,  5,  9,  5,  6, 11,  9,  7,  3,  6,  5,  5,  4,  8,  4,  4,  4,  4,\n",
       "         4,  8,  7, 12,  3,  6,  5,  7,  5,  6, 11,  9,  7, 12,  8,  5,  5,  5,\n",
       "         3,  8,  5,  6, 11,  9,  7,  9,  2])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = TASK_PATH / \"bbbp\"/\"smiles_atom\" / \"input0\" / \"test\"\n",
    "source_dictionary = Dictionary.load(str(data_path.parent / \"dict.txt\"))\n",
    "dataset = load_dataset(data_path)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcee1bcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0246, 0.0210, 0.0196, 0.0265, 0.0154, 0.0214, 0.0214, 0.0208, 0.0295,\n",
       "        0.0160, 0.0239, 0.0230, 0.0213, 0.0262, 0.0206, 0.0216, 0.0202, 0.0211,\n",
       "        0.0204, 0.0263, 0.0196, 0.0243, 0.0281, 0.0209, 0.0245, 0.0232, 0.0268,\n",
       "        0.0187, 0.0222, 0.0197, 0.0211, 0.0205, 0.0285, 0.0239, 0.0227, 0.0251,\n",
       "        0.0310, 0.0252, 0.0208, 0.0173, 0.0171, 0.0187, 0.0194, 0.0201, 0.0198],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = dataset[0].unsqueeze(-1)\n",
    "prev_output_tokens = tokens.clone()\n",
    "\n",
    "prev_output_tokens[:, 0] = tokens.gather(\n",
    "    0,\n",
    "    (tokens.ne(source_dictionary.pad()).sum(0)-1).unsqueeze(-1)\n",
    ").squeeze()\n",
    "\n",
    "prev_output_tokens[:, 1:] = tokens[:, :-1]\n",
    "# same as in predict\n",
    "attention = model.model(dataset[0].unsqueeze(0), None, prev_output_tokens)[1][\"attn\"][0][0][0]\n",
    "attention"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
